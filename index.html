<!DOCTYPE html>
<html lang="en">

  <head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">

  <!-- Begin Jekyll SEO tag v2.7.1 -->
<title>Decentralized Infrastructure for (Neuro)science | Not sure where this shows up yet so if you see it then go ahead and tell me hey jonny ur website is bad</title>
<meta name="generator" content="Jekyll v4.2.1" />
<meta property="og:title" content="Decentralized Infrastructure for (Neuro)science" />
<meta name="author" content="Jonny Saunders" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Not sure where this shows up yet so if you see it then go ahead and tell me hey jonny ur website is bad" />
<meta property="og:description" content="Not sure where this shows up yet so if you see it then go ahead and tell me hey jonny ur website is bad" />
<link rel="canonical" href="https://jon-e.net/infrastructure" />
<meta property="og:url" content="https://jon-e.net/infrastructure" />
<meta property="og:site_name" content="Decentralized Infrastructure for (Neuro)science" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Decentralized Infrastructure for (Neuro)science" />
<script type="application/ld+json">
{"description":"Not sure where this shows up yet so if you see it then go ahead and tell me hey jonny ur website is bad","url":"https://jon-e.net/infrastructure","@type":"WebSite","headline":"Decentralized Infrastructure for (Neuro)science","sameAs":["https://twitter.com/json_dirs","https://jon-e.net"],"author":{"@type":"Person","name":"Jonny Saunders"},"name":"Jonny Saunders","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->


  <!-- CSS -->
  <link rel="stylesheet" href="/infrastructure/assets/main.css">
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Libre+Baskerville:400,400i,700">

  <!-- Favicon -->
<!--   <link rel="icon" type="image/png" sizes="32x32" href="/infrastructure/assets/favicon-32x32.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/infrastructure/assets/favicon-16x16.png">
  <link rel="apple-touch-icon" sizes="180x180" href="/infrastructure/assets/apple-touch-icon.png">
 -->
  <!-- RSS -->
    <script async=async src="https://hypothes.is/embed.js"></script>
    <script src="/infrastructure/assets/js/hypothesis.js"></script>

</head>


  <body>

    

    <main>
      <div class="post">
    <div class="post-info">
        <h1 class="post-title">Decentralized Infrastructure for (Neuro)science</h1>
        
        
          <h2 class="post-subtitle">Or, Kill the Cloud in Your Mind</h2>
        

        <div class="post-line"></div>

        
          <span class="post-author">Jonny Saunders</span>
        

        <br>

        

        <div class="post-date">
            <span>Rendered: </span>
            <time datetime="2021-29-24"> 2021-10-24</time>
        </div>
    </div>
    <div id="react-test"></div>

<div class="annotation-container">
	<h2>Current Document Status (21-10-15):</h2>
	<p>
		I have removed all the text past the <p><a href="#draftmarker"><code class="language-plaintext highlighter-rouge">#draftmarker</code></a></p>
 and replaced it with extreme summary placeholder text so that I could start sharing this with people to read and give feedback on. If you are reading this, at this point <strong>please do not redistribute this link!</strong>
	</p>
	<p>
		Paragraphs preceded with double exclamation points !! are provisional, placeholder text that is intended to be rewritten &lt;3 
	</p>
	<p>
		Everything else is text that has been drafted several times, but as of the current draft needs an enormous amount of editing! sorry for the redundancy (please mark it when you see it)! I tend to write very, uh, "emotional" and then edit for tone later, so I also ask for the assumption of good intent and not trying to talk shit about anyone's work!
	</p>
</div>
<div class="annotation-container">
	<h2>This document is in-progress, and I welcome feedback!</h2>
	<p>
		This page embeds <a href="https://hypothes.is">hypothes.is</a>, which allows you to annotate the text!
        Existing annotations should show up highlighted like this,
        and you can make a new annotation by highlighting an area of the text and clicking the 'annotate' button that appears! You can see all annotations by opening the sidebar on the right.
	</p>
	<p>
		To ask a question or make a comment about the document as a whole, we can use the 'page notes' feature sort of like a chatroom. Open the sidebar at the right, and click the 'page notes' tab at the top.
	</p>
</div>

<ol id="markdown-toc">
  <li><a href="#introduction" id="markdown-toc-introduction">Introduction</a></li>
  <li><a href="#the-state-of-things" id="markdown-toc-the-state-of-things">The State of Things</a>    <ol>
      <li><a href="#the-costs-of-being-deinfrastructured" id="markdown-toc-the-costs-of-being-deinfrastructured">The Costs of being Deinfrastructured</a></li>
      <li><a href="#systems-neuroscience-specifically" id="markdown-toc-systems-neuroscience-specifically">Systems Neuroscience Specifically…</a>        <ol>
          <li><a href="#diversity-of-measurements" id="markdown-toc-diversity-of-measurements">Diversity of Measurements</a></li>
          <li><a href="#diversity-of-preps" id="markdown-toc-diversity-of-preps">Diversity of Preps</a></li>
          <li><a href="#the-hacker-spirit-and-celebration-of-heroism" id="markdown-toc-the-hacker-spirit-and-celebration-of-heroism">The Hacker Spirit and Celebration of Heroism</a></li>
          <li><a href="#focus-on-the-science" id="markdown-toc-focus-on-the-science">Focus on the Science</a></li>
          <li><a href="#combinatorics-of-recent-technology" id="markdown-toc-combinatorics-of-recent-technology">Combinatorics of Recent Technology</a></li>
        </ol>
      </li>
      <li><a href="#scientific-software-generally" id="markdown-toc-scientific-software-generally">Scientific Software Generally…</a>        <ol>
          <li><a href="#incentivized-fragmentation" id="markdown-toc-incentivized-fragmentation">Incentivized Fragmentation</a></li>
          <li><a href="#domain-specific-silos" id="markdown-toc-domain-specific-silos">Domain-Specific Silos</a></li>
          <li><a href="#the-long-now-of-immediacy-vs-idealism" id="markdown-toc-the-long-now-of-immediacy-vs-idealism">“The Long Now” of Immediacy vs. Idealism</a></li>
          <li><a href="#neatness-vs-scruffiness" id="markdown-toc-neatness-vs-scruffiness">“Neatness” vs “Scruffiness”</a></li>
          <li><a href="#taped-on-interfaces-open-loop-user-testing" id="markdown-toc-taped-on-interfaces-open-loop-user-testing">Taped-on Interfaces: Open-Loop User Testing</a></li>
          <li><a href="#platforms-industry-capture-and-the-profit-motive" id="markdown-toc-platforms-industry-capture-and-the-profit-motive">Platforms, Industry Capture, and the Profit Motive</a></li>
          <li><a href="#protection-of-institutional-and-economic-power" id="markdown-toc-protection-of-institutional-and-economic-power">Protection of Institutional and Economic Power</a></li>
        </ol>
      </li>
      <li><a href="#whose-job-is-infrastructure---the-ivies-institutes-consortia-and-the-rest-of-us" id="markdown-toc-whose-job-is-infrastructure---the-ivies-institutes-consortia-and-the-rest-of-us">Whose Job is Infrastructure? - The Ivies, Institutes, Consortia, and “The Rest of Us”</a>        <ol>
          <li><a href="#institutional-core-facilities" id="markdown-toc-institutional-core-facilities">Institutional Core Facilities</a></li>
          <li><a href="#centralized-institutes" id="markdown-toc-centralized-institutes">Centralized Institutes</a></li>
          <li><a href="#meso-scale-collaborations" id="markdown-toc-meso-scale-collaborations">Meso-scale collaborations</a></li>
          <li><a href="#the-rest-of-us" id="markdown-toc-the-rest-of-us">The rest of us…</a></li>
        </ol>
      </li>
    </ol>
  </li>
  <li><a href="#a-draft-of-decentralized-scientific-infrastructure" id="markdown-toc-a-draft-of-decentralized-scientific-infrastructure">A Draft of Decentralized Scientific Infrastructure</a>    <ol>
      <li><a href="#design-principles" id="markdown-toc-design-principles">Design Principles</a>        <ol>
          <li><a href="#protocols-not-platforms" id="markdown-toc-protocols-not-platforms">Protocols, not Platforms</a></li>
          <li><a href="#integration-not-invention" id="markdown-toc-integration-not-invention">Integration, not Invention</a></li>
          <li><a href="#embrace-heterogeneity-be-uncoercive" id="markdown-toc-embrace-heterogeneity-be-uncoercive">Embrace Heterogeneity, Be Uncoercive</a></li>
          <li><a href="#empower-people-not-systems" id="markdown-toc-empower-people-not-systems">Empower People, not Systems</a></li>
          <li><a href="#infrastructure-is-social" id="markdown-toc-infrastructure-is-social">Infrastructure is Social</a></li>
          <li><a href="#usability-matters" id="markdown-toc-usability-matters">Usability Matters</a></li>
        </ol>
      </li>
      <li><a href="#shared-data" id="markdown-toc-shared-data">Shared Data</a>        <ol>
          <li><a href="#formats-as-onramps" id="markdown-toc-formats-as-onramps">Formats as Onramps</a></li>
          <li><a href="#peer-to-peer-as-a-backbone" id="markdown-toc-peer-to-peer-as-a-backbone">Peer-to-peer as a Backbone</a></li>
          <li><a href="#archives-need-communities" id="markdown-toc-archives-need-communities">Archives Need Communities</a></li>
          <li><a href="#linked-data-or-surveillance-capitalism" id="markdown-toc-linked-data-or-surveillance-capitalism">Linked Data or Surveillance Capitalism?</a></li>
          <li><a href="#federated-systems-of-language" id="markdown-toc-federated-systems-of-language">Federated Systems (of Language)</a></li>
        </ol>
      </li>
      <li><a href="#shared-tools" id="markdown-toc-shared-tools">Shared Tools</a>        <ol>
          <li><a href="#analytical-framework" id="markdown-toc-analytical-framework">Analytical Framework</a></li>
          <li><a href="#experimental-framework" id="markdown-toc-experimental-framework">Experimental Framework</a></li>
          <li><a href="#collectivizing-the-state-of-the-art" id="markdown-toc-collectivizing-the-state-of-the-art">Collectivizing the State of the Art</a></li>
        </ol>
      </li>
      <li><a href="#shared-knowledge" id="markdown-toc-shared-knowledge">Shared Knowledge</a>        <ol>
          <li><a href="#axes-of-communication-systems" id="markdown-toc-axes-of-communication-systems">Axes of Communication Systems</a></li>
          <li><a href="#the-wiki-way" id="markdown-toc-the-wiki-way">The Wiki Way</a></li>
          <li><a href="#rebuilding-scientific-communication" id="markdown-toc-rebuilding-scientific-communication">Rebuilding Scientific Communication</a></li>
          <li><a href="#credit-assignment" id="markdown-toc-credit-assignment">Credit Assignment</a></li>
        </ol>
      </li>
    </ol>
  </li>
  <li><a href="#conclusion" id="markdown-toc-conclusion">Conclusion</a>    <ol>
      <li><a href="#shared-governance" id="markdown-toc-shared-governance">Shared Governance</a></li>
      <li><a href="#contrasting-visions-for-science" id="markdown-toc-contrasting-visions-for-science">Contrasting visions for science</a>        <ol>
          <li><a href="#the-worst-platform-capitalist-world" id="markdown-toc-the-worst-platform-capitalist-world">The worst platform capitalist world</a></li>
          <li><a href="#what-we-could-hope-for" id="markdown-toc-what-we-could-hope-for">What we could hope for</a></li>
        </ol>
      </li>
    </ol>
  </li>
  <li><a href="#references" id="markdown-toc-references">References</a></li>
  <li><a href="#footnotes" id="markdown-toc-footnotes">Footnotes</a></li>
</ol>

<p><a href="/infrastructure/tex/decentralized_infrastructure_render.pdf"><strong>PDF VERSION</strong></a></p>

<p><em>This is a draft document, so if you do work that you think is relevant here but I am not citing it, it’s 99% likely that’s because I haven’t read it , not that I’m deliberately ignoring you! Odds are I’d love to read &amp; cite your work, and if you’re working in the same space try and join efforts!</em></p>

<hr />

<blockquote>
  <p>If we can make something decentralised, out of control, and of great simplicity, we must be prepared to be astonished at whatever might grow out of that new medium.</p>

  <p><a href="https://www.w3.org/1998/02/Potential.html">Tim Berners-Lee (1998): Realising the Full Potential of the Web</a></p>
</blockquote>

<blockquote>
  <p>A good analogy for the development of the Internet is that of
constantly renewing the individual streets and buildings of a city,
rather than razing the city and rebuilding it. The architectural
principles therefore aim to provide a framework for creating
cooperation and standards, as a small “spanning set” of rules that
generates a large, varied and evolving space of technology.</p>

  <p><a href="https://datatracker.ietf.org/doc/html/rfc1958">RFC 1958: Architectural Principles of the Internet</a></p>
</blockquote>

<blockquote>
  <p>In building cyberinfrastructure, the key question is not whether a problem is a “social” problem or a “technical” one. That is putting it the wrong way around. The question is whether we choose, for any given problem, a primarily social or a technical solution</p>

  <p><a href="https://doi.org/10.1007/978-1-4020-9789-8_5">Bowker, Baker, Millerand, and Ribes (2010): Toward Information Infrastructure Studies</a> <a class="citation" href="#bowkerInformationInfrastructureStudies2010">[1]</a></p>
</blockquote>

<blockquote>
  <p>The critical issue is, how do actors establish generative platforms by instituting a set of control points acceptable to others in a nascent ecosystem? <a class="citation" href="#tilsonDigitalInfrastructuresMissing2010">[2]</a></p>
</blockquote>

<p>Acknowledgements in no order at all!!! (make sure to double check spelling!!! and then also double check it’s cool to list them!!!):</p>

<ul>
  <li>Lucas Ott, the steadfast</li>
  <li>Tillie Morris</li>
  <li>Nick Sattler</li>
  <li>Sam Mehan</li>
  <li>Molly Shallow</li>
  <li>Mike and as always ty for letting me always go rogue</li>
  <li>Matt Smear</li>
  <li>Santiago Jaramillo</li>
  <li>Gabriele Hayden</li>
  <li>Eartha Mae</li>
  <li>jakob voigts for participating in the glue wiki</li>
  <li>nwb &amp; dandi team for dealing w/ my inane rambling</li>
  <li>Tomasz Pluskiewicz</li>
  <li>James Meickle</li>
  <li>Gonçalo Lopes</li>
  <li>Mackenzie Mathis</li>
  <li>Lauren E. Wool</li>
  <li>Gabi Hayden</li>
  <li>Mark Laubach &amp; Open Behavior Team</li>
  <li>Os Keyes</li>
  <li>Avery Everhart</li>
  <li>Eartha Mae Guthman</li>
  <li>Olivia Guest</li>
  <li>NWB &amp; DANDI teams</li>
  <li>Kris Chauvin</li>
  <li>Phil Parker</li>
  <li>Chris Rogers</li>
  <li>Danny Mclanahan</li>
  <li>Petar</li>
  <li>Jeremy Delahanty</li>
  <li>Andrey Andreev</li>
  <li>Joel Chan</li>
  <li>Sanjay Srivastava &amp; Metascience Class</li>
  <li>Ralph Emilio Peterson</li>
  <li>Manuel Schottdorf</li>
  <li>Ceci Herbert</li>
  <li>The Emerging ONICE team</li>
  <li>The Janet Smith House, especially Leslie Harka</li>
  <li>Rumbly Tumbly Lawnmower</li>
  <li>lmk if we talked and i missed ya!</li>
</ul>

<h1 id="introduction">Introduction</h1>

<p>We work in technical islands that range from individual researchers, to labs, consortia, and at their largest a few well-funded organizations. Our knowledge dissemination systems are as nimble as the static pdfs and ephemeral conference talks that they have been for decades (save for the godforsaken Science Twitter that we all correctly love to hate). Experimental instrumentation except for that at the polar extremes of technological complexity or simplicity is designed and built custom, locally, and on-demand. Software for performing experiments is a patchwork of libraries that satisfy some of the requirements of the experiment, sewn together by some uncommented script written years ago by a grad student who left the lab long-since. The technical knowledge to build both instrumentation and software is fragmented and unavailable as it sifts through the funnels of word-limited methods sections and never-finished documentation. And O Lord Let Us Pray For The Data, born into this world without coherent form to speak of, indexable only by passively-encrypted notes in a paper lab notebook, dressed up for the analytical ball once before being mothballed in ignominy on some unlabeled external drive.</p>

<p>In sum, all the ways our use and relations with computers are idiosyncratic and improvised are not isolated, but a symptom of a broader deficit in <strong>digital infrastructure</strong> for science. The yawning mismatch between our ambitions of what digital technology <em>should</em> allow us to do and the state of digital infrastructure hints at the magnitude of the problem: the degree to which the symptoms of digital deinfrastructuring define the daily reality of science is left as an exercise to the reader.</p>

<p>If the term infrastructure conjures images of highways and plumbing, then surely digital infrastructure would be flattered at the association. By analogy they illustrate many of its promises and challenges: when designed to, it can make practically impossible things trivial, allowing the development of cities by catching water where it lives and snaking it through tubes and tunnels sometimes directly into your kitchen. Its absence or failure is visible and impactful, as in the case of power outages. There is no guarantee that it “optimally” satisfies some set of needs for the benefit of the greatest number of people, as in the case of the commercial broadband duopolies. It exists not only as its technical reality, but also as an embodied and shared set of social practices, and so even when it does exist its form is not inevitable or final; as in the case of bottled water producers competing with municipal tap water on a behavioral basis despite being dramatically less efficient and more costly. Finally it is not socially or ethically neutral, and the impact of failure to build or maintain it is not equally shared, as in the expression of institutional racism that was the Flint, Michigan water crisis <a class="citation" href="#michicancivilrightscommissionFlintWaterCrisis2017">[3]</a>.</p>

<p>Being digitally deinfrastructured is not our inevitable and eternal fate, but the course of infrastructuring is far from certain. It is not the case that “scientific digital infrastructure” will rise from the sea monolithically as a natural result of more development time and funding, but instead has many possible futures<a class="citation" href="#mirowskiFutureOpenScience2018">[4]</a>, each with their own advocates and beneficiaries. Without concerted and strategic counterdevelopment based on a shared and liberatory ethical framework, science is poised to follow other domains of digital technology down the dark road of platform capitalism. The prize of owning the infrastructure that the practice of science is built on is too great, and it is not hard to imagine tech behemoths buying out the emerging landscape of small scientific-software-as-a-service startups and selling subscriptions to Science Prime.</p>

<p>This paper is an argument that <strong>decentralized</strong> digital infrastructure is the best means of realizing the promise of digital technology for science. I will draw from several disciplines and knowledge communities like Science and Technology Studies (STS), Library and Information Science, open source software developers, and internet pirates, among others to articulate a vision of an infrastructure in three parts: <strong>shared data, shared tools, and shared knowledge.</strong> I will start with a brief description of what I understand to be the state of our digital infrastructure and the structural barriers and incentives that constrain its development. I will then propose a set of design principles for decentralized infrastructure and possible means of implementing it informed by prior successes and failures at building mass digital infrastructure. I will close with contrasting visions of what science could be like depending on the course of our infrastructuring, and my thoughts on how different actors in the scientific system can contribute to and benefit from decentralization.</p>

<p>I insist that what I will describe is <em>not utopian</em> but is eminently practical — the truly impractical choice is to do nothing and continue to rest the practice of science on a pyramid scheme <a class="citation" href="#ponziSciencePyramidScheme2020">[5]</a> of underpaid labor. With a bit of development to integrate and improve the tools, <strong>everything I propose here already exists and is widely used.</strong> A central principle of decentralized systems is embracing heterogeneity: harnessing the power of the diverse ways we do science instead of constraining them. Rather than a patronizing argument that everyone needs to fundamentally alter the way they do science, the systems that I describe are specifically designed to be easily incorporated into existing practices and adapted to variable needs. In this way I argue decentralized systems are <em>more practical</em> than the dream that one system will be capable of expanding to the scale of all science — and as will hopefully become clear, inarguably <em>more powerful</em> than a disconnected sea of centralized platforms and services.</p>

<p>An easy and common misstep is to categorize this as solely a <em>technical</em> challenge. Instead the challenge of infrastructure is also <em>social</em> and <em>cultural</em> — it involves embedding any technology in a set of social practices, a shared belief that such technology should exist and that its form it not neutral, and a sense of communal valuation and purpose that sustains it <a class="citation" href="#bietzSustainingDevelopmentCyberinfrastructure2012">[6]</a>.</p>

<p>The social and technical perspectives are both essential, but make some conflicting demands on the construction of the piece: Infrastructuring requires considering the interrelatedness and mutual reinforcement of the problems to be addressed, rather than treating them as isolated problems that can be addressed piecemeal with a new package. Such a broad scope trades off with a detailed description of the relevant technology and systems, but a myopic techno-zealotry that does not examine the social and ethical nature of scientific practice risks reproducing or creating new sources of harm. As a balance I will not be proposing a complete technical specification or protocol, but describing the general form of the tools and some existing examples that satisfy them; I will not attempt a full history or treatment of the problem of infrastructuring, but provide enough to motivate the form of the proposed implementations.</p>

<p>My understanding of this problem is, of course, uncorrectably structured by the horizon of disciplines around systems neuroscience that has preoccupied my training. While the core of my argument is intended to be a sketch compatible with sciences and knowledge systems generally, my examples will sample from, and my focus will skew to my experience. In many cases, my use of “science” or “scientist” could be “neuroscience” or “neuroscientist,” but I will mostly use the former to avoid the constant context switches. I ask the reader for a measure of patience for the many ways this argument requires elaboration and modification for distant fields.</p>

<h1 id="the-state-of-things">The State of Things</h1>

<h2 id="the-costs-of-being-deinfrastructured">The Costs of being Deinfrastructured</h2>

<p>Framing the many challenges of scientific digital technology development as reflective of a general digital infrastructure deficit gives a shared etiology to the technical and social harms that are typically treated separately. It also allows us to problematize other symptoms that are embedded in the normal practice of contemporary science.</p>

<p>To give a sense of the scale of need for digital scientific infrastructure, as well as a general scope for the problems the proposed system is intended to address, I will list some of the present costs. These lists are grouped into rough and overlapping categories, but make no pretense at completeness and have no particular order.</p>

<p>Impacts on the <strong>daily experience</strong> of researchers include:</p>

<ul>
  <li>A prodigious duplication and dead-weight loss of labor as each lab, and sometimes each person within each lab, will reinvent basic code, tools, and practices from scratch. Literally it is the inefficiency of the <a href="https://en.wikipedia.org/wiki/Deadweight_loss#Harberger's_triangle">Harberger’s triangle</a> in the supply and demand system for scientific infrastructure caused by inadequate supply. Labs with enough resources are forced to pay from other parts of their grants to hire professional programmers and engineers to build the infrastructure for their lab (and usually their lab or institute only), but most just operate on a purely amateur basis. Many PhD students will spend the first several years of their degree re-solving already-solved problems, chasing the tails of the wrong half-readable engineering whitepapers, in their 6th year finally discovering the technique that they actually needed all along. That’s not an educational or training model, it’s the effect of displacing the undone labor of unbuilt infrastructure on vulnerable graduate workers almost always paid poverty wages.</li>
  <li>At least the partial cause of the phenomenon where “every scientist needs to be a programmer now” as people who aren’t particularly interested in being programmers — which is <em>fine</em> and <em>normal</em> — need to either suffer through code written by some other unlucky amateur or learn an entire additional discipline in order to do the work of the one they chose. Because there isn’t more basic scientific programming infrastructure, everyone needs to be a programmer.</li>
  <li>A great deal of pain and alienation for early- career researchers (ECRs) not previously trained in programming before being thrown in the deep end. Learning data hygeine practices like backup, annotation, etc. “the hard way” through some catastrophic loss is accepted myth in much of science. At some scale all the very real and widespread pain, and guilt, and shame felt by people who had little choice but to reinvent their own data management system must be recognized as an infrastructural, rather than a personal problem.</li>
  <li>The high cost of “openness” and the dearth of data transparency. It is still rare for systems neuroscience papers to publish the full, raw data along with all the analysis code, often because (in addition to the extraordinarily meagre incentives to do so) the data <em>and</em> analysis code are both completely homebrew and often omitted just due to the labor of cleaning it or the embarassment of sharing it<sup id="fnref:4" role="doc-noteref"><a href="#fn:4" class="footnote" rel="footnote">1</a></sup>. The “Open science” movement, roughly construed, has made a holy mess of the social climate around openness, publicly shaming “closed scientists” on leaderboards and only occasionally recalling the relative luxury of labor or expertise to become “open.” “Openness” is not a uniform or universal goal for all science, but for those for whom it makes sense, we need to provide the appropriate tooling before insisting on a change in scientific norms. We can’t expect data transparency from researchers while it is still so <em>hard.</em></li>
</ul>

<p>Impacts on the <strong>system of scientific inquiry</strong> include:</p>

<ul>
  <li>A profoundly leaky knowledge acquisition system where entire PhDs worth of data can be lost and rendered useless when a student leaves a lab and no one remembers how to access the data or how it’s formatted.</li>
  <li>The inevitability of continual replication crises because it is often literally impossible to replicate an experiment that is done on a rig that was built one time, used entirely in-lab code, and was never documented</li>
  <li>Outside of increasingly archaic PDFs distributed by already archaic journals, the need to rely on communication platforms and knowledge systems that weren’t designed to, and don’t come close to satisfying the needs of scientific communication. In the absence of some generalized means of knowledge organization, scientists ask the void (Twitter) for advice or guidance from anyone that algorithmically stumbles by. The highest we can aspire is to make a Slack about something, which even if it reaches the rare escape velocity of participation to make it useful, is incapable of producing a public, durable, and cumulative resource: and so the questions will be asked again… and again…</li>
  <li>A perhaps doomed intellectual endeavor<sup id="fnref:solaris" role="doc-noteref"><a href="#fn:solaris" class="footnote" rel="footnote">2</a></sup> as we attempt to understand the staggering complexity of the brain by peering at the brain through the pinprickiest peephole of just the most recent data you or your lab have collected rather than being able to index across all relevant data from not only your lab, but all other labs that have measured the same phenomena. The unnecessary reduplication of experiments becomes not just a methodological limitation, but an ethical catastrophe as researchers have little choice but to abandon the elemental principle of sacrificing as few animals as possible to understand a phenomenon.</li>
  <li>A hierarchy of prestige that devalues the labor of multiple groups of technicians, animal care workers, and so on. Authorship is the coin of the realm, but many researchers that do work fundamental to the operation of science only receive the credit of an acknowledgement. We need a system to value and assign credit for the immense amount of technical and practical knowledge and labor they produce.</li>
</ul>

<p>Impacts on the relationship between <strong>science and society</strong>:</p>

<ul>
  <li>An insular system where the inaccessibility of all the “contextual” knowledge <a class="citation" href="#woolKnowledgeNetworksHow2020">[7, 8]</a> that is beneath the level of publication but necessary to perform experiments, like “how to build this apparatus,” “what kind of motor would work here,” etc. is a force that favors established and well-funded labs who can rely on local knowledege and hiring engineers/etc. and excludes new, lesser-funded labs at non-ivy institutions. The concentration of technical knowledge magnifies the inequity of strongly skewed funding distributions such that the most well-funded labs can do a completely different kind of science than the rest of us, turning the positive-feedback loop of funding begetting funding ever faster.</li>
  <li>An absconscion with the public resources we are privileged enough to receive, where rather than returning the fruits of the many technical challenges we are tasked with solving to the public in the form of data, tools, collected practical knowledge, etc. we largely return papers, multiplying the above impacts of labor duplication and knowledge inaccessibility by the scale of society.</li>
  <li>The complicity of scientists in rendering our collective intellectual heritage nothing more than another regiment in the ever-advancing armies of platform capitalism. If our highest aspirations are to shunt all our experiments, data, and analysis tools onto Amazon Web Services, our failure of imagination will be responsible for yet another obligate funnel of wealth into some of the most harmful corporations that have ever existed. For ourselves, we guarantee another triple-pay industry skimming public money: pay to store data, pay for access to the database, maybe with a premium subscription for the most in-demand datasets. For society, we squander the chance for one of the very few domains of non-economic labor to build systems to recollectivize the basic infrastrucutre of the internet: rather than providing an alternative to the information overlords and their digital enclosure movement, we will be run right into their arms.</li>
</ul>

<p>and so on.</p>

<p>Considered separately, these are serious problems, but together they are a damning indictment of our role as stewards of our corner of the human knowledge project.</p>

<p>We arrive at this situation not because scientists are lazy and incompetent, but because the appropriate tools that fit the requirements of their discipline don’t exist. The tools don’t exist in part because we are embedded in a system of scientific labor that largely lack the reward mechanisms to build them, and in fact incentivize new, unintegrated, often quickly-abandoned tools rather than maintaining and expanding tools. After all, pull requests don’t get get publications. We are unlikely to arrive at a set of tools that meet our needs because we are embedded in a model of scientific and digital technology production that depend on maintaining points of centralized control to guarantee continued profit extraction: put bluntly, “we are dealing with a massively entrenched set of institutions, built around the last information age and fighting for its life” <a class="citation" href="#bowkerInformationInfrastructureStudies2010">[1]</a></p>

<p>There is, of course, an enormous amount of work being done by researchers and engineers on all of these problems, and a huge amount of progress has been made on them. My intention is not to shame or devalue anyone’s work, but to try and describe a path towards integrating it and making it mutually reinforcing.</p>

<p>Before proposing a potential solution to some of the above problems, it is important to motivate why they haven’t already been solved, or why their solution is not necessarily imminent. To do that, we need a sense of the social and technical challenges that structure the development of our tools.</p>

<h2 id="systems-neuroscience-specifically">Systems Neuroscience Specifically…</h2>

<p>Every discipline has its own particular technical needs, and is subject to its own peculiar history and culture. Though the type of comprehensive distributed infrastructure I will describe later is a domain-general project, systems neuroscience specifically lacks some features of it that are present in immediately neighboring disciplines like genetics and cognitive psychology. I won’t attempt a complete explanation, but instead will offer a few patterns I have noticed in my own limited exposure to the field that might serve as the beginnings of one. I want to be very clear throughout that I am never intending to cast shade on the work of anyone who has or does build and maintain the scientific infrastructure that exists — in fact the opposite, that y’all deserve more resources.</p>

<h3 id="diversity-of-measurements">Diversity of Measurements</h3>

<p>Molecular biology and genetics are perhaps the neighboring disciplines with the best data sharing and analytical structure, spawning and occupying the near totality of a new subdiscipline of Bioinformatics (for an absolutely fascinating ethnography, see <a class="citation" href="#bietzCollaborationMetagenomicsSequence2009">[9]</a>). Though the experiments are of course just as complex as those in systems neuroscience, most rely on a small number of stereotyped sequencing (meta?)methods that result in the same one-dimensional, four character sequence data structure of base pairs. Systems neuroscience experiments increasingly incorporate dozens of measurements, electrophysiology, calcium imaging, multiple video streams, motion, infrared, and other sensors, and so on. This is increasingly true as neuroscientists are attempting ever more complex and naturalistic neuroethological experiments. Even the seemingly “common” electrophysiological or multiphoton imaging data can have multiple forms — raw voltage traces? spike times? spike templates and times? single or multiunit? And these forms go through multiple intermediate stages of processing — binning, filtering, aggregating, etc. — each of which could be independently valuable and thus represented alongside their provenance in a theoretical data schema. Mainen and colleagues note this problem as well:</p>

<blockquote>
  <p>The data sets generated by a functional neuroscience experiment are large. They can also be complex and multimodal in ways that, say, genomic data might not be, embracing recordings of activity, behavioural patterns, responses to perturbations, and subsequent anatomical analysis. Researchers have no agreed formats for integrating different types of information. Nor are there standard systems for curating, uploading and hosting highly multimodal data. <a class="citation" href="#mainenBetterWayCrack2016">[10]</a></p>
</blockquote>

<p>The <a href="https://www.nwb.org/">Neurodata Without Borders</a> project has made a valiant effort to unify these multiple formats, but has for reasons that I won’t lay claim to knowing has yet to see widespread adoption. Contrast this with the <a href="https://bids.neuroimaging.io/">BIDS</a> data structure for fMRI data, where by converting your data to the structure you unlock a huge library of analysis pipelines for free. The beginnings of generalized platforms for neuroscientific data built on top of NWB are starting to happen in trickles and droplets, but they are still very much the exception rather than the rule.</p>

<p>We should not be so proud as to believe that our data is somehow uniquely complex. Theorizing about and reconciling the mass and heterogeneity of data in the universe is the subject of <a href="https://en.wikipedia.org/wiki/Information_science">multiple</a> full-fledged <a href="https://en.wikipedia.org/wiki/Library_science">disciplines</a>, and the conflict between simplified and centralized <a class="citation" href="#bakerMaintainingDublinCore2005">[11]</a> and sprawling and distributed <a class="citation" href="#berners-leeSEMANTICWEB2001">[12]</a> systems is well-trodden — and we should learn from it! We could instead think of the complexity of our data and the tools we develop to address it as what we have to offer the broader human mission towards a unified system of knowledge.</p>

<h3 id="diversity-of-preps">Diversity of Preps</h3>

<p>Though there are certain well-limbered experimental backbones like the two-alternative forced choice task, even within them there seems to be a comparatively broad diversity of experimental preparations in systems neuro relative to adjacent fields. Even a visual two-alternative forced choice task is substantially different than an auditory one, but there is almost nothing shared between those and, for example, <a href="https://doi.org/10.7554/eLife.29053">measuring the representation of 3d space in a free-flying echolocating bat</a>. So unlike cognitive neuroscience and psychophysics that has tools like <a href="https://pavlovia.org/">pavlovia</a> where the basic requirements and structure of experiments are more standardized, BioRXiv is replete with technical papers documenting “high throughput systems for this one very specific experiment” and there <a href="https://docs.auto-pi-lot.com">isn’t</a> a true experimental framework that satisfies the need for flexibility.</p>

<p>Mainen and colleagues note that this causes another problem distinct from variable outcome data, the even more variable and largely unreported metadata that parameterizes the minute details of experimental preps:</p>

<blockquote>
  <p>Worse, neuroscientists lack standardized vocabularies for describing the experimental conditions that affect brain and behavioural functions. Such a vocabulary is needed to properly annotate functional neural data. For instance, even small differences in when a water drop is released can affect how a mouse’s brain processes this event, but there is no standard way to specify such aspects of an experiment. <a class="citation" href="#mainenBetterWayCrack2016">[10]</a></p>
</blockquote>

<p>The problem of universal annotation and metadata reporting can be reframed, not as a <em>barrier to developing</em>, but as a <em>design constraint</em> of experimental programming infrastructure.  Because of the fragmentation of scientific programming infrastructure, where each experimental prep is implemented with entirely different, and often single-use software, there is no established reporting system for automatically capturing these minute details — but that doesn’t mean there can’t be (as I wrote previously, see section 2.3 in <a class="citation" href="#saundersAutopilotAutomatingBehavioral2019">[13]</a>, and coincidentally measured the effect of variable water droplets).</p>

<h3 id="the-hacker-spirit-and-celebration-of-heroism">The Hacker Spirit and Celebration of Heroism</h3>

<p>Many people are attracted to systems neuroscience precisely <em>because</em> of the… playful… attitude we take towards our rigs. If you want to do something, don’t ask questions just break out the <a href="http://jvoigts.scripts.mit.edu/blog/review-hot-glue/">hot glue</a>, vaseline, and aluminum foil and hack at it until it does what you want. The natural conclusion of widespread embodiment of this lovable scamp hacker spirit is its veneration as heroism: it is a <em>good thing</em> to have done an experiment that only you are capable of doing because that means you’re the best hacker. Not unrelated is the strong incentive to make something new rather than build on existing tools — you don’t get publications from pull requests, and you don’t get a job without publications. The initial International Brain Laboratory described the wily nature of neuroscientists accordingly:</p>

<blockquote>
  <p>Simply maintaining a true collaboration between 21 laboratories accustomed to going their own way will be a major novelty in neuroscience. <a class="citation" href="#abbottInternationalLaboratorySystems2017">[14]</a></p>
</blockquote>

<p>And yes, like the rest of the universe, perhaps the most influential forces in this domain are inertia and entropy. Once the boulder starts rolling down the hill of heroic idiosyncracy, tumbling along in a semi-stable jumble<sup id="fnref:butno" role="doc-noteref"><a href="#fn:butno" class="footnote" rel="footnote">3</a></sup> that supports the experiments of a lab, retooling and standardizing that system has to be <em>so very cool and worth it</em> that it overcomes the various, uncertain, but typically substantial costs (including the valid emotional costs of wishing a peaceful voyage to well-loved handcrafted tools). More than a single moment of adoption, the universe always has room for another course of disorder, and a commitment to using communal tools must be constantly reaffirmed. As we dream up new wild experiments, it needs to be easier to implement them with the existing system and integrate the labor expended in doing so back into it than it is to patch over the problem with a quick script saved to Desktop. As people cycle through the lab, it must be easier to learn than it is to start from scratch.</p>

<p>Yes again, Mainen and colleagues:</p>

<blockquote>
  <p>Neuroscientists frequently live on the ‘bleeding’ edge technologically, building bespoke and customized tools. This do-it-yourself approach has allowed innovators to get ahead of the competition, but hampered the standardization of methods essential to making experiments efficient and replicable.</p>

  <p>Remarkably, it is standard practice for each lab to custom engineer all manner of apparatus, from microscopes and electrodes to the computer programmes for analysing data. Thousands of labs worldwide use the calcium sensor GCaMP, for example, for imaging neural activity in vivo. Yet neither the microscopes used for GCaMP imaging nor the algorithms used to analyse the resulting data sets have been standardized. <a class="citation" href="#mainenBetterWayCrack2016">[10]</a></p>
</blockquote>

<p>!! make it clearer that the hacker spirit is not a <em>bad</em> thing but another <em>design constraint</em> and that we should actually avoid the paternalistic approach that says there’s a “right way” to do science, and instead honor, learn from, and support the diversity of our approaches.</p>

<h3 id="focus-on-the-science">Focus on the Science</h3>

<p>Completely understandably… scientists want to focus on their discipline rather than spending time building infrastructure. But because infrastructure touches all of our work and very few people can only build it in their spare time (mostly for the love of the craft) we all have to build some of it. this is a classic collective action problem, and scientists are not evil or selfish for wanting to do their work.</p>

<h3 id="combinatorics-of-recent-technology">Combinatorics of Recent Technology</h3>

<p>A lot of what I will describe here is relatively new! Some ideas are very old, like the semantic web and wikis, but others like federated communication and file transfer protocols are only reaching widespread use recently. The entire universe of open source scientific hardware and software has only sprung into its full and beautiful glory in the last decade or so, from pandas and and jupyter to open ephys and miniscopes and so on. Bittorrent is cool and good but IPFS allows us to think about qualitatively different things. It’s ultimately the <em>combination</em> of these recently technologies that’s important, rather than any single one of them. So in some sense it wasn’t <em>possible</em> to think about the type of basic infrastructure outside the traditional lens of centralized databases and individual experimental software packages.</p>

<h2 id="scientific-software-generally">Scientific Software Generally…</h2>

<p>The constraints posed by the structure of systems neuroscience as a discipline are of course echos and elaborations of larger constraints in the system of scientific infrastructure production.</p>

<h3 id="incentivized-fragmentation">Incentivized Fragmentation</h3>

<p>The incentive systems in science are complex, but tend to reward the production of many isolated, single-purpose software packages rather than cumulative work on shared infrastructure. The primary means of evaluation for a scientist is academic reputation, primarily operationalized by publications, but a software project will yield a single paper (if any at all). Traditional publications are static units of work that are “finished” and frozen in time, but software is never finished: the thousands of commits needed to maintain and extend the software are formally not a part of the system of academic reputation.</p>

<p>Shoehorning reputational rewards through traditional scientific publications has three immediate consequences: 1) Scientists are incentivized to make new, independent software that can be independently published, rather than integrating their work to extend the functionality of existing software. Howison &amp; Herbsleb described this dynamic in the context of BLAST</p>

<blockquote>
  <p>In essence we found that BLAST innovations from those motivated to improve BLAST by academic reputation are motivated to develop and to reveal, but not to integrate their contributions. Either integration is actively avoided to maintain a separate academic reputation or it is highly conditioned on whether or not publications on which they are authors will receive visibility and citation. <a class="citation" href="#howisonIncentivesIntegrationScientific2013">[15]</a></p>
</blockquote>

<p>For an example in Neuroscience, one can browse the papers that cite the DeepLabCut <a class="citation" href="#mathisDeepLabCutMarkerlessPose2018a">[16]</a> to find hundreds of downstream projects that make various extensions and improvements that are not integrated into the main library. While the logical extreme of the alternative of a single monolithic ur-library is also undesirable, the point is that a scientist that released 10 barely working, barely documented, rapidly abandoned packages along with 10 code papers would have 10 times the academic credit than one who spent the time integrating them into a unified, well-documented framework for something 1/10th<sup id="fnref:figuratively" role="doc-noteref"><a href="#fn:figuratively" class="footnote" rel="footnote">4</a></sup> as useful.</p>

<p>2) After publication, scientists have little incentive to <strong>maintain</strong> software outside of the domains in which the primary contributors use it (to satisfy reputational incentives by publishing in their own discipline), so outside of the most-used libraries most scientific software is brittle and difficult to use <a class="citation" href="#mangulImprovingUsabilityArchival2019">[17, 18]</a>.</p>

<p>3) Since the reputational value of a publication depends on its placement within a journal and number of citations (among other metrics), and citation practices for scientific software are far from uniform and universal, the incentive to write scientific software at all is relatively low compared to its near-universal use <a class="citation" href="#howisonSoftwareScientificLiterature2016">[19]</a>.</p>

<p>!! fragmentation has a subtler, self-reinforcing effect on planning – considering one part of the problem in isolation necessarily limits the horizon of the imaginable solutions to it. So considering just the journal system in isolation prevents you from imagining some broader new kind of science that also requires changes in data and tooling infrastructure. This is reflected in the direction of the open science movement, which initially was quite rad (cite those scientific utopia papers) but eventually focused more and more on the immediately implementable thigns like preregistration without addressing the deeper parts of the ecosystem of scientific labor. We have to think of the whole thing at once, but that’s really hard.</p>

<h3 id="domain-specific-silos">Domain-Specific Silos</h3>

<p>When funding exists for scientific infrastructure development, it typically comes in the form of side effects from, or administrative supplements to research grants. The NIH describes as much in their Strategic Plan for Data Science <a class="citation" href="#NIHStrategicPlan2018">[20]</a>:</p>

<blockquote>
  <p>from 2007 to 2016, NIH ICs used dozens of different funding strategies to support data 
resources, most of them linked to research-grant mechanisms that prioritized innovation and hypothesis testing over user service, utility, access, or efficiency. In addition, although the need for open and efficient data sharing is clear, where to store and access datasets generated by individual laboratories—and how to make them compliant with FAIR principles—is not yet straightforward. Overall, it is critical that the data-resource ecosystem become seamlessly integrated such that different data types and information about different organisms or diseases can be used easily together rather than existing in separate data “silos” with only local utility.</p>
</blockquote>

<p>The National Library of Medicine within the NIH currently lists 122 separate databases in its <a href="https://eresources.nlm.nih.gov/nlm_eresources/">search tool</a>, each serving a specific type of data for a specific research community. Though their current funding priorities signal a shift away from domain-specific tools, the rest of the scientific software system consists primarily of tools and data formats purpose-built for a relatively circumscribed group of scientists without any framework for their integration. Every field has its own challenges and needs for software tools, but there is little incentive to build tools that serve as generalized frameworks to integrate them.</p>

<h3 id="the-long-now-of-immediacy-vs-idealism">“The Long Now” of Immediacy vs. Idealism</h3>

<p>Digital infrastructure development takes place at multiple timescales simultaneously — from the momentary work of implementing it, through longer timescales of planning, organization, and documenting to the imagined indefinite future of its use — what Ribes and Finholt call “The Long Now. <a class="citation" href="#ribesLongNowTechnology2009">[21]</a>” Infrastructural projects constitutively need to contend with the need for immediately useful results vs. general and robust systems; the need to involve the effort of skilled workers vs. the uncertainty of future support; the  balance between stability with mutability; and so on. The tension between hacking something together vs. building something sustainable for future use is well-trod territory in the hot-glue and exposed wiring of systems neuroscience rigs.</p>

<p>Deinfrastructuring divides the incentives and interests of senior and junior researchers. Established researchers face little pressure to improve the state of infrastructure, as (very crudely) their primary incentives are to push enough publications through the door to be able to secure the next round of funding to keep their lab afloat. Their time preference is very short: hack it together, get the paper out, we’ll fix it later.</p>

<p>ECRs are tasked with develping the tools, often interested in developing tools they’ll be able to use throughout their careers, but between the pressure to establish their reputation with publications rarely have the time to develop something fully. As a consequence, a lot of software tools are developed by ECRs with no formal software training, contributing to the brittleness of scientific software and the low rates of adoption of best practices <a class="citation" href="#altschulAnatomySuccessfulComputational2013">[22]</a>. Anecdotally, the constant need to produce software that <em>does something</em> in the context of scientific programming which largely lacks the institutional systems and expert mentorship needed for well-architected software means that some programmers <em>never</em> have a chance to learn best practices commonly accepted in software engineering.</p>

<p>The problem of time horizon in development is not purely a product of inexperience, and a longer time horizon is not uniformly better. For an example, look no further than the history and cultural dynamics of the semantic web and linked data communities, revisted more fully in a moment as Scruffiness vs. Neatness. In the semantic web era, thousands of some of the most gifted programmers worked with an eye to the indefinite future, but the raw idealism and neglect of the pragmatic reality of the need for software to <em>do something</em> drove many to abandon the effort:</p>

<blockquote>
  <p>But there was no <em>use</em> of it. I wasn’t using any of the technologies for anything, except for things related to the technology itself. The Semantic Web is utterly inbred in that respect. The problem is in the model, that we create this metaformat, RDF, and <em>then</em> the use cases will come. But they haven’t, and they won’t. Even the genealogy use case turned out to be based on a fallacy. The very few use cases that there are, such as Dan Connolly’s hAudio export process, don’t justify hundreds of eminent computer scientists cranking out specification after specification and API after API.</p>

  <p>When we discussed this on the Semantic Web Interest Group, the conversation kept turning to how the formats could be fixed to make the use cases that I outlined happen. “Yeah, Sean’s right, let’s fix our languages!” But it’s not the languages which are broken, except in as much as they are entirely broken: because it’s the <em>mentality</em> of their design which is broken. You can’t, it has turned out, make a metalanguage like RDF and then go looking for use cases. We thought you could, but you can’t. It’s taken eight years to realise.
<a class="citation" href="#palmerDitchingSemanticWeb2008">[23]</a></p>
</blockquote>

<p>Developing digital infrastructure must be both bound to fulfilling immediate needs and a sense of incrementalism as well as guided by a long-range vision. The technical and social lessons run in parallel: We need software that solves problems people actually have right now, but can flexibly support its eventual form. We need a long-range vision to know what kind of tools we should build and which we shouldn’t, and we need to keep it in a tight loop with the always-changing needs of the people it supports. In short, to develop digital infrastructure we need to be <em>strategic.</em> To be strategic we need a <em>plan.</em> To have a plan we need to value planning as <em>work.</em> On this, Ribes and Finholt are instructive:</p>

<blockquote>
  <p>“On the one hand, I know we have to keep it all running, but on the other, LTER is about long-term data archiving. If we want to do that, we have to have the time to test and enact new approaches. But if we’re working on the to-do lists, we aren’t working on the tomorrow-list” (LTER workgroup discussion 10/05).</p>

  <p>The tension described here involves not only time management, but also the differing valuations placed on these kinds of work. The implicit hierarchy places scientific research first, followed by deployment of new analytic tools and resources, and trailed by maintenance work. […] While in an ideal situation development could be tied to everyday maintenance, in practice, maintenance work is often invisible and undervalued. As Star notes, infrastructure becomes visible upon breakdown, and only then is attention directed at its everyday workings (1999). Scientists are said to be rewarded for producing new knowledge, developers for successfully implementing a novel technology, but the work of maintenance (while crucial) is often thankless, of low status, and difficult to track. <em>How can projects support the distribution of work across research, development, and maintenance?</em> <a class="citation" href="#ribesLongNowTechnology2009">[21]</a></p>
</blockquote>

<p><a class="citation" href="#gawerBridgingDifferingPerspectives2014">[24]</a></p>

<p>test</p>

<h3 id="neatness-vs-scruffiness">“Neatness” vs “Scruffiness”</h3>

<p>Closely related to the tension between “Now” and “Later” is the tension between “Neatness” and “Scruffiness.” Lindsay Poirier traces its reflection in the semantic web community as the way that differences in “thought styles” result in different “design logics”  <a class="citation" href="#poirierTurnScruffyEthnographic2017">[25]</a>. On the question of how to develop technology for representing the ontology of the web – the system of terminology and structures with which everything should be named – there were (very roughly) two camps. The “neats” prioritized consistency, predictability, uniformity, and coherence – a logically complete and formally valid System of Everything. The “scruffies” prioritized local systems of knowledge, expressivity, “believing that ontologies will evolve organically as everyday webmasters figure out what schemas they need to describe and link their data. <a class="citation" href="#poirierTurnScruffyEthnographic2017">[25]</a>”</p>

<p>Practically, the differences between these thought communities impact the tools they build. Aaron Swartz put the approach of the “neat” semantic web architects the way he did:</p>

<blockquote>
  <p>Instead of the “let’s just build something that works” attitude that made the Web (and the Internet) such a roaring success, they brought the formalizing mindset of mathematicians and the institutional structures of academics and defense contractors. They formed committees to form working groups to write drafts of ontologies that carefully listed (in 100-page Word documents) all possible things in the universe and the various properties they could have, and they spent hours in Talmudic debates over whether a washing machine was a kitchen appliance or a household cleaning device.</p>

  <p>With them has come academic research and government grants and corporate R&amp;D and the whole apparatus of people and institutions that scream “pipedream.” And instead of spending time building things, they’ve convinced people interested in these ideas that the first thing we need to do is write standards. (To engineers, this is absurd from the start—standards are things you write after you’ve got something working, not before!) <a class="citation" href="#swartzAaronSwartzProgrammable2013">[26]</a></p>
</blockquote>

<p>The “scruffies,” recognizing the limitations of this approach diverged into a distinct thought community under the mantle of linked data. The linked data developers, starting by acknowledging that no one system can possibly capture everything, build tools that allow expression of local systems of meaning with the expectation and affordances for linking data between these systems as an ongoing social process.</p>

<p>The outcomes of this cultural rift are subtle, but the broad strokes are clear: the linked data community has taken some of the core semantic web technology like RDF, OWL, and the like, and developed a broad range of downstream technologies that have found broad use across information sciences, library sciences, and other applied domains. The vision of a totalizing and logically consistent semantic web, however, has largely faded into obscurity. One developer involved with semantic web technologies (who requested not be named), captured the present situation in their description of a still-active developer mailing list:</p>

<blockquote>
  <p>I think that some people are completely detached from practical applications of what they propose. […] I could not follow half of the messages. these guys seem completely removed from our plane of existence and I have no clue what they are trying to solve.</p>
</blockquote>

<p>This division in thought styles generalizes across domains of infrastructure, though outside of the linked data and similar worlds the dichotomy is more frequently between “neatness” and “people doing whatever” – with integration and interoperability becoming nearly synonymous with standardization. Calls for standardization without careful consideration and incorporation of existing practice have a familiar cycle: devise a standard that will solve everything, implement it, wonder why people aren’t using it, funding and energy dissipiates, rinse, repeat. The difficulty of scaling an exacting vision of how data should be formatted, the tools researchers should use for their experiments, and so on is that they require dramatic and sometimes total changes to the way people do science. The alternative is not between standardization and chaos, but a potential third way is designing infrastructures that allow the diversity of approaches, tools, and techniques to be expressed in a common framework or protocol along with the community infrastructure to allow the continual negotiation of their relationship.</p>

<h3 id="taped-on-interfaces-open-loop-user-testing">Taped-on Interfaces: Open-Loop User Testing</h3>

<p>The point of most active competition in many domains of commercial software is the user interface and experience (UI/UX), and to compete software companies will exhaustively user-test and refine them with pixel precision to avoid any potential customer feeling even a thimbleful of frustration. Scientific software development is largely disconnected from usability testing, as what little support exists is rarely tied to it. This, combined with the above incentives for developing new packages – and thus reduplicating the work of interface development – and the preponderance of semi-amateurs make it perhaps unsurprising that most scientific software is hard to use!</p>

<p>I intend the notion of “interface” in an expansive way: In addition to the graphical user interface (GUI) exposed to the end-user, I am referring generally to all points of contact with users, developers, and other software. Interfaces are intrinsically social, and include the surrounding documentation and experience of use — part of using an API is being able to figure out how to use it! The typical form of scientific software is a black box: I implemented an algorithm of some kind, here is how to use it, but beneath the surface there be dragons. Ideally, software would be designed with programming interfaces and documentation at multiple scales of complexity to enable clean entrypoints for developers with differing levels of skill and investment to contribute. Additionally, it would include interfaces for use and integration with other software. Without care given to either of these interfaces, the community of codevelopers is likely to remain small, and the labor they expend is less likely to be useful outside that single project. This, in turn, reinforces the incentives for developing new packages and fragmentation.</p>

<h3 id="platforms-industry-capture-and-the-profit-motive">Platforms, Industry Capture, and the Profit Motive</h3>

<p>Publicly funded science is an always-irresistable golden goose for private industry. The fragmented interests of scientists and the historically light touch of funding agencies on encroaching privatization means that if some company manages to capture and privatize a corner of scientific practice they are likely to keep it. Industry capture has been thoroughly criticized in the context of the journal system (eg. recently, <a class="citation" href="#brembsReplacingAcademicJournals2021">[27]</a>), and that criticism should extend to the rest of our infrastructure. Another major engine for privatization of scientific infrastructure has been the preponderance of software as a service (SaaS), from startups to international megacorporations, that sell access to some, typically proprietary software without selling the software itself.</p>

<p>While in isolation SaaS can make individual components of the infrastructural landscape easier to access — and even free!!* — the business model is fundamentally incompatible with integrated and accessible infrastructure. The SaaS model derives revenue from subscription or use costs, often operating as freemium models that make some subset of its services available for free. Even in freemium models, though, the business model requires that some functionality of the platform is paywalled (See a more thorough treatment of platform capitalism in science in <a class="citation" href="#mirowskiFutureOpenScience2018">[4]</a>)</p>

<p>As isolated services, one can imagine the practice of science devolving along a similar path as the increasingly-fragmented streaming video market: to do my work I need to subscribe to a data storage service, a cloud computing service, a platform to host my experiments, etc. For larger software platforms, however, vertical integration of multiple complementary services makes their impact on infrastructure more insidious. Locking users into more and more services makes for more and more revenue, which encourages platforms to be as mutually incompatible as they can get away with <a class="citation" href="#macinnesCompatibilityStandardsMonopoly2005">[28]</a>. To encourage adoption, platforms that can offer multiple services may offer one of the services – say, data storage – for free, forcing the user to use the adjoining services – say, a cloud computing platform.</p>

<p>Since these platforms are often subsidiaries of information industry monopolists, scientists become complicit in their ethically nightmarish behavior by funneling millions of dollars into, for example, the parent company of Elsevier and their surveillance technology agreement with ICE <a class="citation" href="#biddleLexisNexisProvideGiant2021">[29]</a>, or AWS and the laundry list of human rights abuses by Amazon <a class="citation" href="#CriticismAmazon2021">[30]</a>.</p>

<p>Structurally, the adoption of SaaS on a wide scale necessarily sacrifices the goals of an integrated mass infrastructure as the practice of research is carved into small, marketable chunks within vertically integrated technology platforms. Worse, it stands to amplify, rather than reduce, inequities in science, as the labs and institutes that are able to afford the tolls between each of the weigh stations of infrastructure are able to operate more efficiently, in turn begetting more funding, and the cycle spins ever faster.</p>

<p>Funding models and incentive structures in science are uniformly aligned towards the platformatization of scientific infrastructure. Aside from the tragic rhetoric of “technology transfer” that pervades the neoliberal university, the relative absence of major funding opportunities for scientific software developers competitive with the profit potential from “industry” often leaves it as the only viable career path. The preceding structural constraints on local infrastructural development strongly incentivize labs and researchers to rely on SaaS that provides a readymade solution to specific problems. Distressingly, rather than supporting infrastructural development that would avoid obligate payments to platform-holders, funding agencies seem all too happy to lean into them:</p>

<blockquote>
  <p>NIH will leverage what is available in the private sector, either through strategic partnerships or procurement, to create a workable Platform as a Service (PaaS) environment. […] NIH will partner with cloud-service providers for cloud storage, computational, and related infrastructure services needed to facilitate the deposit, storage, and access to large, high-value NIH datasets.</p>

  <p>These negotiations may result in partnership agreements with top infrastructure providers from U.S.-based companies whose focus includes support for research. Suitable cloud environments will house diverse data types and high-value datasets created with public funds. NIH will ensure that they are stable and adhere to stringent security requirements and applicable law, to protect against data compromise or loss. […] NIH’s cloud-marketplace initiative will be the first step in a phased operational framework that establishes a SaaS paradigm for NIH and its stakeholders. (-NIH Strategic Plan for Data Science, 2018 <a class="citation" href="#NIHStrategicPlan2018">[20]</a>)</p>
</blockquote>

<p>The articulated plan being to pay platform holders to house data while also paying for the labor to maintain those databases veers into parody, haplessly building another triple-pay industry <a class="citation" href="#buranyiStaggeringlyProfitableBusiness2017">[31]</a> into the economic system of science — one can hardly wait until they have the opportunity to rent their own data back with a monthly subscription.</p>

<p>!! this isn’t a metaphor – elsevier got the deal to build the analysis pipelining system using mendeley data <a class="citation" href="#ElsevierSevenBridges2017">[32]</a></p>

<p>!! and <a href="https://web.archive.org/web/20210729131920/https://cloud.nih.gov/">cloud.nih.gov</a> goes to the STRIDES program, which has cost $85 million since 2018 to establish, has a special account classification for <a href="https://web.archive.org/web/20211006003547/https://cloud.nih.gov/enrollment/account-type/">“extramural”</a> accounts that are researcher invoiced and managed:<br />
<a class="citation" href="#reillyNIHSTRIDESInitiative2021">[33]</a></p>

<p>!! Even on their success stories “We have been storing data in both cloud environments because we wanted the ecosystem we are creating to work on both clouds,” and they are developing their own overlay on top of it to bridge them! <a class="citation" href="#STRIDESInitiativeSuccess2020">[34]</a></p>

<p>It is unclear to me whether this is the result of the cultural hegemony of platform capitalism narrowing the space of imaginable infrastructures, industry capture of the decision-making process, or both, but the effect is the same in any case.</p>

<h3 id="protection-of-institutional-and-economic-power">Protection of Institutional and Economic Power</h3>

<p>The current state of deinfrastructuring certainly is not without its beneficiaries — those that have already accrued power and status within science. (I have already articulated the positive feedback loop of scientific funding, engineering costs, and prestige publishing and need to consolidate that here. The result of the protective nature of deinfrastructuring on concentrated power means that, barring some exogenous effort, we should not expect liberatory infrastructure to be developed by the places where the resources it requires are concentrated.)</p>

<p>!! Incentives misaligned across power and wealth strata — those at the top of the power hierarchy have every incentive to maintain the fragmentation that prevents people from competing (whether that be malicious and conscious or not). centralized infrastructure and information companies are 7 out of the 10 largest companies in the world (cite mozilla internet health), and if there are any goodhearted decentralized tech companies out there they’re simply swamped in size. The problem is that this kind of technology is specifically designed to not only be unprofitable, but anti-profitable: to make it impossible to profit from it. It’s designed to distribute power, which is a natural threat to concentrated power. This has practical consequences in terms of development resources, access (eg. publishing industry has mobilized hundreds of peopel to lobby librarians and CISOs across academia to implement multifactor authentication to kill sci-hub), and the kinds of tools that are developed.</p>

<p>!! So they have a lot of concerted power, but we have numbers. The problem is that in order to <em>use</em> those numbers have to have a plan and a system of organizing. This is actually useful too, because the first objection people have to big overhauls is that they will cost a lot of <em>money,</em> but the opposite is true: vastly more sums of money are spent reduplicating data that is lost, compensating for inefficient tooling, paying middlemen, etc.</p>

<h2 id="whose-job-is-infrastructure---the-ivies-institutes-consortia-and-the-rest-of-us">Whose Job is Infrastructure? - The Ivies, Institutes, Consortia, and “The Rest of Us”</h2>

<p>These constraints manifest differently depending on the circumstance of scientific practice. Differences in circumstance of practices also influence the kind of infrastructure developed, as well as where we should expect infrastructure development to happen as well as who benefits from it.</p>

<h3 id="institutional-core-facilities">Institutional Core Facilities</h3>

<p>Centralized “core” facilities are maybe the most typical form of infrastructure development and resource sharing at the level of departments and institutions. These facilities can range from minimal to baroque extravagance depending on institutional resources and whatever complex web of local history brought them about.</p>

<p><a href="https://projectreporter.nih.gov/project_info_details.cfm?aid=9444124">PNI Systems Core</a> lists <a href="https://reporter.nih.gov/project-details/9444124#sub-Projects">subprojects</a> echo a lot of the thoughts here, particularly around effort duplication<sup id="fnref:tymae" role="doc-noteref"><a href="#fn:tymae" class="footnote" rel="footnote">5</a></sup>:</p>

<blockquote>
  <p>Creating an Optical Instrumentation Core will  address the problem that much of the technical work required to innovate and maintain these instruments has  shifted to students and postdocs, because it has exceeded the capacity of existing staff. This division of  labor is a problem for four reasons: (1) lab personnel often do not have sufficient time or expertise to produce  the best possible results, (2) the diffusion of responsibility leads people to duplicate one another’s efforts, (3)  researchers spend their time on technical work at the expense of doing science, and (4) expertise can be lost  as students and postdocs move on. For all these reasons, we propose to standardize this function across  projects to improve quality control and efficiency. Centralizing the design, construction, maintenance, and  support of these instruments will increase the efficiency and rigor of our microscopy experiments, while  freeing lab personnel to focus on designing experiments and collecting data.</p>
</blockquote>

<p>While core facilities are an excellent way of expanding access, reducing redundancy, and standardizing tools within an instutition, as commonly structured they can displace work spent on those efforts outside of the institution. Elite institutions can attract the researchers with the technical knowledge to develop the instrumentation of the core and infrastructure for maintain it, but this development is only occasionally made usable by the broader public. The Princeton data science core is an excellent example of a core facility that does makes its software infrastructure development <a href="https://github.com/BrainCOGS">public</a><sup id="fnref:pnidatascience" role="doc-noteref"><a href="#fn:pnidatascience" class="footnote" rel="footnote">6</a></sup>, which they should be applauded for, but also illustrative of the problems with a core-focused infrastructure project. For an external user, the documentation and tutorials are incomplete – it’s not clear to me how I would set this up for my institute, lab, or data, and there are several places of hard-coded princeton-specific values that I am unsure how exactly to adapt<sup id="fnref:pnicaveat" role="doc-noteref"><a href="#fn:pnicaveat" class="footnote" rel="footnote">7</a></sup>. I would consider this example a high-water mark, and the median openness of core infrastructure falls far below it. I was unable to find an example of a core facility that maintained publicly-accessible documentation on the construction and operation of its experimental infrastructure or the management of its facility.</p>

<h3 id="centralized-institutes">Centralized Institutes</h3>

<p>Outside of universities, the Allen Brain Institute is perhaps the most impactful reflection of centralization in neuroscience. The Allen Institute has, in an impressively short period of time, created several transformative tools and datasets, including its well-known atlases <a class="citation" href="#leinGenomewideAtlasGene2007">[35]</a> and the first iteration of its <a href="http://observatory.brain-map.org/">Observatory</a> project which makes a massive, high-quality calcium imaging dataset of visual cortical activity available for public use. They also develop and maintain software tools like their <a href="https://allensdk.readthedocs.io/en/latest/">SDK</a> and Brain Modeling Toolkit <a href="https://alleninstitute.github.io/bmtk/">(BMTK)</a>, as well as a collection of <a href="https://portal.brain-map.org/explore/toolkit/hardware">hardware schematics</a> used in their experiments. The contribution of the Allen Institute to basic neuroscientific infrastructure is so great that, anecdotally, when talking about scientific infrastructure it’s not uncommon for me to hear something along the lines of “I thought the Allen was doing that.”</p>

<p>Though the Allen Institute is an excellent model for scale at the level of a single organization, its centralized, hierarchical structure cannot (and does not attempt to) serve as the backbone for all neuroscientific infrastructure. Performing single (or a small number of, as in its also-admirable <a href="https://alleninstitute.org/what-we-do/brain-science/news-press/articles/three-collaborative-studies-launch-openscope-shared-observatory-neuroscience">OpenScope Project</a>) carefully controlled experiments a huge number of times is an important means of studying constrained problems, but is complementary with the diversity of research questions, model organisms, and methods present in the broader neuroscientific community.</p>

<p>Christof Koch, its director, describes the challenge of centrally organizing a large number of researchers:</p>

<blockquote>
  <p>Our biggest institutional challenge is organizational: assembling, managing, enabling and motivating large teams of diverse scientists, engineers and technicians to operate in a highly synergistic manner in pursuit of a few basic science goals <a class="citation" href="#grillnerWorldwideInitiativesAdvance2016">[36]</a></p>
</blockquote>

<blockquote>
  <p>These challenges grow as the size of the team grows. Our anecdotal evidence suggests that above a hundred members, group cohesion appears to become weaker with the appearance of semi-autonomous cliques and sub-groups. This may relate to the postulated limit on the number of meaningful social interactions humans can sustain given the size of their brain <a class="citation" href="#kochBigScienceTeam2016">[37]</a></p>
</blockquote>

<p>!! These institutes are certainly helpful in building core technologies for the field, but they aren’t necessarily organized for developing mass-scale infrastructure.</p>

<h3 id="meso-scale-collaborations">Meso-scale collaborations</h3>

<p>Given the diminishing returns to scale for centralized organizations, many have called for smaller, “meso-scale” collaborations and consortia that combine the efforts of multiple labs <a class="citation" href="#mainenBetterWayCrack2016">[10]</a>. The most successful consortium of this kind has been the International Brain Laboratory <a class="citation" href="#abbottInternationalLaboratorySystems2017">[14, 7]</a>, a group of 22 labs spread across six countries. They have been able to realize the promise of big team neuroscience, setting a new standard for performing reproducible experiments performed by many labs <a class="citation" href="#laboratoryStandardizedReproducibleMeasurement2020">[38]</a> and developing data management infrastructure to match <a class="citation" href="#laboratoryDataArchitectureLargescale2020">[39]</a> (seriously, don’t miss their extremely impressive <a href="https://data.internationalbrainlab.org/">data portal</a>). Their project thus serves as the benchmark for large-scale collaboration and a model from which all similar efforts should learn from.</p>

<p>Critical to the IBL’s success was its adoption of a flat, non-hierarchical organizational structure, as described by Lauren E. Wool:</p>

<blockquote>
  <p>IBL’s virtual environment has grown to accommodate a diversity of scientific activity, and is supported by a flexible, ‘flattened’ hierarchy that emphasizes horizontal relationships over vertical management. […] Small teams of IBL members collaborate on projects in Working Groups (WGs), which are defined around particular specializations and milestones and coordinated jointly by a chair and associate chair (typically a PI and researcher, respectively). All WG chairs sit on the Executive Board to propagate decisions across WGs, facilitate operational and financial support, and prepare proposals for voting by the General Assembly, which represents all PIs. In parallel, associate chairs convene on their own committee to share decisions, which are then conveyed to the entire researcher community so it may weigh in on proposals before a formal vote. The interests of PIs and researchers intersect via staff liaisons who sit on both the Executive Board and the Associate Chairs Committee, as well as an elected researcher representative, who sits on the Executive Board and is a voting member of the General Assembly. <a class="citation" href="#woolKnowledgeNetworksHow2020">[7]</a></p>
</blockquote>

<p>They should also be credited with their adoption of a form of consensus decision-making, <a href="https://sociocracy.info">sociocracy</a>, rather than a majority-vote or top-down decisionmaking structure. Consensus decision-making systems are derived from those developed by <a href="https://rhizomenetwork.wordpress.com/2011/06/18/a-brief-history-of-consenus-decision-making/">Quakers and some Native American nations</a>, and emphasize, perhaps unsurprisingly, the value of collective consent rather than the will of the majority. Sociocracy specifically describes consent:</p>

<blockquote>
  <p>Consent means “no objections.” Giving consent does not mean unanimity, agreement, or even endorsement. Decisions are made to guide actions. Can we move forward if we make this decision? Consent is given in the context of moving forward. Consent to a policy decision means you believe that it is “worth trying.”  Or “I can work with it.” Moving forward is important for making better decisions because it provides more information. Not moving forward until a perfect decision is found, means operating in the blind. Information will always be limited to what is already known.</p>

  <p>Consent is required for all policy decisions for many reasons. The two most important are that it ensures (1) the decision will allow all members of the group to participate or produce without feeling oppressed, and (2) it will be supported by everyone. Everyone is expected to participate in the reasoning behind the decision. And no one can be excluded. https://www.sociocracy.info/what-is-sociocracy/</p>
</blockquote>

<p>The central lesson of the IBL, in my opinion, is that governance matters. Even if a consortium of labs were to form on an ad-hoc basis, without a formal system to ensure contributors felt heard and empowered to shape the project it would soon become unsustainable. Even if this system is not perfect, with some labor still falling unequally on some researchers, it is a promising model for future collaborative consortia.</p>

<p>The infrastructure developed by the IBL is impressive, but its focus on a single experiment makes it difficult to expand and translate to widescale use. The hardware for the IBL experimental apparatus is exceptionally well-documented, with a <a href="https://figshare.com/articles/preprint/A_standardized_and_reproducible_method_to_measure_decision-making_in_mice_Appendix_3_IBL_protocol_for_setting_up_the_behavioral_training_rig/11634732">complete and detailed build guide</a> and <a href="https://figshare.com/articles/online_resource/A_standardized_and_reproducible_method_to_measure_decision-making_in_mice_CAD_files_for_behavior_rig/11639973">library of CAD parts</a>, but the documentation is not modularized such that it might facilitate use in other projects, remixed, or repurposed. The <a href="https://github.com/int-brain-lab/iblrig">experimental software</a> is similarly single-purpose, a chimeric combination of Bonsai <a class="citation" href="#lopesBonsaiEventbasedFramework2015">[40]</a> and <a href="https://github.com/pybpod/pybpod">PyBpod</a> <a href="https://github.com/int-brain-lab/iblrig/tree/master/tasks/_iblrig_tasks_ephysChoiceWorld">scripts</a>. It unfortunately <a href="https://iblrig.readthedocs.io/en/latest/index.html">lacks</a> the API-level documentation that would facilitate use and modification by other developers, so it is unclear to me, for example, how I would use the experimental apparatus in a different task with perhaps slightly different hardware, or how I would then contribute that back to the library. The experimental software, according to the <a href="https://figshare.com/articles/preprint/A_standardized_and_reproducible_method_to_measure_decision-making_in_mice_Appendix_3_IBL_protocol_for_setting_up_the_behavioral_training_rig/11634732">PDF documentation</a>, will also not work without a connection to an <a href="https://github.com/cortex-lab/alyx">alyx</a> database. While alyx was intended for use outside the IBL, it still has <a href="https://github.com/cortex-lab/alyx/blob/07f481f6bbde668b81ad2634f4c42df4d6a74e44/alyx/data/management/commands/files.py#L188">IBL-specific</a> and <a href="https://github.com/cortex-lab/alyx/blob/07f481f6bbde668b81ad2634f4c42df4d6a74e44/alyx/data/fixtures/data.datasettype.json#L29">task-specific</a> values in its source-code, and makes community development difficult with a similar <a href="https://alyx.readthedocs.io/en/latest/">lack</a> of API-level documentation and requirement that users edit the library itself, rather than temporary user files, in order to use it outside the IBL.</p>

<p>My intention is not to denigrate the excellent tools built by the IBL, nor their inspiring realization of meso-scale collaboration, but to illustrate a problem that I see as an extension of that discussed in the context of core facilities — designing infrastructure for one task, or one group in particular makes it much less likely to be portable to other tasks and groups.</p>

<p>It is also unclear how replicable these consortia are, and whether they challenge, rather than reinforce technical inequity in science. Participating in consortia systems like the IBL requires that labs have additional funding for labor hours spent on work for the consortium, and in the case of graduate students and postdocs, that time can conflict with work on their degrees or personal research which are still far more potent instruments of “remaining employed in science” than collaboration. In the case that only the most well-funded labs and institutions realize the benefits of big team science without explicit consideration given to scientific equity, mesoscale collaborations could have the unintended consequence of magnifying the skewed distribution of access to technical expertise and instrumentation.</p>

<h3 id="the-rest-of-us">The rest of us…</h3>

<p>Outside of ivies with rich core facilities, institutes like the Allen, or nascent multi-lab consortia, the rest of us are largely on our own, piecing together what we can from proprietary and open source technology. The world of open source scientific software has plenty of energy and lots of excellent work is always being done, though constrained by the circumstances of its development described briefly above. Anything else comes down to whatever we can afford with remaining grant money, scrape together from local knowledge, methods sections, begging, borrowing, and (hopefully not too much) stealing from neighboring labs.</p>

<p>A third option from the standardization offered by centralization and the blooming, buzzing, beautiful chaos of disconnected open-source development is that of decentralized systems, and with them we might build the means by which the “rest of us” can mutually benefit by capturing and making use of each other’s knowledge and labor.</p>

<h1 id="a-draft-of-decentralized-scientific-infrastructure">A Draft of Decentralized Scientific Infrastructure</h1>

<p>Where do we go from here?</p>

<p>The decentralized infrastructure I will describe here is similar to previous notions of “grass-roots” science articulated within systems neuroscience <a class="citation" href="#mainenBetterWayCrack2016">[10]</a> but has broad and deep history in many domains of computing. My intention is to provide a more prescriptive scaffolding for its design and potential implementation as a way of painting a picture of what science could be like. This sketch is not intended to be final, but a starting point for further negotiation and refinement.</p>

<p>Throughout this section, when I am referring to any particular piece of software I want to be clear that I don’t intend to be dogmatically advocating that software <em>in particular</em>, but software <em>like it</em> that <em>shares its qualities</em> — no snake oil is sold in this document. Similarly, when I describe limitations of existing tools, without exception I am describing a tool or platform I love, have learned from, and think is valuable — learning from something can mean drawing respectful contrast!</p>

<h2 id="design-principles">Design Principles</h2>

<p>I won’t attempt to derive a definition of decentralized systems from base principles here, but from the systemic constraints described above, some design principles that illustrate the idea emerge naturally. For the sake of concrete illustration, in some of these I will additionally draw from the architectural principles of the internet protocols: the most successful decentralized digital technology project.</p>

<h3 id="protocols-not-platforms">Protocols, not Platforms</h3>

<p>Much of the basic technology of the internet was developed as <em>protocols</em> that describe the basic attributes and operations of a process. A simple and common example is email over SMTP (Simple Mail Transfer Protocol)<a class="citation" href="#Rfc5321SimpleMail">[41]</a>. SMTP describes a series of steps that email servers must follow to send a message: the sender initiates a connection to the recipient server, the recipient server acknowledges the connection, a few more handshake steps ensue to describe the senders and receivers of the message, and then the data of the message is transferred. Any software that implements the protocol can send and and receive emails to and from any other. The protocol basis of email is the reason why it is possible to send an email from a gmail account to a hotmail account (or any other hacky homebrew SMTP client) despite being wholly different pieces of software.</p>

<p>In contrast, <em>platforms</em> provide some service with a specific body of code usually without any pretense of generality. In contrast to email over SMTP, we have grown accustomed to not being able to send a message to someone using Telegram from WhatsApp, switching between multiple mutually incompatible apps that serve nearly identical purposes. Platforms, despite being <em>theoretically</em> more limited than associated protocols, are attractive for many reasons: they provide funding and administrative agencies a single point of contracting and liability, they typically provide a much more polished user interface, and so on. These benefits are short-lived, however, as the inevitable toll of lock-in and shadowy business models is realized.</p>

<h3 id="integration-not-invention">Integration, not Invention</h3>

<p>At the advent of the internet protocols, several different institutions and universities had already developed existing network infrastructures, and so the “top level goal” of IP was to “develop an effective technique for multiplex utilization of existing interconnected networks,” and “come to grips with the problem of integrating a number of separately administered entities into a common utility” <a class="citation" href="#clarkDesignPhilosophyDARPA1988">[42]</a>. As a result, IP was developed as a ‘common language’ that could be implemented on any hardware, and upon which other, more complex tools could be built. This is also a cultural practice: when the system doesn’t meet some need, one should try to extend it rather than building a new, separate system — and if a new system is needed, it should be interoperable with those that exist.</p>

<p>This point is practical as well as tactical: to compete, an emerging protocol should integrate or be capable of bridging with the technologies that currently fill its role. A new database protocol should be capable of reading and writing existing databases, a new format should be able to ingest and export to existing formats, and so on. The degree to which switching is seamless is the degree to which people will be willing to switch.</p>

<p>This principle runs directly contrary to the current incentives for novelty and fragmentation, which must be directly counterbalanced by design choices elsewhere to address the incentives driving them.</p>

<h3 id="embrace-heterogeneity-be-uncoercive">Embrace Heterogeneity, Be Uncoercive</h3>

<p>A reciprocal principle to integration with existing systems is to design the system to be integratable with existing practice. Decentralized systems need to anticipate unanticipated uses, and can’t rely on potential users making dramatic changes to their existing practices. For example, an experimental framework should not insist on a prescribed set of supported hardware and rigid formulation for describing experiments. Instead it should provide affordances that give a clear way for users to extend the system to fit their needs <a class="citation" href="#carpenterRFC1958Architectural1996">[43]</a>. In addition to integrating with existing systems, it must be straightforward for future development to be integrated. This idea is related to “the test of independent invention”, summarized with the question “if someone else had already invented your system, would theirs work with yours?” <a class="citation" href="#berners-leePrinciplesDesign1998">[44]</a>.</p>

<p>This principle also has tactical elements. An uncoercive system allows users to gradually adopt it rather than needing to adopt all of its components in order for any one of them to be useful. There always needs to be a <em>benefit</em> to adopting further components of the system to encourage <em>voluntary</em> adoption, but it should never be <em>compulsory.</em> For example, again from experimental frameworks, it should be possible to use it to control experimental hardware without needing to use the rest of the experimental design, data storage, and interface system. To some degree this is accomplished with a modular system design where designers are mindful of keeping the individual modules independently useful.</p>

<p>A noncoercive architecture also prioritizes the ease of leaving. Though this is somewhat tautological to protocol-driven design, specific care must be taken to enable export and migration to new systems. Making leaving easy also ensures that early missteps in development of the system are not fatal to its development, preventing lock-in to a component that needs to be restructured.</p>

<p>!! the coercion of centralization has a few forms. this is related to the authoritarian impulse in the open science movement that for awhile bullied people into openness. that instinct in part comes from a belief that everyone should be doing the same thing, should be posting their work on the one system. decentralization is about autonomy, and so a reciprocal approach is to make it easy and automatic.</p>

<h3 id="empower-people-not-systems">Empower People, not Systems</h3>

<p>Because IP was initially developed as a military technology by DARPA, a primary design constraint was survivability in the face of failure. The model adopted by internet architects was to move as much functionality from the network itself to the end-users of the network — rather than the network itself guaranteeing a packet is transmitted, the sending computer will do so by requiring a response from the recipient <a class="citation" href="#clarkDesignPhilosophyDARPA1988">[42]</a>.</p>

<p>For infrastructure, we should make tools that don’t require a central team of developers to maintain, a central server-farm to host data, or a small group of people to govern. Whenever possible, data, software, and hardware should be self-describing, so one needs minimal additional tools or resources to understand and use it. It should never be the case that funding drying up for one node in the system causes the entire system to fail.</p>

<p>Practically, this means that the tools of digital infrastructure should be deployable by individual people and be capable of recapitulating the function of the system without reference to any central authority. Researchers need to be given control over the function of infrastructure: from controlling sharing permissions for eg. clinically sensitive data to assurance that their tools aren’t spying on them. Formats and standards must be negotiable by the users of a system rather than regulated by a central governance body.</p>

<h3 id="infrastructure-is-social">Infrastructure is Social</h3>

<p>The alternative to centralized governing and development bodies is to build the tools for community control over infrastructural components. This is perhaps the largest missing piece in current scientific tooling. On one side, decentralized governance is the means by which an infrastructure can be maintained to serve the ever-evolving needs of its users. On the other, a sense of community ownership is what drives people to not only adopt but contribute to the development of an infrastructure. In addition to a potentially woo-woo sense of socially affiliative “community-ness,” any collaborative system needs a way of ensuring that the practice of maintaining, building, and using it is designed to <em>visibly and tangibly benefit</em> those that do, rather than be relegated to a cabal of invisible developers and maintainers <a class="citation" href="#grudinGroupwareSocialDynamics1994">[45, 46]</a>.</p>

<p>Governance and communication tools also make it possible to realize the infinite variation in application that infrastructures need while keeping them coherent: tools must be built with means of bringing the endless local conversations and modifications of use into a common space where they can become a cumulative sense of shared memory.</p>

<p>This idea will be given further treatment and instantiation in a later discussion of the social dynamics of private bittorrent trackers, and is necessarily diffuse because of the desire to not be authoritarian about the structure of governance.</p>

<h3 id="usability-matters">Usability Matters</h3>

<p>It is not enough to build a technically correct technology and assume it will be adopted or even useful, it must be developed embedded within communities of practice and <em>be useful for solving problems that people actually have.</em> We should learn from the struggles of the semantic web project. Rather than building a fully prescriptive and complete system first and instantiating it later, we should develop tools whose usability is continuously improved <em>en route</em> to a (flexible) completed vision.</p>

<p>The adage from RFC 1958 “nothing gets standardized until there are multiple instances of running code” <a class="citation" href="#carpenterRFC1958Architectural1996">[43]</a> captures the dual nature of the constraint well. Workable standards don’t emerge until they have been extensively tested in the field, but development without an eye to an eventual protocol won’t make one.</p>

<p>We should read the <a href="https://en.wikipedia.org/wiki/Embrace,_extend,_and_extinguish">gobbling up</a> of open protocols into proprietary platforms that defined “Web 2.0” as instructive (in addition to a demonstration of the raw power of concentrated capital) <a class="citation" href="#markoffTomorrowWorldWide1996">[47]</a>. <em>Why</em> did Slack outcompete IRC? The answer is relatively simple: it was relatively simple to use. Using a contemporary example, to <a href="https://matrix-org.github.io/synapse/latest/setup/installation.html">set up a Synapse server</a> to communicate over <a href="https://matrix.org/docs/spec/">Matrix</a> one has to wade through dozens of shell commands, system-specific instructions, potential conflicts between dependent packages, set up an SQL server… and that’s just the backend, we don’t even have a frontend client yet! In contrast, to use Slack you download the app, give it your email, and you’re off and running.</p>

<p>The control exerted by centralized systems over their system design does give certain structural advantages to their usability, and their for-profit model gives certain advantages to their development process. There is no reason, however, that decentralized systems <em>must</em> be intrinsically harder to use, we just need to focus on user experience to a comparable degree that centralized platforms: if it takes a college degree to turn the water on, that ain’t infrastructure.</p>

<p>People are smart, they just get frustrated easily. We have to raise our standards of design such that we don’t expect users to have even a passing familiarity with programming, attempting to build tools that are truly general use. We can’t just design a peer-to-peer system, we need to make the data ingestion and annotation process automatic and effortless. We can’t just build a system for credit assignment, it needs to happen as an automatic byproduct of using the system. We can’t just make tools that <em>work,</em> they need to <em>feel good to use.</em></p>

<p>Centralized systems also have intrinsic limitations that provide openings for decentralized systems, like cost, incompatibility with other systems, inability for extension, and opacity of function. The potential for decentralized systems to capture the independent development labor of all of its users, rather than just that of a core development team, is one means of competition. If the barriers to adoption can be lowered, and the benefits raised these constant negative pressures of centralization might overwhelm intertia.</p>

<p>With these principles in mind, and drawing from other knowledge communities solving similar problems: internet infrastructure, library/information science, peer-to-peer networks, and radical community organizers, I conceptualize a system of distributed infrastructure for systems neuroscience as three objectives: <a href="#shared-data"><strong>shared data</strong></a>, <a href="#shared-tools"><strong>shared tools</strong></a>, and <a href="#shared-knowledge"><strong>shared knowledge</strong></a>.</p>

<h2 id="shared-data">Shared Data</h2>

<h3 id="formats-as-onramps">Formats as Onramps</h3>

<p>The shallowest onramp towards a generalized data infrastructure is to make use of existing discipline-specific standardized data formats. As will be discussed later, a truly universal pandisciplinary format is effectively impossible, but to arrive at the alternative we should first congeal the wild west of unstandardized data into a smaller number of established formats.</p>

<p>Data formats consist of some combination of an abstract specification, an implementation in a particular storage medium, and an API for interacting with the format. I won’t dwell on the particular qualities that a particular format needs, assuming that most that would be adopted would abide by FAIR principles. For now we assume that the particular constellation of these properties that make up a particular format will remain mostly intact with an eye towards semantically linking specifications and unifying their implementation.</p>

<p>There are a dizzying number of scientific data formats <a class="citation" href="#teamScientificDataFormats">[48]</a>, so a comprehensive treatment is impractical here and I will use the Neurodata Without Borders:N (NWB)<a class="citation" href="#rubelNWBAccessibleData2019a">[49]</a> as an example. NWB is the de facto standard for systems neuroscience, adopted by many institutes and labs, though far from uniformly. NWB <a href="https://www.nwb.org/nwb-software/">consists of</a> a <a href="https://schema-language.readthedocs.io/en/stable/">specification language</a>, a <a href="https://nwb-schema.readthedocs.io/en/stable/">schema written in that language</a>, a <a href="https://nwb-storage.readthedocs.io/en/stable/">storage implementation in hdf5</a>, and an <a href="https://pynwb.readthedocs.io/en/stable/">API for interacting with the data</a>. They have done an admirable job of engaging with community needs <a class="citation" href="#rubelNeurodataBordersEcosystem2021">[50]</a> and making a modular, extensible format ecosystem.</p>

<p>The major point of improvement for NWB, and I imagine many data standards, is the ease of conversion. The conversion API requires extensive programming, knowledge of the format, and navigation of several separate tutorial documents. This means that individual labs, if they are lucky enough to have some partially standardized format for the lab, typically need to write (or hire someone to write) their own <a href="https://github.com/catalystneuro/tank-lab-to-nwb">software</a> <a href="https://github.com/catalystneuro/mease-lab-to-nwb">library</a> for conversion.</p>

<p>Without being prescriptive about its form, substantial interface development is needed to make mass conversion possible. It’s usually untrue that unstandardized data had <em>no structure,</em> and researchers are typically able to articulate it – “the filenames have the data followed by the subject id,” and so on. Lowering the barriers to conversion mean designing tools that match the descriptive style of folk formats, for example by prompting them to describe where each of an available set of metadata fields are located in their data. It is not an impossible goal to imagine a piece of software that can be downloaded and with minimal recourse to reference documentation allow someone to convert their lab’s data within an afternoon. The barriers to conversion have to be low and the benefits of conversion have to outweigh the ease of use from ad-hoc and historical formats.</p>

<p>NWB also has an extension interface, which allows, for example, common data sources to be more easily described in the format. These are registered in an <a href="https://nwb-extensions.github.io/">extensions catalogue</a>, but at the time of writing it is relatively sparse. The preponderance of lab-specific conversion packages relative to extensions is indicative of an interface and community tools problem: presumably many people are facing similar conversion problems, but because there is not a place to share these techniques in a human-readable way, the effort is duplicated in dispersed codebases. We will return to some possible solutions for knowledge preservation and format extension when we discuss tools for <a href="#shared-knowledge">shared knowledge</a>.</p>

<p>For the sake of the rest of the argument, let us assume that some relatively trivial conversion process exists to subdomain-specific data formats and we reach some reasonable penetrance of standardization. The interactions with the other pieces of infrastructure that may induce and incentivize conversion will come later.</p>

<h3 id="peer-to-peer-as-a-backbone">Peer-to-peer as a Backbone</h3>

<p>We should adopt a <em>peer-to-peer</em> system for storing and sharing scientific data. There are, of course <a href="https://www.dandiarchive.org/">many</a> <a href="https://openneuro.org/">existing</a> <a href="https://www.brainminds.riken.jp/">databases</a> <a href="https://biccn.org/">for</a> scientific data, ranging from domain-general like <a href="https://figshare.com/">figshare</a> and <a href="https://zenodo.org/">zenodo</a> to the most laser-focused subdiscipline-specific. The notion of a database, like a data standard, is not monolithic. As a simplification, they consist of at least the hardware used for storage, the software implementation of read, write, and query operations, a formatting schema, some API for interacting with it, the rules and regulations that govern its use, and especially in scientific databases some frontend for visual interaction. For now we will focus on the storage software and read-write system, returning to the format, regulations, and interface later.</p>

<p>Centralized servers are fundamentally constrained by their storage capacity and bandwidth, both of which cost money. In order to be free, database maintainers need to constantly raise money from donations or grants<sup id="fnref:grantdb" role="doc-noteref"><a href="#fn:grantdb" class="footnote" rel="footnote">8</a></sup> in order to pay for both. Funding can never be infinite, and so inevitably there must be some limit on the amount of data that someone can upload and the speed at which it can serve files<sup id="fnref:osfspeed" role="doc-noteref"><a href="#fn:osfspeed" class="footnote" rel="footnote">9</a></sup>. In the case that a researcher never sees any of those costs, they are still being borne by some funding agency, incurring the social costs of funneling money to database maintainers. Centralized servers are also intrinsically out of the control of their users, requiring them to abide whatever terms of use the server administrators set. Even if the database is carefully backed up, it serves as a single point of infrastructural failure, where if the project lapses then at worst data will be irreversibly lost, and at best a lot of labor needs to be expended to exfiltrate, reformat, and rehost the data. The same is true of isolated, local, institutional-level servers and related database platforms, with the additional problem of skewed funding allocation making them unaffordable for many researchers.</p>

<p>Peer-to-peer (p2p) systems solve many of these problems, and I argue are the only type of technology capable of making a database system that can handle the scale of all scientific data. There is an enormous degree of variation between p2p systems<sup id="fnref:p2pdiscipline" role="doc-noteref"><a href="#fn:p2pdiscipline" class="footnote" rel="footnote">10</a></sup>, but they share a set of architectural advantages. The essential quality of any p2p system is that rather than each participant in a network interacting only with a single server that hosts all the data, everyone hosts data and interacts directly with each other.</p>

<p>For the sake of concreteness, we can consider a (simplified) description of Bittorrent <a class="citation" href="#cohenBitTorrentProtocolSpecification2017">[52]</a>, arguably the most successful p2p protocol. To share a collection of files, a user creates a <code class="language-plaintext highlighter-rouge">.torrent</code> file which consists of a <a href="https://en.wikipedia.org/wiki/Cryptographic_hash_function">cryptographic hash</a>, or a string that is unique to the collection of files being shared; and a list of “trackers.” A tracker, appropriately, keeps track of the <code class="language-plaintext highlighter-rouge">.torrent</code> files that have been uploaded to it, and connects users that have or want the content referred to by the <code class="language-plaintext highlighter-rouge">.torrent</code> file. The uploader (or seeder) then leaves a <a href="https://en.wikipedia.org/wiki/Glossary_of_BitTorrent_terms#Client">torrent client</a> open waiting for incoming connections. Someone who wants to download the files (a leecher) will then open the <code class="language-plaintext highlighter-rouge">.torrent</code> file in their client, which will then ask the tracker for the IP addresses of the other peers who are seeding the file, directly connect to them, and begin downloading. So far so similar to standard client-server systems, but the magic is just getting started. Say another person wants to download the same files before the first person has finished downloading it: rather than <em>only</em> downloading from the original seeder, the new leecher downloads from <em>both</em> the original seeder and the first leecher. Leechers are incentivized to share among each other to prevent the seeders from spending time reuploading the pieces that they already have, and once they have finished downloading they become seeders themselves.</p>

<p>From this very simple example, a number of qualities of p2p systems become clear.</p>

<ul>
  <li>First, the system is extremely <strong>inexpensive to maintain</strong> since it takes advantage of the existing bandwidth and storage space of the computers in the swarm, rather than dedicated servers. Near the height of its popularity in 2009, The Pirate Bay, a notorious bittorrent tracker, was estimated to cost $3,000 per month to maintain while serving approximately 20 million peers <a class="citation" href="#roettgersPirateBayDistributing2009">[53]</a>. According to a database dump from 2013 <a class="citation" href="#PirateBayArchiveteam2020">[54]</a>, multiplying the size of each torrent by the number of seeders (ignoring any partial downloads from leechers), the approximate instantaneous storage size of The Pirate Bay was ~26 Petabytes. The comparison to centralized services is not straightforward, since it is hard to evaluate the distributed costs of additional storage media (as well as the costs avoided by being able to take advantage of existing storage infrastructure within labs and institutes), but for the sake of illustration: hosting 26PB would cost $546,000/month with standard AWS S3 hosting ($0.021/GB/month).</li>
  <li>The <strong>speed</strong> of a bittorrent swarm <em>increases,</em> rather than decreases, the more people are using it since it is capable of using all of the available bandwidth in the system.</li>
  <li>The network is extremely <strong>resilient</strong> since the data is shared across many independent peers in the system. If our goal is to make a resilient and robust data architecture, we would benefit by paying attention to the tools used in the broader archival community, especially the archival communities that especially need resilience because their archives are frequent targets of governments and IP-holders<a class="citation" href="#spiesDataIntegrityLibrarians2017">[55]</a>.  Despite more than 15 years of concerted effort by governments and intellectual property holders, the pirate bay is still alive and kicking <a class="citation" href="#kim15YearsPirate2019">[56]</a><sup id="fnref:knockin" role="doc-noteref"><a href="#fn:knockin" class="footnote" rel="footnote">11</a></sup>. This is because even if the entire infrastructure of the tracker is destroyed, as it was in 2006, the files are distributed across all of its users, the actual database of <code class="language-plaintext highlighter-rouge">.torrent</code> metadata is quite small, and the tracker software is extraordinarily simple to rehost <a class="citation" href="#vandersarOpenBayNow2014">[57]</a> – The Pirate Bay was back online in 2 days.  When another tracker, what.cd (which we will return to <a href="#archives-need-communities">soon</a>) was shut down, a series of successors popped up using the open source tools <a href="https://github.com/WhatCD/Gazelle">Gazelle</a> and <a href="https://github.com/WhatCD/Ocelot">Ocelot</a> that what.cd developers built. Within two weeks, one successor site had recovered and reindexed 200,000 of its torrents resubmitted by former users <a class="citation" href="#vandersarWhatCdDead2016">[58]</a>. Bittorrent is also used by archival groups with little funding like <a href="https://wiki.archiveteam.org/index.php/Main_Page">Archive Team</a>, who struggled – but eventually succeeded – to disseminate their <a href="https://wiki.archiveteam.org/index.php/GeoCities_Project">historic preservation</a> over a single “crappy cable modem” <a class="citation" href="#scottGeocitiesTorrentUpdate2010">[59]</a>. And by groups who disseminate !! return here talking about ddosevrets.</li>
  <li>The network is extremely <strong>scalable</strong> since there is no cost to connecting new peers and the users of a system expand the storage capacity of the system depending on their needs. Rather than having one extremely fast data center (or a privatized network designed to own the internet), the model of p2p systems is to leverage many approachable peer/servers.</li>
</ul>

<p>Peer-to-peer systems are not mutually exclusive with centralized servers: servers are peers too, after all. A properly implemented will always be <em>at least</em> as fast and have <em>at least</em> as much storage as any alternative centralized centralized server because peers can use <em>both</em> the bandwidth of the server <em>and</em> that of any peers that have the file. In the bittorrent ecosystem large-bandwidth/storage peers are known as “seedboxes”<a class="citation" href="#rossiPeekingBitTorrentSeedbox2014">[60]</a> when they use the bittorrent protocol, and “web seeds”<a class="citation" href="#hoffmanHTTPBasedSeedingSpecification">[61]</a> when they use a protocol built on top of traditional HTTP. <a href="https://archive.org">Archive.org</a> has been distributing all of its materials <a href="https://archive.org/details/bittorrent">with bittorrent</a> by using its servers as web seeds since 2012 and makes this point explicitly: “BitTorrent is now the fastest way to download items from the Archive, because the Bittorrent client downloads simultaneously from two different Archive servers located in two different datacenters, and from other Archive users who have downloaded these Torrents already.” <a class="citation" href="#kahle000000Torrents2012">[62]</a></p>

<p>p2p systems complement centralized servers in a number of ways beyond raw download speed, increasing the efficiency and performance of the network as a whole. Spotify began as a joint client/server and p2p system <a class="citation" href="#kreitzSpotifyLargeScale2010b">[63]</a>, where when a listener presses play the central server provides the data until peers that have the song cached are found by the p2p system to download the rest of the song from. The central server is able to respond quickly and reliably to so the song is played as quickly as possible, and is the server of last resort in the case of rare files that aren’t being shared by anyone else in the network. A p2p system complements the server and makes that possible by alleviating pressure on the server for more predictable traffic.</p>

<p>A peer to peer system is a particularly natural fit for many of the common circumstances and practices in science, where centralized server architectures seem (and prove) awkward and inefficient. Most labs, institutes, or other organized bodies of science have some form of local or institutional storage systems. In the most frequent cases of sharing data within a lab or institute, sending it back and forth to some nationally-centralized server is like walking across the lab by going the long way around the Earth. That’s the method invoked by a Dropbox or AWS link, but in the absence of a formal one you can always revert to a low-fi p2p transfer: walking a flash drive across the lab. The system makes less sense when several people in the same place need to access the same data at the same time, as is frequently the case with multi-lab collaborations, or scientific conferences and workshops. Instead of needing to wait on the 300kb/s conference wifi bandwidth as it’s cheese-gratered across every machine, we instead could directly beam it between all computers in range simultaneously, full blast through the decrepit network switch that won’t have seen that much excitement in years.</p>

<p>!! if we take the suggestion of Andrey Andreev et al. and invest in server clusters within institutes <a class="citation" href="#andreevBiologistsNeedModern2021">[64, 65]</a>, their impact could be multiplied manyfold by being able to use them all fluidly and simultaneously for file transfer and storage. !! compatible and extends calls for more institutional support for storage liek andreev’s paper, but satisfies the need for generalized storage systems that the NIH doesn’t have to develop a whole new institute to handle. extra bonus! in that system each server would have to serve the entire file each time. WIth p2p then the load can be spread between all of them, decreasing costs for all institutions!!!!</p>

<p>So far I have relied on the Extraordinarily Simplified Bittorrent™️ depiction of a peer to peer system, but there are many improvements and variants that can address different needs for scientific data infrastructure.</p>

<p>One obvious need that bittorrent can’t currently support is version control, but more recent p2p systems do. 
<a href="https://ipfs.io/">IPFS</a> functions like “a single BitTorrent swarm, exchanging objects within one Git repository.” <a class="citation" href="#benetIPFSContentAddressed2014">[66]</a><sup id="fnref:whatsgit" role="doc-noteref"><a href="#fn:whatsgit" class="footnote" rel="footnote">12</a></sup> Dat <a class="citation" href="#ogdenDatDistributedDataset2017">[67]</a>, specifically designed for data synchronization and versioning, handles versioning and more. A full description of IPFS is out of scope, and it has plenty of problems <a class="citation" href="#patsakisHydrasIPFSDecentralised2019">[68]</a>, but for now sufficent to say p2p systems can handle version control.</p>

<p>Bittorrent swarms are vulnerable to data loss if all the peers seeding a file disconnect (though the tail is longer than typically assumed, see <a class="citation" href="#zhangUnravelingBitTorrentEcosystem2011">[69]</a>), but this too can be addressed with updated p2p system design. A first-order solution to this problem is a variant of IPFS’ notion of ‘pinning.’ Since backup to lab-level or institutional servers is already commonplace, one peer could be able to ‘pin’ another and automatically download all the data that they share. This concept could scale to institutes and national infrastructure as scientists can request the datasets they’d like to be saved permanently be pinned.</p>

<p>Another could be something akin to Freenet <a class="citation" href="#clarkeFreenetDistributedAnonymous2001">[70]</a>. Peers could allocate a certain amount of their unused storage space to be used to automatically download, cache, and rehost shards of other datasets. Distributing chunks and encrypting them at rest so the rehoster can’t inspect their contents would make it possible to maintain privacy and network availability for sensitive data (see, for example, <a href="https://inqlab.net/projects/eris/">ERIS</a>). IPFS has an analogous concept – BitSwap – that is makes it into a barter system. Peers who seek to download will have to ‘earn’ it by finding some chunk of data that the other peers want, download, and share them, though it seems like an empirical question whether or not a barter system works or is necessary.</p>

<p><a href="https://solidproject.org/">Solid</a> is a project that almost exactly meets all these needs <a class="citation" href="#capadisliSolidProtocol2020">[71, 72, 73]</a>. Solid allows people to share data in <a href="https://solidproject.org/about">Pods</a>, which let them control access and distribution across storage system with a unified identity system. It is implementation-agnostic, and so can support peer-to-peer storage and transfer systems that comply with its <a href="https://solidproject.org/TR/protocol">protocol specification</a>.</p>

<p>There are a number of additional requirements for a peer to peer scientific data infrastructure, but even these seemingly very technical problems of versioning and distributed storage show the clear need to consider the structure of the surrounding social system. What control do we give to researchers over the version history of their data? Should people that aren’t the originating researcher be able to issue new versions? What structure of distributed/centralized storage works? How should we incentivize sharing of excess storage and resources?</p>

<p>Even before considering additional social systems, a peer to peer structure in itself implies a different relationship to a generalized data infrastructure. Scientists always unavoidably make their data available to at least one person: themselves; on at least one computer: theirs. A peer-to-peer backbone for scientific infrastructure is the unnecessarily radical notion that everyday practices like these can make up our infrastructure, rather than having it exist exogenously as something “out there.” Subtly, it’s the notion that our infrastructure can reflect and consist of <em>ourselves</em> instead of something out of our control that we need to buy from someone else.</p>

<p>Scientists don’t need to reinvent the notion of distributed, community curated data archives from scratch. In addition to scholarly work on the social systems of digital infrastructure, we can learn from communities of practice, and there has been no more important and impactful decentralized archival project than internet piracy.</p>

<h3 id="archives-need-communities">Archives Need Communities</h3>

<p>Why do hundreds of thousands of people, completely anonymously, with zero compensation, spend their time to do something that is as legally risky as curating pirated cultural archives?</p>

<p>Scholarly work, particularly from Economics, tends to focus on understanding piracy in order to prevent it<a class="citation" href="#basamanowiczReleaseGroupsDigital2011">[74, 75]</a>, taking the moral good of intellectual property markets as an <em>a priori</em> imperative and investigating why people behave <em>badly</em> and “rend [the] moral fabric associated with the respect of intellectual property.” <a class="citation" href="#hindujaDeindividuationInternetSoftware2008">[75]</a>. If we put the legality of piracy aside, we may find a wealth of wisdom and insight to draw from for building scientific infrastructure.</p>

<p>The world of digital piracy is massive, from entirely disorganized efforts of individual people on public sites to extraordinarily organized release groups <a class="citation" href="#basamanowiczReleaseGroupsDigital2011">[74]</a>, and so a full consideration is out of scope, but many of the important lessons are taught by the structure of bittorrent trackers.</p>

<p>An underappreciated element of the BitTorrent protocol is the effect of the separation between the data transfer protocol and the ‘discovery’ part of the system — or “overlay” — on the community structure of torrent trackers (for a more complete picture of the ecosystem, see <a class="citation" href="#zhangUnravelingBitTorrentEcosystem2011">[69]</a>). Many peer to peer networks like KaZaA or the gnutella-based Limewire had searching for files integrated into the transfer interface. The need for torrent trackers to share .torrent files spawned a massive community of private torrent trackers that for decades have been iterating on cultures of archival, experimenting with different community structures and incentives that encourage people to share and annotate some of the world’s largest, most organized libraries.</p>

<p>One of these private trackers was the site of one of the largest informational tragedies of the past decade: what.cd<sup id="fnref:whatdiss" role="doc-noteref"><a href="#fn:whatdiss" class="footnote" rel="footnote">13</a></sup>, which I will use as an example to describe some of these community systems.</p>

<p>What.cd was a bittorrent tracker that was arguably the largest collection of music that has ever existed. At the time of its destruction in 2016, it was host to just over one million unique releases, and approximately 3.5 million torrents<sup id="fnref:dbsize" role="doc-noteref"><a href="#fn:dbsize" class="footnote" rel="footnote">14</a></sup> <a class="citation" href="#dunhamWhatCDLegacy2018">[76]</a>. Every torrent was organized in a meticulous system of metadata communally curated by its roughly 200,000 global users. The collection was built by people who cared deeply about music, rather than commercial collections provided by record labels notorious for ceasing distribution of recordings that are not commercially viable — or just losing them in a fire <a class="citation" href="#rosenDayMusicBurned2019">[77]</a><sup id="fnref:lostartists" role="doc-noteref"><a href="#fn:lostartists" class="footnote" rel="footnote">15</a></sup>. Users would spend large amounts of money to find and digitize extremely rare recordings, many of which were unavailable anywhere else and are now unavailable anywhere, period. One former user describes one example:</p>

<blockquote>
  <p>“I did sound design for a show about Ceaușescu’s Romania, and was able to pull together all of this 70s dissident prog-rock and stuff that has never been released on CD, let alone outside of Romania” <a class="citation" href="#sonnadEulogyWhatCd2016">[78]</a></p>
</blockquote>

<p><img src="/infrastructure/assets/images/kanye-what.png" alt="A what.cd artist page (Kanye west) that shows each of his albums having perhaps a dozen different torrents: each time the album was released, on cd, vinyl, and web, each in multiple different audio formats." />
<em>The what.cd artist page for Kanye West (taken from <a href="https://qz.com/840661/what-cd-is-gone-a-eulogy-for-the-greatest-music-collection-in-the-world/">here</a> in the style of pirates, without permission). For the album “Yeezus,” there are ten torrents, grouped by each time the album was released on CD and Web, and in multiple different qualities and formats (.flac, .mp3). Along the top is a list of the macro-level groups, where what is in view is the “albums” section, there are also sections for bootleg recordings, remixes, live albums, etc.</em></p>

<p>What.cd was a “private” bittorrent tracker, where unlike public trackers that anyone can access, membership was strictly limited to those who were personally invited or to those who passed an interview (for more on public and private tracker, see <a class="citation" href="#meulpolderPublicPrivateBitTorrent">[79]</a>). Invites were extremely rare, and the interview process was demanding to the point where <a href="https://opentrackers.org/whatinterviewprep.com/index.html">entire guides</a> were written to prepare for them. When I interviewed in 2009, I had to find my way onto an obscure IRC server, wait in a lobby all day until a volunteer moderator could get to me, and was then grilled on the arcana of digital music formats, spectral analysis<sup id="fnref:spectral" role="doc-noteref"><a href="#fn:spectral" class="footnote" rel="footnote">16</a></sup>, the ethics of piracy, and so on for half an hour. Getting a question wrong was an instant failure and you were banned from the server for 48 hours. A single user was only allowed one account per lifetime, so between that policy and the extremely high barriers to entries, even anonymous users were strongly incentivized to follow <a href="https://opentrackers.org/whatinterviewprep.com/prepare-for-the-interview/what-cd-rules/index.html">the sophisticated, exacting rules for contributing</a>. While we certainly don’t want such a grueling barrier to entry for scientific data infrastructure, the problem is different and arguably simpler when the system can exist in the open. For example public reputation loss can be a reasonably strong incentive to play by the rules that may trade off with the threat of banning.</p>

<p>The what.cd incentive system was based on a required ratio of data uploaded vs. data downloaded <a class="citation" href="#jiaHowSurviveThrive2013">[80]</a>. Peer to peer systems need to overcome a free-rider problem where users might download a torrent (“leeching”) and turn their computer off, rather than leaving their connection open to share it to others (or, “seeding”). In order to download additional music, then, one would have to upload more. Since downloading is highly restricted, and everyone is trying to upload as much as they can, torrents had a large number of “seeders,” and even rare recordings would be sustained for years, a pattern common to private trackers <a class="citation" href="#liuUnderstandingImprovingRatio2010">[81]</a>.</p>

<p>The high seeder/leecher ratio made it so it was extremely difficult to acquire upload credit, so users were additionally incentivized to find and upload new recordings to the system. What.cd implemented a “bounty” system, where users with a large amount of excess upload credit would be able to offer some of it to whoever was able to upload the album they wanted. To “prime the pump” and keep the economy moving, highlight artists in an album of the week, or direct users to preserve rare recordings, moderators would also use a “freeleech” system, where users would be able to download a specified set of torrents without it counting against their download quantity <a class="citation" href="#kashEconomicsBitTorrentCommunities2012">[82, 83]</a>.</p>

<p>Depending on the age of your account and the amount you had contributed, what.cd users also were given <em>user classes</em> that conferred differing degrees of prestige and abilities. This is a common tactic for publicly moderates sites like <a href="https://stackexchange.com">StackExchange</a> or <a href="https://genius.com">Genius</a>, where users need to demonstrate a certain degree of competency and good faith before they are given the keys to the castle. User classes are both <em>aspirational</em> and incentivize additional work on the site, as well as <em>reputational</em> where a user class meant you have paid your dues and were a senior contributor.</p>

<p>The other half of what.cd was the more explicitly social elements: its forums, comment sections, and moderation systems. The forum was home to roiling debates that lasted years about the structure of some tagging schema, whether one genre was just another with a different name, and so on. The structure of the community was an object of constant, public negotiation, and over time the metadata system evolved to be able to support a library of the entirety of human music output<sup id="fnref:subtlety" role="doc-noteref"><a href="#fn:subtlety" class="footnote" rel="footnote">17</a></sup>, and the rules and incentive structures were made to align with building it. To support the good operation of the site, the forums were also home to a huge amount of technical knowledge, like guides on how to make a perfect upload, that eased new users into being able to use the system.</p>

<p>A critical problem in maintaining coherent databases is correcting metadata errors and departures from schemas. Finding errors was rewarded. Users were able to discuss and ask questions of the uploader in a comment section below each upload, which would allow “polite” resolution of low-level errors like typos. More serious problems could be reported to the moderation team, which caused the upload to be visibly marked as under review, and the report could then be discussed either in the comment sections or the forum. If the moderation team affirmed your report, they would usually kick back a few gigabytes of upload credit depending on the severity. Unless the problem was a repeat and malicious one, the “offender” was alerted to it, warned, and told what to do instead next time – though, being an anonymous, gray-area community, there was plenty of power that was tripped on. Rather than being a messy hodgepodge of fake, low-quality uploads, what.cd was always teetering just shy of perfection.</p>

<p>These structural considerations do not capture the most elusive but indisputably important features of what.cd’s community infrastructure: <em>the sense of commmunity</em>. The What.cd forums were the center of many user’s relationships to music. Threads about all the finest scales of music nichery could last for years: it was a rare place people who probably cared a little bit too much about music could talk to people with the same condition. What made it more satisfying than other music forums was that no matter what music you were talking about, everyone else in the conversation would always have access to it if they wanted to hear it. Independent musicians released albums in the supportive<sup id="fnref:mostly" role="doc-noteref"><a href="#fn:mostly" class="footnote" rel="footnote">18</a></sup> Vanity House section, and people from around the world came to hold the one true album that only they knew about high aloft like a divine tablet. Beyond any structural incentives, people spent so much time building and maintaining what.cd because it became a source of community and a sink of personal investment.</p>

<p>Structural norms supported by social systems converge as a sort of <em>reputational</em> incentive. Uploading a new album to fill a bounty both makes the network more functional and complete, but it also <em>people respect you for it</em> because it’s prominently displayed on your profile as well as in the bounty charts and that <em>feels good</em>. Becoming known on the forums for answering questions, writing guides, or even just having a good taste in music <em>feels good</em> and also contributes to the overall health of the system. Though there are plenty of databases, and even plenty of different communication venues for scientists, there aren’t any databases (to my knowledge) with integrated community systems.</p>

<p>The tracker overlay model mirrors and extends some of the recommendations made by Benedikt Fecher and colleagues in their work on the reputational economy surrounding data sharing <a class="citation" href="#fecherReputationEconomyHow2017">[84]</a>. They give three policy recommendations: Increasing reputational benefits, reducing transaction costs, and “increasing market transparency by making open access to research data more visible to members of the research community.” The primary problem, in their eye, is that the reputational reward of data sharing is too small. In addition to increasing transparency, another way of increasing the reputational reward to sharing data is to embed it within a social system that is designed to reward communitarian behavior with reputational rewards. They continue to ideas like greater reward for data citations (which we will return to in <a href="#credit-assignment">credit assignment</a>), as well as awards for good datasets. Community awards are also longstanding parts of many digital communities, like What.cd’s Album of the Week, which rewarded someone who has done good work by letting them choose an album that would be freely downloadable, or Wikipedia’s <a href="https://en.wikipedia.org/wiki/Wikipedia:Barnstars">Barnstars</a>.</p>

<p>Many features of what.cd’s structure are undesirable for scientific infrastructure, but they demonstrate that a robust archive is not only a matter of building a database with some frontend, but by building a community <a class="citation" href="#brossCommunityCollaborationContribution2013">[85]</a>. Of course, we need to be careful with building the structural incentives for a data sharing system: the very last thing we want is another <a href="https://etiennelebel.com/cs/t-leaderboard/t-leaderboard.html">coercive leaderboard</a>. In contrast to what.cd, for infrastructure we want extremely low barriers to entry, and be agnostic to resources — researchers with access to huge server farms should not be unduly favored. We shouldn’t use downloading as the “cost,” because downloading and analyzing huge amounts of data is <em>good</em> and what we <em>want.</em> A better system for science might closer to <a href="https://wiki.installgentoo.com/wiki/Private_trackers#No_economy">ratioless trackers</a> that allow infinite downloads as long as they remain seeded for a certain amount of time afterwards.</p>

<p>These are all solvable problems, and can be worked on iteratively. They hint at a communication medium where we can discuss our experiments in the same place that they live; linking, embedding, comparing data and techniques to have the kind of longform, cumulative scientific discourse that is for now still relegated to being a fever dream. Rather than being prescriptive about one community structure, what allowed private bittorent trackers to develop and experiment with many different types of systems is the separation from the underlying data from the community overlay.</p>

<p>This model has its own problems, including the lack of interoperability between different trackers, the need to recreate a new set of accounts and database for each new tracker, among others. It’s also been tried before: sharing data in specific formats (as our running example, Neurodata Without Borders) on indexing systems like bittorrent trackers amounts to something like BioTorrents <a class="citation" href="#langilleBioTorrentsFileSharing2010">[86]</a> or <a href="https://academictorrents.com/">AcademicTorrents</a> <a class="citation" href="#cohenAcademicTorrentsCommunityMaintained2014">[87]</a>. Even with our extensions of version control and some model of automatic mirroring of data across the network, we still have some work to do. To address these and several other remaining needs for scientific data infrastructure, we can take inspiration from <em>federated systems.</em></p>

<h3 id="linked-data-or-surveillance-capitalism">Linked Data or Surveillance Capitalism?</h3>

<p>There is no shortage of databases for scientific data, but their traditional structure chokes on the complexity of representing multi-domain data. Typical relational databases require some formal schema to structure the data they contain, which have varying reflections in the APIs used to access them and interfaces built atop them. This broadly polarizes database design into domain-specific and domain-general<sup id="fnref:trackeranalogy" role="doc-noteref"><a href="#fn:trackeranalogy" class="footnote" rel="footnote">19</a></sup>. This design pattern results in a fragmented landscape of databases with limited interoperability. In a moment we’ll consider <em>federated systems</em> as a way to resolve this dichotomy and continue developing the design of our p2p data infrastructure, but for now we need a better sense of the problem.</p>

<p>Domain-specific databases require data to be in one or a few specific formats, and usually provide richer tools for manipulating and querying by metadata, visualization, summarization, aggregation that are purpose-built for that type of data. For example, NIH’s <a href="https://www.ncbi.nlm.nih.gov/gene/12550">Gene</a> tool has several visualization tools and cross-referencing tools for finding expression pathways, genetic interactions, and related sequences (Figure xx). This pattern of database design is reflected at several different scales, through institutional databases and tools like the Allen <a href="https://connectivity.brain-map.org/">brain atlases</a> or <a href="http://observatory.brain-map.org/visualcoding/">observatory</a>, to lab- and project-specific dashboards. This type of database is natural, expressive, and powerful — for the researchers they are designed for. While some of these databases allow open data submission, they often require explicit moderation and approval to maintain the guaranteed consistency of the database, which can hamper mass use.</p>

<p><img src="/infrastructure/assets/images/nih_gene_cdh1.png" alt="An example specialized plot of genomic regions, transcripts and products for the CDH1 gene (linked above), showing how specific tools have been built for this specific dataset" />
<em>NIH’s Gene tool included many specific tools for visualizing, cross-referencing, and aggregating genetic data. Shown is the “genomic regions, transcripts, and product” plot for Mouse Cdh1, which gives useful, common summary descriptions of the gene, but is not useful for, say, visualizing reading proficiency data.</em></p>

<p>General-purpose databases like <a href="https://figshare.com/">figshare</a> and <a href="https://zenodo.org/">zenodo</a><sup id="fnref:yrcool" role="doc-noteref"><a href="#fn:yrcool" class="footnote" rel="footnote">20</a></sup> are useful for the mass aggregation of data, typically allowing uploads from most people with minimal barriers. Their general function limits the metadata, visualization, and other tools that are offered by domain-specific databases, however, and are essentially public, versioned, folders with a DOI. Most have fields for authorship, research groups, related publications, and a single-dimension keyword or tags system, and so don’t programmatically reflect the metadata present in a given dataset.</p>

<p>The dichotomy of fragmented, subdomain-specific databases and general-purpose databases makes combining information from across even extremely similar subdisciplines combinatorically complex and laborious. In the absence of a formal interoperability and indexing protocol between databases, even <em>finding</em> the correct subdomain-specific database can be an act of raw experience or the raw luck of stumbling across just the right blog post list of databases. It also puts researchers who want to be good data stewards in a difficult position: they can hunt down the appropriate subdomain specific database and risk general obscurity; use a domain-general database and make their work more difficult for themselves and their peers to use; or spend all the time it takes to upload to multiple databases with potentially conflicting demands on format.</p>

<p>What can be done? There are a few parsimonious answers from standardizing different parts of the process: If we had a universal data format, then interoperability becomes trivial. Conversely, we could make a single ur-database that supports all possible formats and tools.</p>

<p>Universalizing a single part of a database system is unlikely to work because organizing knowledge is intrinsically political. Every system of representation is necessarily rooted in its context: one person’s metadata is another person’s data. Every subdiscipline has conflicting <em>representational</em> needs, will develop different local terminology, allocate differing granularity and develop different groupings and hierarchies for the same phenomena. At mildest, differences in representational systems can be incompatible, but at their worst they can reflect and reinforce prejudices and become tools of intellectual and social power struggles. Every subdiscipline has conflicting <em>practical</em> needs, with infinite variation in privacy demands, different priorities between storage space, bandwidth, and computational power, and so on. In all cases the boundaries of our myopia are impossible to gauge: we might think we have arrived at a suitable schema for biology, chemistry, and physics… but what about the historians?</p>

<p>Matthew J Bietz and Charlotte P Lee articulate this tension better than I can in their ethnography of metagenomics databases:</p>

<blockquote>
  <p>“Participants describe the individual sequence database systems as if they were shadows, poor representations of a widely-agreed-upon ideal. We find, however, that by looking across the landscape of databases, a different picture emerges. Instead, <strong>each decision about the implementation of a particular database system plants a stake for a community boundary. The databases are not so much imperfect copies of an ideal as they are arguments about what the ideal Database should be.</strong> […]</p>

  <p>When the microbial ecology project adopted the database system from the traditional genomic “gene finders,” they expected the database to be a boundary object. They knew they would have to customize it to some extent, but thought it would be able to “travel across borders and maintain some sort of constant identity”. In the end, however, <strong>the system was so tailored to a specific set of research questions that the collection of data, the set of tools, and even the social organization of the project had to be significantly changed.</strong> New analysis tools were developed and old tools were discarded. Not only was the database ported to a different technology, the data itself was significantly restructured to fit the new tools and approaches. While the database development projects had begun by working together, in the end they were unable to collaborate. <strong>The system that was supposed to tie these groups together could not be shielded from the controversies that formed the boundaries between the communities of practice.</strong>” <a class="citation" href="#bietzCollaborationMetagenomicsSequence2009">[9]</a></p>
</blockquote>

<p>As one ascends the scales of formalizing to the heights of the ontology designers, the ideological nature of the project is like a klaxon (emphasis in original):</p>

<blockquote>
  <p>An exception is the Open Biomedical Ontologies (OBO) Foundry initiative, which accepts under its label only those ontologies that adhere to the principles of ontological realism. Where the prevailing, i.e. computer science, view of ontology is focused on the logical consistency and inferential implications of ontologies as sets of assertions, the view of the OBO Foundry is that the quality of an ontology is also - indeed primarily - determined by the accuracy with which it represents the preexisting structure of reality. Ontologies, from this perspective, are representational artifacts, comprising a taxonomy as their central backbone, whose representational units are intended to designate <em>universals</em> (such as <em>human being</em> and <em>patient role</em>) or <em>classes defined in terms of universals</em> (such as <em>patient,</em> a class encompassing <em>human beings</em> in which there inheres a <em>patient role</em>) and certain relations between them.</p>

  <p>[…]</p>

  <p>BFO is a realist ontology [15,16]. This means, most importantly, that representations faithful to BFO can acknowledge only those entities which exist in (for example, biological) reality; thus they must reject all those types of putative negative entities - lacks, absences, non-existents, possibilia, and the like - which are sometimes postulated as artifacts of specific terminologies or of associated logical or computational frameworks <a class="citation" href="#ceustersFoundationsRealistOntology2010">[88]</a></p>
</blockquote>

<p>Aside from unilateral standardization, another formulation that doesn’t require existing server infrastructure to be dramatically changed is to link existing databases. The problem of linking databases is an old one with much well-trodden ground, and in the current regime of large server farms tend to find themselves somewhere close to metadata-indexing overlays. These overlays provide some additional tool that can translate and combine data between databases with some mapping between the terminology in the overlay and that of the individual databases. The NIH articulates this as a “Biomedical Data Translator” in its Strategic plan for Data Science:</p>

<blockquote>
  <p>Through its Biomedical Data Translator program, the National Center for Advancing Translational Sciences (NCATS) is supporting research to develop ways to connect conventionally separated data types to one another to make them more useful for researchers and the public. The Translator aims to bring data types together in ways that will integrate multiple types of existing data sourcess, including objective signs and symptoms of disease, drug effects, and other types of biological data relevant to understanding the development of disease and how it progresses in patients. <a class="citation" href="#NIHStrategicPlan2018">[20]</a></p>
</blockquote>

<p>And NCATS elaborates it a bit more on the project <a href="https://ncats.nih.gov/translator/about">“about”</a> page (emphasis mine):</p>

<blockquote>
  <p>As a result of recent scientific advances, a tremendous amount of data is available from biomedical research and clinical interactions with patients, health records, clinical trials and adverse event reports that could be useful for understanding health and disease and for developing and identifying treatments for diseases. <strong>Ideally, these data would be mined</strong> collectively to provide insights into the relationship between molecular and cellular processes (the targets of rational drug design) and the signs and symptoms of diseases. Currently, these very rich yet different data sources are housed in various locations, often in forms that are not compatible or interoperable with each other.  - https://ncats.nih.gov/translator/about</p>
</blockquote>

<p>The Translator is being developed by 28 institutions and nearly 200 team members as of 2019. They credit their group structure and flexible Other Transaction Award (OTA) funding mechanism for their successes <a class="citation" href="#consortiumBiomedicalDataTranslator2019">[89]</a>. OTA awards give the granting agency broad flexibility in to whom and for what money can be given, and consist of an initial competetive segment with possibility for indefinite noncompetitive extensions at the discretion of the agency <a class="citation" href="#fleisherOtherTransactionAward2019">[90]</a>.</p>

<p>The project appears to be in a relatively early phase, and so it’s relatively difficult to figure out exactly what it is that has been built. The <a href="https://web.archive.org/web/20210710012427/https://ncats.nih.gov/translator/projects">projects page</a> is currently a list of the leaders of different areas, but some parts of the project are visible through a bit of searching. They describe a registry of APIs for existing databases collected on their platform <a href="https://smart-api.info/portal/translator">SmartAPI</a> that are to be combined into a semantic knowledge graph <a class="citation" href="#consortiumUniversalBiomedicalData2019">[91]</a>. There are many kinds of knowledge graphs, and we will return to them and other semantic web technologies in <a href="#shared-knowledge">shared knowledge</a>, but the Translator’s knowledge graph explicitly sits “on top” of the existing databases as the only source of knowledge. Specifically, the graph structure consists of the nodes and edges of the <a href="https://github.com/biolink/biolink-model">biolink model</a> <a class="citation" href="#bruskiewichBiolinkBiolinkmodel2021">[92]</a>, and an edge is matched to a corresponding API that provides data for both elements. For each edge in the graph, then, a number of possible APIs can provide data without necessarily making a guarantee of consistency or accuracy.</p>

<p>They articulate a very similar set of beliefs about the impossibility of a unified dataset or ontology<sup id="fnref:impossibledata" role="doc-noteref"><a href="#fn:impossibledata" class="footnote" rel="footnote">21</a></sup><a class="citation" href="#consortiumUniversalBiomedicalData2019">[91]</a>, although arguably create one in <a href="https://biolink.github.io/biolink-model/docs/">biolink</a>, and this problem seems to have driven the focus of the project away from linking data as such towards developing a graph-powered query engine. The Translator is being designed to use machine-learning powered “autonomous relay agents” that sift through the inhomogenous data from the APIs and are able to return a human-readable response, also generated with machine-learning. The final form of the translator is still unclear, but between <a href="https://smart-api.info/portal/translator">SmartAPI</a>, a seemingly-preliminary description of the reasoning engine <a class="citation" href="#goelExplanationContainerCaseBased2021">[93]</a>, and descriptions from contractors <a class="citation" href="#ROBOKOPCoVar2021">[94]</a>, the machine learning component of the system could make it quite dangerous.</p>

<blockquote>

  <p>Based on these observations, our final assertion is that automating the ability to reason across integrated data sources and providing users who pose inquiries with a dossier of translated answers coupled with full provenance and confidence in the results is critical if we wish to accelerate clinical and translational insights, drive new discoveries, facilitate serendipity, improve clinical-trial design, and ultimately improve clinical care. This final assertion represents the driving motivation for the Translator system. <a class="citation" href="#consortiumUniversalBiomedicalData2019">[91]</a></p>
</blockquote>

<p>The intended use of the Translator seems to not be to directly search for and use the data itself, but to use the connected data to answer directed questions <a class="citation" href="#goelExplanationContainerCaseBased2021">[93]</a> — an example that is used repeatedly is drug discovery. For any given query of “drugs that could treat x disease,” the system traces out the connected nodes in the graph from the disease to find its phenotypes, which are connected to genes, which might be connected to some drug, and so on. The Translator builds on top of a large number of databases and database aggregators, and so it then needs a way of comparing and ranking possible answers to the question. In a simple case, a drug that directly acted on several involved genes might be ranked higher than, say, one that acted only indirectly on phenotypes with many off-target effects.</p>

<p>As with any machine-learning based system, if the input data is biased or otherwise (inevitably) problematic then the algorithm can only reflect that. If it is the case that this algorithm remains proprietary (due to, for example, it being developed by a for-profit defense contractor that named it ROBOKOP <a class="citation" href="#ROBOKOPCoVar2021">[94]</a>) harmful input data could have unpredictable long-range consequences on the practice of medicine as well as the course of medical research. Taking a very narrow sample of APIs that return data about diseases, I queried <a href="https://mydisease.info">mydisease.info</a> to see if it still had the outmoded definition of “transsexualism” as a disease <a class="citation" href="#ramTransphobiaEncodedExamination2021">[95]</a>. Perhaps unsurprisingly, it did, and was more than happy to give me a list of genes and variants that supposedly “cause” it - <a href="http://mydisease.info/v1/query?q=%22DOID%3A10919%22">see for yourself</a>.</p>

<p>This is, presumably, the fragility and inconsistency the machine-learning layer was intended to putty over: if one follows the provenance of the entry for “gender identity disorder” (renamed in DSM-V), one reaches first the disease ontology <a href="https://web.archive.org/web/20211007053446/https://www.ebi.ac.uk/ols/ontologies/doid/terms?iri=http%3A%2F%2Fpurl.obolibrary.org%2Fobo%2FDOID_1234">DOID:1234</a> which seems to trace back into an entry in a graph aggregator <a href="http://www.ontobee.org/ontology/DOID?iri=http://purl.obolibrary.org/obo/DOID_1234">Ontobee</a> (<a href="https://web.archive.org/web/20210923110103/http://www.ontobee.org/ontology/DOID?iri=http://purl.obolibrary.org/obo/DOID_1234">Archive Link</a>), which in turn lists this <a href="https://github.com/jannahastings/mental-functioning-ontology">github repository</a> <strong>maintained by a single person</strong> as its source<sup id="fnref:ipredit" role="doc-noteref"><a href="#fn:ipredit" class="footnote" rel="footnote">22</a></sup>.</p>

<p>If at its core the algorithm believes that being transgender is a disease, could it misunderstand and try to “cure” it? Even if it doesn’t, won’t it influence the surrounding network of entities with its links to genes, prior treatment, and so on in unpredictable ways? Combined with the online training that is then shared by other users of the translator <a class="citation" href="#consortiumUniversalBiomedicalData2019">[91]</a>, socially problematic treatment and research practices could be built into our data infrastructure without any way of knowing their effect. In the long-run, an effort towards transparency could have precisely the opposite effect by being run through a series of black boxes.</p>

<p>A larger problem is reflected in the scope and evolving direction of the Translator when combined with the preceding discussion of putting all data in the hands of cloud platform holders. There is mission creep from the original NIH initiative language that essentially amounts to a way to connect different data sources — what could have been as simple as a translation table between different data standards and formats. The original <a href="https://web.archive.org/web/20210709100523/https://ncats.nih.gov/news/releases/2016/feasibility-assessment-translator">funding statement from 2016</a> is similarly humble, and press releases <a href="https://web.archive.org/web/20210709171335/https://ncats.nih.gov/pubs/features/translator">through 2017</a> also speak mostly in terms of querying the data – though some ambition begins to creep in.</p>

<p>That is remarkably different than what is articulated in 2019 <a class="citation" href="#consortiumUniversalBiomedicalData2019">[91]</a> to be much more focused on <em>inference</em> and <em>reasoning</em> from the graph structure of the linked data for the purpose of <em>automating drug discovery.</em> It seems like the original goal of making a translator in the sense of “translating data between formats” has morphed into “translating data to language,” with ambitions of providing a means of making algorithmic predictions for drug discovery and clinical practice rather than linking data <a class="citation" href="#hailuNIHfundedProjectAims2019">[96]</a> Tools like these have been thoroughly problematized elsewhere, eg. <a class="citation" href="#groteEthicsAlgorithmicDecisionmaking2020">[97, 98, 99, 100]</a>.</p>

<p>As of September 2021, it appears there is still some work left to be done to make the Translator functional, but the early example illustrates some potential risks (emphases mine):</p>

<blockquote>
  <p>The strategy used by the Translator consortium in this case is to 1) identify phenotypes that are associated with [Drug-Induced Liver Injury] DILI, then 2) find genes which are correlated with these presumably pathological phenotypes, and then 3) identify drugs which target those genes’ products. The rationale is that drugs which target gene products associated with phenotypes of DILI may possibly serve as candidates for treatment options.</p>

  <p><strong>We constructed a series of three queries,</strong> written in the Translator API standard language and submitted to xARA to select appropriate KPs to collect responses (Figure 4). <strong>From each response, an exemplary result is selected and used in the query for the next step.</strong></p>

  <p>The results of the first query produced several phenotypes, one of them was ”Red blood cell count” (EFO0004305). When using this phenotype in the second step to query for genes, we identified one of the results as the telomerase reverse transcriptase (TERT) gene. This was then used in the third query (Figure 4) to identify targeting drugs, which included the drug Zidovudine.</p>

  <p>xARA use this result to call for an explanation. The xcase retrieved uses a relationship extraction algorithm [6] fine-tuned using BioBert [7]. The explanation solution seeks previously pre-processed publications where both biomedical entities (or one of its synonyms) is found in the same article within a distance shorter than 10 sentences. The excerpt of entailing both terms is then used as input to the relationship extraction method. When implementing this solution for the gene TERT (NCBIGene:7015) and the chemical substance Zidovudine (CHEBI:10110), the solution was able to identify corroborating evidence of this drug-target interaction with the relationship types being one of: ”DOWNREGULATOR,” ”INHIBITOR,” or ”INDIRECT DOWNREGULATOR” with respect to TERT. <a class="citation" href="#goelExplanationContainerCaseBased2021">[93]</a></p>
</blockquote>

<p>As a recap, since I’m not including the screenshots of the queries, the researchers searched first for a phenotypic feature of DILI, then selected “one of them” — red blood cell count — to search for genes that affect the phenotype, and eventually find a drug that effects that gene: all seemingly manually (an additional $1.4 million has been allocated to unify them <a class="citation" href="#haendelCommonDialectInfrastructure2021">[101]</a>). Zidovudine, as a nucleoside reverse transcriptase inhibitor, does inhibit telomerase reverse transcriptase <a class="citation" href="#hukezalieVitroExVivo2012">[102]</a>, but can also cause anemia and lower red blood cell counts <a class="citation" href="#ZidovudinePatientNIH">[103]</a> – so through the extended reasoning chain the system has made a sign flip and recommended a drug that will likely make the identified phenotype (low red blood cell count) worse? The manual input will then be used to train the algorithm for future results, though how data from prior use and data from graph structure will be combined in the ranking algorithm — and then communicated to the end user — is still unclear.</p>

<p>Contrast this with the space-age and chromed-out description from CoVar:</p>

<blockquote>
  <p>ROBOKOP technology scours vast, diverse databases to find answers that standard search technologies could never provide. It does much more than simple web-scraping. It considers inter-relationships between entities, such as colds cause coughs. Then it searches for new connections between bits of knowledge it finds in a wide range of data sources and generates answers in terms of these causal relationships, on-the-fly.</p>

  <p>Instead of providing a simple list of responses, ROBOKOP ranks answers based on various criteria, including the amount of supporting evidence for a claim, how many published papers reference a given fact, and the specificity of any particular relationship to the question.</p>
</blockquote>

<p>For-profit platform holders are not incentivized to do responsible science, or even really make something that works, provided they can get access to some of the government funding that pours out for projects that are eventually canned - <a href="https://reporter.nih.gov/search/kDJ97zGUFEaIBIltUmyd_Q/projects?sort_field=FiscalYear&amp;sort_order=desc">$75.5 million</a> so far since 2016 for the Translator <a class="citation" href="#RePORTRePORTERBiomedical2021">[104]</a>. As exemplified by the trial and discontinuation of the NIH Data Commons after <a href="https://reporter.nih.gov/search/H4LxgMGK9kGw6SeWCom85Q/projects?shared=true">$84.7 million</a>, centralized infrastructure projects often an opportunity to “dance until the music stops.” Again, it is relatively difficult to see from the outside what work is going on and how it all fits together, but judging from RePORTER there seem to be a <a href="https://reporter.nih.gov/project-details/10332268">profusion</a> of <a href="https://reporter.nih.gov/project-details/10333468">projects</a> and <a href="https://reporter.nih.gov/project-details/10333460">components</a> of the <a href="https://reporter.nih.gov/project-details/10330627">system</a> with unclear functional overlap, and the model seems to have developed into allocating funding to develop each separate knowledge source.</p>

<p>The risk with this project is very real because of the context of its development. After 5 years, it still seems like the the Translator is relatively far from realizing the vision of biopolitical control through algorithmic predictions, but combined with Amazon’s aggressive expansion into health technology <a class="citation" href="#AWSAnnouncesAWS2021">[105]</a> and even literally providing <a href="https://amazon.care/">health care</a> <a class="citation" href="#lermanAmazonBuiltIts2021">[106]</a>, and the uploading of all scientific and medical data onto AWS with entirely unenforceable promises of data privacy <a class="citation" href="#quinnYouCanTrust2021">[107]</a> — the notion of spending public money to develop a system for aggregating patient data with scientific and clinical data becomes dangerous. It doesn’t require takeover by Amazon to become dangerous — once you introduce the need for data to train an algorithm, you need to feed it data, and so the translator gains the incentive to suck up as much personal and other data as it can.</p>

<p>Even assuming the Translator works perfectly and has zero unanticipated consequences, the development strategy still reflects the inequities that pervade science rather than challenge them. Biopharmaceutical research, followed by broader biomedical research, being immediately and extremely profitable, attracts an enormous quantity of resources and develops state of the art infrastructure, while no similar infrastructure is built for the rest of science, academia, and society.</p>

<hr />

<p>I think it is important to pause and appreciate the potential for harm in the data infrastructural system describes so far, continuing to use structural transphobia as one example among many possible harms. First, a brief recap:</p>

<p>Through STRIDES, cloud providers like AWS, Google Cloud, and Microsoft Azure are intended to become the primary custodians of scientific data. Regardless of contracts and assurances, since their system is opaque and proprietary, there is no way to ensure that they will not crawl this data and use it to train their various algorithms-as-a-service — and they seem all too happy to do so, as evidenced by GitHub Co-Pilot reproducing copyrighted code and code with licenses that explicitly forbade its use in that context. Given that Amazon is expanding aggressively into health technology<a class="citation" href="#AWSAnnouncesAWS2021">[105]</a>, including wearables and literally providing <a href="https://amazon.care/">health care</a> <a class="citation" href="#lermanAmazonBuiltIts2021">[106]</a>, primary scientific data is a valuable prize in their mission to cement dominance in algorithmic health.</p>

<p>The effort to unify data across the landscape of databases, patient data, and so on is built atop a rickety pile of SaaS so fragile that a <em>single person</em> with a <em>single repository</em> can have ripple effects across the aggregators that impact the whole knowledge graph. In the above example, an outdated set of terminology classifies a subset of human gender as a disease, which then is linked to candidate genes and other nodes in the knowledge graph. Since there is a preponderance of misguided research about about the etiology and “biological mechanisms” of transgender people, the graph neighboorhood around transness is rich with biomarkers and functional data.</p>

<p>All of the above is known to be true now, but let’s see how it could play out practically in an all-too-plausible thought experiment.</p>

<p>Though the translator system now is intended for basic research and drug discovery, there is stated desire for it to eventually become a consumer/clinical product <a class="citation" href="#hailuNIHfundedProjectAims2019">[96]</a>. Say a cloud provider rolls out a service for clinical recommendations for doctors informed by the full range of scientific, clinical, wearable, and other personal data they have available — a trivial extension of <a href="https://web.archive.org/web/20211003070018/https://support.apple.com/en-us/HT208680">existing</a> patient medical aggregation and <a href="https://web.archive.org/web/20210408221213/https://support.google.com/fit/answer/7619539?hl=en&amp;co=GENIE.Platform%3DAndroid">recommendation</a> services that <a href="https://web.archive.org/web/20210930203834/https://press.aboutamazon.com/news-releases/news-release-details/amazon-adds-more-halo-introducing-halo-view-halo-fitness-and">express</a> their biopolitical control as a slick wristband with app. It’s very “smart” and is very “private” in the sense that only the algorithm ever sees your personal data.</p>

<p>Since these cloud providers as a rule depend on developing elaborate personal profiles for targeted advertising algorithmically inferred from available data<sup id="fnref:googlepatent" role="doc-noteref"><a href="#fn:googlepatent" class="footnote" rel="footnote">23</a></sup>, that naturally includes diagnosed or inferred disease — a practice they explicitly describe in the patents for the targeting technology<a class="citation" href="#bharatGeneratingUserInformation2005">[108]</a>, gone to court to defend <a class="citation" href="#SmithFacebookInc2018">[109, 110]</a>, formed secretive joint projects with healthcare systems to pursue <a class="citation" href="#bourreauGoogleFitbitWill2020">[111]</a>, and so on. Nothing too diabolical here, just a system wherein <strong>your search results and online shopping habits influence your health care in unpredictable and frequently inaccurate <a class="citation" href="#rasmyMedBERTPretrainedContextualized2021">[112]</a> ways.</strong></p>

<p>Imagine, through some pattern in your personal data, <strong>Amazon diagnoses you as trans.</strong>Whether their assessment is true or not is unimportant. Since the Translator works as a graph-based knowledge engine, your algorithmic transness, with its links through related genes, “symptoms,” and whatever other uninspectable network links the knowledge graph has, influences the medical care you receive. All part of the constellation of personalized information that constitutes “personalized medicine.”</p>

<p>The Translator assures us that it will give doctors understandable provenance by being able to explain how it arrived at its recommendation. Let’s assume from prior experience with neural net language models that part of the process doesn’t work very well, or at least doesn’t give a fully exhaustive description of every single relevant graph entity. Now let’s further assume based on the above DILI example that the knowledge graph is not able to reliably “understand” the complex cultural-technological context of transness, and since it is classified as a “disease” decides that you need to be “cured.” Since it has access to a diverse array of biomedical data, it might even be able to concoct a very effective conversion therapy regimen <em>personalized just for you.</em> The algorithm could prescribe your conversion therapy <em>without you or the doctor knowing it.</em></p>

<p>Transphobic behavior that impacts treatment is common <a class="citation" href="#ramTransphobiaEncodedExamination2021">[95, 113]</a>. Since the Translator’s algorithm is designed to learn from feedback and use<a class="citation" href="#consortiumUniversalBiomedicalData2019">[91]</a>, transphobic practices could easily reinforce and magnify the algorithm’s initial guess about what transness being a disease should mean for trans people in practice. Combined with the limitations on provision of care from insurance systems <a class="citation" href="#strangioCanReproductiveTrans2016">[113]</a>, on a wide scale transphobic medical practices could be transmuted into a “scientifically justified” standard of care.</p>

<p>Scaling out further, the original intention of the tool is to guide drug discovery and pharmaceutical research, so harm could be encoded into the indefinite future of biomedical research — imperceptibly guiding the array of candidate drugs to test based on an algorithmically biased perception of biology and medical prerogative. Even in the case that society changes and we attempt to make amends in our institution for outdated and harmful notions, the long tail of ingrained learning in a proprietary algorithm could be hard to unlearn if the proprieter is inclined to try at all. So even many years into the future when we “know better,” the ghosts of algorithmically guided medical reserach and practice could still unknowingly guide our hands.</p>

<p>The pathologizing of transgender people is just one example among many demonstrated instances of algorithmic bias like race, disability, and effectively any other marginalized group. The critical issue is that <strong>we might not have any idea</strong> how the algorithm is influencing research and practice at scales large and small, immediate and indefinite. The impacts don’t have to be as dramatic as this particular thought experiment to be harmful. The subtlety of having dosages, prescriptions, and candidate drugs jittered by a massive integrated machine learning system is harm in itself: our medical care becomes training data. The point is that we <em>can’t know</em> the effects of letting the course of our medical research and clinical care be steered by an algorithm embedded within a platform that has <em>any</em> incentive that conflicts with our collective health.</p>

<hr />

<p>How did we get here? How could an effort to link biomedical data become an instrument of mass surveillance and harm?</p>

<p>I have no doubt that everyone working on the Translator is doing so for good reasons, and they have done useful work. Forming a consortium and settling on a development model is hard work and this group should be applauded for that. Unifying APIs with Smart-API, drafting an ontology, and making a knowledge graph, are all directly useful to reducing barriers to desiloing data and shared in the vision articulated here.</p>

<p>The problems here come in a few mutually reinforcing flavors, I’ll group them crudely into the constraints of existing infrastructure, centralized models of development, and a misspecification of what the purpose of the infrastructure should be.</p>

<p>Navigating a relationship with existing technology in new development is tricky, but there is a distinction between integrating with it and embodying its implications. Since the other projects spawned from the Data Science Initiative embraced the use of cloud storage, the constraint of using centralized servers with the need for a linking overlay was baked in the project from the beginning. From this decision immediately comes the impossibility of enforcing privacy guarantees and the rigidity of database formats and tooling. Since the project started from a place of presuming that the data would be hosted “out there” where much of its existence is prespecified, building the Translator “on top” of that system is a natural conclusion. Further, since the centralized systems proposed in the other projects don’t aim to provide a means of standardization or integration of scientific data that doesn’t already have a form, the reliance on APIs for access to structured data follows as well.</p>

<p>Organizing the process as building a set of tools as a relatively large, but nonetheless centralized and demarcated group pose additional challenges. I won’t speculate on the incentives and personal dynamics that led there, but I also believe this development model comes from good intention. While there is clearly a lot of delegation and distributed work, the project in its different teams takes on specific tools that <em>they</em> build and <em>we</em> use. This is broadly true of scientific tools, especially databases, and contributes to how they <em>feel</em>: they feel disconnected with our work, don’t necessarily help us do it more easily or more effectively, and contributing to them is a burdensome act of charity.</p>

<p>This is reflected in the form of the biolink ontology, where rather than a tool for scientists to <em>build</em> ontologies, it is intended to be <em>built towards.</em> There is tension between the articulated impossibility of a grand unified ontology and the eventual form of the algorithm that depends on one that, in their words, motivated the turn to machine learning to reconcile that impossibility. The compromise seems to be the use of a quasi-“neutral” meta-ontology that instantiates its different abstract objects depending on the contents of its APIs. A ranking algorithm to parse the potentially infinite results follows, and so too does the need for feedback and training and the potential for long-lived and uninterrogatable algorithmic bias.</p>

<p>These all contribute to the misdirection in the goal of the project. Linking <em>all</em> or <em>most</em> biomedical data in single mutually coherent system drifted into an API-driven knowledge-graph for pharmaceutical and clinical recommendations. Here we meet a bit of a reprise of the <a href="#neatness-vs-scruffiness">#neat</a> mindset, which emphasizes global coherence as a basis for reasoning rather than providing a means of expressing the natural connections between things in their local usage. Put another way, the emphasis is on making something logically complete for some dream of algorithmically-perfect future rather than to be useful to do the things researchers at large want to do but find difficult. The press releases and papers of the Translator project echo a lot of the heady days of the semantic web<sup id="fnref:diderot" role="doc-noteref"><a href="#fn:diderot" class="footnote" rel="footnote">24</a></sup> and its attempt to link everything — and seems ready to follow the same path of the fledgling technologies being gobbled up by technology giants to finish and privatize.</p>

<p>!! this development direction is intrinsic to the “data lake” model — deprive people of tools to organize their data because that’s “easier for them” or whatever, but then build proprietary aggregation tools on top of that. What about giving people the tools to make their work useful to them?</p>

<p>I think the problem with the initial and eventual goals of the translater can be illustrated by problematizing the central focus on linking “all data,” or at least “all biomedical data.” Who is a system of “all (biomedical) data” for? Outside of metascientists and pharmaceutical companies, I think most people are interested primarily in the data of their colleagues and surrounding disciplines. Every infrastructural model is an act of balancing constraints, and prioritizing “all data” seems to imply “for some people.” Who is supposed to be able to upload data? change the ontology? inspect the machine learning model? Who is in charge of what? Who is a knowledge-graph query engine useful for?</p>

<p>Another prioritization might be building systems for <em>all people</em> that can <em>embed with existing practices</em> and <em>help them do their work</em> which typically involves accessing <em>some data.</em> The system needs to not only be designed to allow anyone to integrate their data into it, but also to be integrated into how researchers collect and use their data. It needs to give them firm, verifiable, and fine-grained control over who has access to their data and for what purpose. It needs to be <em>multiple,</em> governable and malleable in local communities of practice. Through the normal act of making my data available to my colleague and vice versa, build on a cumulative and negotiable understanding of the relationship between our work and its meaning.</p>

<p>Without too much more prefacing, let’s return to the scheduled programming.</p>

<h3 id="federated-systems-of-language">Federated Systems (of Language)</h3>

<p>When last we left it, our peer-to-peer system needed some way of linking data together. Instead of a big bucket of files as is traditional in torrents and domain-general databases, we need some way of exposing the metadata of disparate data formats so that we can query for and find the particular range of datasets appropriate to our question. !! For this section, I want to develop a notion of data linking that’s a lot closer to natural language than an engineering specification.</p>

<p>Each format has a different metadata structure with different names, and even within a single format we want to support researchers who extend and modify the core format. Additionally, each format has a different implementation, eg. as an hdf5 file, binary files in structured subdirectories, SQL-like databases.</p>

<p>That’s a lot of heterogeneity to manage, but fret not: there is hope. Researchers navigate this variability manually as a standard part of the job, and we can make that work cumulative by building tools that allow researchers to communally describe and negotiate over the structure of their data and the local relationships to other data structures. We can extend our peer-to-peer system to be a <em>federated database</em> system.</p>

<p>Federated systems consist of <em>distributed</em>, <em>heterogeneous</em>, and <em>autonomous</em> agents that implement some minimal agreed-upon standards for mutual communication and (co-)operation. Federated databases<sup id="fnref:federatedterm" role="doc-noteref"><a href="#fn:federatedterm" class="footnote" rel="footnote">25</a></sup> were proposed in the early 1980’s <a class="citation" href="#heimbignerFederatedArchitectureInformation1985">[114]</a> and have been developed and refined in the decades since as an alternative to either centralization or non-integration <a class="citation" href="#litwinInteroperabilityMultipleAutonomous1990">[115, 116, 117]</a>. Their application to the dispersion of scientific data in local filesystems is not new <a class="citation" href="#busseFederatedInformationSystems1999">[118, 119, 120]</a>, but their implementation is more challenging than imposing order with a centralized database or punting the question into the unknowable maw of machine learning.</p>

<p>!! There is a lot of subtlety to the terminology surrounding “federated” and the typology of distributed systems generally, I am using it more in the federated messaging sense of forming groups of people, rather than the strict term federated databases which do imply a standardized schema across a federation. I am largely in line with the notion of distributed databases here <a class="citation" href="#hankeDefenseDecentralizedResearch2021">[121]</a>.</p>

<p>Amit Sheth and James Larson, in their reference description of federated database systems, describe <strong>design autonomy</strong> as one critical dimension that characterizes them:</p>

<blockquote>
  <p>Design autonomy refers to the ability of a component DBS to choose its own design with respect to any matter, including</p>

  <p>(a) The <strong>data</strong> being managed (i.e., the Universe of Discourse),</p>

  <p>(b) The <strong>representation</strong> (data model, query language) and the <strong>naming</strong> of the data elements,</p>

  <p>(c) The conceptualization or <strong>semantic interpretation</strong> of the data (which greatly contributes to the problem of semantic heterogeneity),</p>

  <p>(d) <strong>Constraints</strong> (e.g., semantic integrity constraints and the serializability criteria) used to manage the data,</p>

  <p>(e) The <strong>functionality</strong> of the system (i.e., the operations supported by system),</p>

  <p>(f) The <strong>association and sharing with other systems</strong>, and</p>

  <p>(g) The <strong>implementation</strong> (e.g., record and file structures, concurrency control algorithms).</p>
</blockquote>

<p>Susanne Busse and colleagues add an additional dimension of <strong>evolvability,</strong> or the ability of a particular system to adapt to inevitable changing uses and requirements <a class="citation" href="#busseFederatedInformationSystems1999">[118]</a>.</p>

<p>In order to support such radical autonomy and evolvability, federated systems need some means of translating queries and representations between heterogeneous components. The typical conceptualization of federated databases have five layers that implement different parts of this reconciliation process <a class="citation" href="#shethFederatedDatabaseSystems1990">[122]</a>:</p>

<ul>
  <li>A <strong>local schema</strong> is the representation of the data on local servers, including the means by which they are implemented in binary on the disk</li>
  <li>A <strong>component schema</strong> serves to translate the local schema to a format that is compatible with the larger, federated schema</li>
  <li>An <strong>export schema</strong> defines permissions, and what parts of the local database are made available to the federation of other servers</li>
  <li>The <strong>federated schema</strong> is the collection of export schemas, allowing a query to be broken apart and addressed to different export schemas. There can be multiple federated schemas to accomodate different combinations of export schemas.</li>
  <li>An <strong>export schema</strong> can further be used to make the federated schema better available to external users, but in this case since there is no notion of “external” it is less relevant.</li>
</ul>

<p>This conceptualization provides a good starting framework and isolation of the different components of a database system, but a peer-to-peer database system has different constraints and opportunities <a class="citation" href="#bonifatiDistributedDatabasesPeertopeer2008">[123]</a>. In the strictest, “tightly coupled” federated systems, all heterogeneity in individual components has to be mapped to a single, unified federation-level schema. Loose federations don’t assume a unified schema, but settle for a uniform query language, and allow multiple translations and views on data to coexist. A p2p system naturally lends itself to a looser federation, and also gives us some additional opportunities to give peers agency over schemas while also preserving some coherence across the system. I will likely make some database engineers cringe, but the emphasis for us will be more on building a system to support distributed social control over the database, rather than guaranteeing consistency and transparency between the different components.</p>

<p>Though there are hundreds of subtleties and choices in implementation beneath the level of detail I’ll reach here, allow me to illustrate the system by example:</p>

<p>Let us start with the ability for a peer to choose who they are associated with at multiple scales of organization: a peer can directly connect with another peer, but peers can also federate into groups, groups can federate into groups of groups, and so on. Within each of these grouping structures, the peer is given control over what data of theirs is shared.</p>

<p>Clearly, we need some form of <em>identity</em> in the system, let’s make it simple and flat and denote that in pseudocode as <code class="language-plaintext highlighter-rouge">@username</code> — in reality, without any form of distributed uniqueness checking, we would need to have some notion of where this username is “from,” so let’s say we actually have a system like <code class="language-plaintext highlighter-rouge">username@name-provider</code> but for this example assume a single name provider, say ORCID<sup id="fnref:chainz" role="doc-noteref"><a href="#fn:chainz" class="footnote" rel="footnote">26</a></sup>. Someone would then be able to use their <code class="language-plaintext highlighter-rouge">@name</code>space as a root, under which they could refer to their data, schemas, and so on, which will be denoted <code class="language-plaintext highlighter-rouge">@name:subobject</code> (see this notion of personal namespaces for knowledge organization discussed in early wiki culture here <a class="citation" href="#MeatballWikiPersonalCategories">[124]</a>). Let us also assume that there is no categorical difference between <code class="language-plaintext highlighter-rouge">@usernames</code> used by individual researchers, institutions, consortia, etc. — everyone is on the same level.</p>

<p>We pick up where we left off earlier with a peer who has their data in some discipline-specific format, which let us assume for the sake of concreteness has a representation as an <a href="https://www.w3.org/OWL/">OWL</a> schema.</p>

<p>That schema could be “owned” by the <code class="language-plaintext highlighter-rouge">@username</code> corresponding to the standard-writing group — eg <code class="language-plaintext highlighter-rouge">@nwb</code> for neurodata without borders. In a <a href="https://www.w3.org/TR/turtle/">turtle-ish</a> pseudocode, then, our dataset might look like this:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>&lt;#cool-dataset&gt;
    a @nwb:NWBFile
    @nwb:general:experimenter @jonny
    @nwb:ElectricalSeries
        .electrodes [1, 2, 3]
        .rate 30000
        .data [...]
</code></pre></div></div>

<p>Where I indicate that me, <code class="language-plaintext highlighter-rouge">@jonny</code> collected <code class="language-plaintext highlighter-rouge">a @nwb:NWBFile</code> dataset (indicated with <code class="language-plaintext highlighter-rouge">&lt;#dataset-name&gt;</code> to differentiate an application/instantiation of a schema from its definition) that consisted of an <code class="language-plaintext highlighter-rouge">@nwb:ElectricalSeries</code> and the relevant attributes (where a leading <code class="language-plaintext highlighter-rouge">.</code> is a shorthand for the parent schema element).</p>

<p>!! pause to describe notion of using triplet links and the generality they afford us.</p>

<p>I have some custom field for my data, though, which I extend the format specification to represent. Say I have invented some new kind of solar-powered electrophysiological device and want to annotate its specs alongside my data.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>@jonny:SolarEphys &lt; @nwb:NWBContainer
    ManufactureDate
    InputWattageSeries &lt; @nwb:ElectricalSeries
        newprop
        -removedprop
</code></pre></div></div>
<p>!! think of a better example lmao^^ and then annotate what’s going on.</p>

<p>There are many strategies for making my ontology extension available to others in a federated network. We could use a distributed hash table, or <a href="https://en.wikipedia.org/wiki/Distributed_hash_table"><strong>DHT</strong></a>, like bittorrent, which distributes references to information across a network of peers (eg. <a class="citation" href="#pirroDHTbasedSemanticOverlay2012">[125]</a>). We could use a strategy like the <a href="https://matrix.org/"><strong>Matrix</strong> messaging protocol</a>, where users belong to a single home server that federates with other servers. Each server is responsible for keeping a synchronized copy of the messages sent on the servers and rooms it’s federated with, and each server is capable of continuing communication if any of the others failed. We could use <a href="https://www.w3.org/TR/2018/REC-activitypub-20180123/"><strong>ActivityPub</strong> (AP)</a> <a class="citation" href="#Webber:18:A">[126]</a>, a publisher-subscriber model where users affiliated with a server post messages to their ‘outbox’ and are sent to listening servers (or made available to HTTP GET requests). AP uses <a href="https://json-ld.org/">JSON-LD</a> <a class="citation" href="#spornyJSONLDJSONbasedSerialization2020">[127]</a>, so is already capable of representing linked data, and the related ActivityStreams vocabulary <a class="citation" href="#snellActivityStreams2017">[128]</a> also has plenty of relevant <a href="https://www.w3.org/TR/activitystreams-vocabulary/#activity-types">action types</a> for <a href="https://www.w3.org/TR/activitystreams-vocabulary/#dfn-create">creating</a>, <a href="https://www.w3.org/TR/activitystreams-vocabulary/#dfn-question">discussing</a>, and <a href="https://www.w3.org/TR/activitystreams-vocabulary/#dfn-tentativeaccept">negotiating</a> over links (also see <a href="https://github.com/openEngiadina/cpub">cpub</a>). We’ll return to ActivityPub later, but for now the point is to let us assume we have a system for distributing schemas/extensions/links associated with an identity publicly or to a select group of peers.</p>

<p>For the moment our universe is limited only to other researchers using NWB. Conveniently, the folks at NWB have set up a federating group so that everyone who uses it can share their format extensions. Since our linking system for manipulating schemas is relatively general, we can use it to “formalize” a basic configuration for a federating group that automatically <code class="language-plaintext highlighter-rouge">Accept</code>s request to <code class="language-plaintext highlighter-rouge">Join</code> and allows any schema that inherits from their base <code class="language-plaintext highlighter-rouge">@nwb:NWBContainer</code> schema. Let’s say <code class="language-plaintext highlighter-rouge">@fed</code> defines some basic properties of our federating system — it constitutes our federating “protocol” — and loosely use some terms from the <a href="https://www.w3.org/ns/activitystreams#class-definitions">ActivityStreams</a> vocabulary as <code class="language-plaintext highlighter-rouge">@as</code></p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>&lt;#nwbFederation&gt;
  a @fed:Federation
  onReceive
    @as:Join @as:Accept
  allowSchema
    extensionOf @nwb:NWBContainer
</code></pre></div></div>

<p>Now anyone that is a part of the <code class="language-plaintext highlighter-rouge">@nwbFederation</code> would be able to see the schemas we have submitted, sort of like a beefed up, semantically-aware version of the existing <a href="https://nwb-extensions.github.io/">neurodata extensions catalog</a>. In this system, many overlapping schemas could exist simultaneously, but wouldn’t become a hopeless clutter because similar schemas could be compared and reconciled based on their semantic properties.</p>

<p>So far we have been in the realm of metadata, but how would my computer know how to read and write the data to my disk so i can use it? In a system with heterogeneous data types and database implementations, we need some means of specifying different programs to use to read and write, different APIs, etc. Why not make that part of the file schema as well? Suppose the HDF5 group (or anyone, really!) has a namespace <code class="language-plaintext highlighter-rouge">@hdf</code> that defines the properties of an <code class="language-plaintext highlighter-rouge">@hdf:HDF5</code> file, basic operations like <code class="language-plaintext highlighter-rouge">Read</code>, <code class="language-plaintext highlighter-rouge">Write</code>, or <code class="language-plaintext highlighter-rouge">Select</code>. NWB could specify that in their definition of <code class="language-plaintext highlighter-rouge">@nwb:NWBFile</code>:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>@nwb.NWBFile
  a @hdf:HDF5
    isVersion x.y.z
    hasDependency libhdf5==x.y.z
  usesContainer @nwb:NWBContainer
</code></pre></div></div>

<p>The abstraction around the file implementation makes it easier for others to consume my data, but it also makes it easier for <em>me</em> to use and contribute to the system. Making an extension to the schema wasn’t some act of charity, it was the most direct way for me to use the tool to do what I wanted. Win-win: I get to use my fancy new instrument and store its data by extending some existing format standard, and in the process make the standard more complete and useful. We are able to make my work useful by <em>aligning the modalities of use and contribution.</em></p>

<p>Now that I’ve got my schema extension written and submitted to the federation, time to submit my data! Since it’s a p2p system, I don’t need to manually upload it, but I do want to control who gets it. By default, I have all my NWB datasets set to be available to the <code class="language-plaintext highlighter-rouge">@nwbFederation</code> , and I list all my metadata on, say the Society for Neuroscience’s <code class="language-plaintext highlighter-rouge">@sfnFederation</code>.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>&lt;#globalPermissions&gt;
  a @fed:Permissions
  permissionsFor @jonny

  federatedWith 
    name @nwbFederation
    @fed:shareData 
      is @nwb:NWBFile

  federatedWith
    name @sfnFederation
    @fed:shareMetadata
</code></pre></div></div>

<p>Let’s say this dataset in particular is a bit sensitive — say we apply a set of permission controls to be compliant with <code class="language-plaintext highlighter-rouge">@hhs.HIPAA</code> — but we do want to make use of some public server space run by our Institution, so we let it serve an encrypted copy that those I’ve shared it with can decrypt. Since we’ve applied the <code class="language-plaintext highlighter-rouge">@hhs.HIPAA</code> ruleset, we would be able to automatically detect if we have any conflicting permissions, but we’re doing fine in this example.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>&lt;#datasetPermissions&gt;
  a @fed:Permissions
  permissionsFor @jonny:cool-dataset

  accessRuleset @hhs:HIPAA
    .authorizedRecipient &lt;#hash-of-patient-ids&gt;
  
  federatedWith
    name @institutionalCloud
    @fed:shareEncrypted
</code></pre></div></div>

<p>Now I want to make use of some of my colleagues data. Say I am doing an experiment with a transgenic dragonfly and collaborating with a chemist down the hall. This transgene, known colloquially in our discipline as <code class="language-plaintext highlighter-rouge">"@neuro:superstar6"</code> (oh-so-uncreatively ripped off by the chemists as <code class="language-plaintext highlighter-rouge">"@chem:SUPER6"</code>) fluoresces when the dragonfly is feeling bashful, and we have plenty of photometry data stored as <code class="language-plaintext highlighter-rouge">@nwb:Fluorescence</code> objects. We think that its fluorescence is caused by the temperature-dependent conformational change from blushing. They’ve gathered NMR and Emission spectroscopy data in their chemistry-specific format, say <code class="language-plaintext highlighter-rouge">@acs:NMR</code> and <code class="language-plaintext highlighter-rouge">@acs:Spectroscopy</code>.</p>

<p>We get tired of having our data separated and needing to maintain a bunch of pesky scripts and folders, so we decide to make a bridge between our datasets. We need to indicate that our different names for the gene are actually the same thing and relate the spectroscopy data.</p>

<p>Let’s make the link explicit, say we use <a href="https://www.w3.org/2009/08/skos-reference/skos.html"><code class="language-plaintext highlighter-rouge">@skos</code></a>?</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>&lt;#super-link-6&gt;
  a @fed:Link
  
  from @neuro:superstar6
  to @chem:SUPER6
  link @skos:exactMatch

</code></pre></div></div>

<p>Our <code class="language-plaintext highlighter-rouge">@nwb:Fluorescence</code> data has the emission wavelength in its <code class="language-plaintext highlighter-rouge">@nwb:Fluorescence:excitation_lambda</code> property<sup id="fnref:notreallynwb" role="doc-noteref"><a href="#fn:notreallynwb" class="footnote" rel="footnote">27</a></sup>, which is the value of their <code class="language-plaintext highlighter-rouge">@acs:Spectroscopy</code> data at a particular value of its <code class="language-plaintext highlighter-rouge">wavelength</code>. Unfortunately, <code class="language-plaintext highlighter-rouge">wavelength</code> isn’t metadata for our friend, but a column in the <code class="language-plaintext highlighter-rouge">@acs:Spectroscopy:readings</code> table, so for now the best we can do is indicate that <code class="language-plaintext highlighter-rouge">excitation_lambda</code> is one of the values in <code class="language-plaintext highlighter-rouge">wavelength</code> and pick it up in our analysis tools.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>&lt;#imaging&gt;
 a @fed:Link
 
 from @nwb:Fluorescence:excitation_lambda
 to @acs:Spectroscopy:readings
 link @fed:Subset
   valueIn "wavelength"
</code></pre></div></div>

<p>This makes it much easier for us to index our data against each other and solves a few real practical problems we were facing in our collaboration. We don’t need to do as much cleaning when it’s time to publish the data since it can be released as a single linked entity.</p>

<p>Rinse and repeat our sharing and federating process from our previous schema extension, add a little bit of extra federation with the <code class="language-plaintext highlighter-rouge">@acs</code> namespace, and in the normal course of our doing our research we’ve contributed to the graph structure linking two common data formats. Ours is one of many, with ugly little names like <code class="language-plaintext highlighter-rouge">@jonny:super-link-6</code><sup id="fnref:creditte" role="doc-noteref"><a href="#fn:creditte" class="footnote" rel="footnote">28</a></sup>. We might not have followed the exact rules, and we only made a few links rather than a single authoratative mapping, but if someone is interested in compiling one down the line they’ll start off a hell of a lot further than if we hadn’t contributed it!</p>

<p>With a protocol for how queries can be forwarded and transformed between users and federations, one could access the same kind of complex query structure as traditional databases with <a href="https://www.w3.org/TR/sparql11-federated-query/">SPARQL</a> <a class="citation" href="#SPARQLFederatedQuery2013">[129]</a> as has been proposed for biology many times before <a class="citation" href="#simaEnablingSemanticQueries2019">[130, 119, 120]</a>. Some division in the way that data and metadata are handled is necessary for the network to work in practice, since we can’t expect a search to require terabytes of data transfer. A natural solution to this is to have metadata query results point to <a href="https://en.wikipedia.org/wiki/Content-addressable_storage">content addressed</a> identifiers that are served peer to peer. A mutable/changeable/human-readable name and metadata system that points to a system of permanent, unique identifiers has been one need that has hobbled IPFS, and is the direction pointed to by DataLad <a class="citation" href="#hankeDefenseDecentralizedResearch2021">[121]</a>. A <a href="https://mastodon.social/@humanetech/107155144840782386">parallel</a> 
<a href="https://web.archive.org/web/20211024082055/https://socialhub.activitypub.rocks/t/which-links-between-activitypub-and-solid-project/529">set</a> of <a href="https://web.archive.org/web/20211024080845/https://socialhub.activitypub.rocks/t/how-solid-and-activitypub-complement-each-other-best/727">conversations</a> has been <a href="https://web.archive.org/web/20211024081238/https://forum.solidproject.org/t/discussion-solid-vs-activitypub/2685">happening</a> in the broader linked data community with regard to using ActivityPub as a way to index data on Solid.</p>

<p>In this example I have been implicitly treating the <code class="language-plaintext highlighter-rouge">@nwbFederation</code> users like bittorrent trackers, keeping track of different datasets in their federation, but there is no reason why queries couldn’t themselves be distributed across the participating peers, though I believe tracker-like federations are useful and might emerge naturally. A system like this doesn’t need the radical zero trust design of, for example, some distributed ledgers, and an overlapping array of institutional, disciplinary, interest, and so on federations would be a good means of realizing the evolvable community structure needed for sustained archives.</p>

<p>Extend this practice across the many overlapping gradients of cooperation and collaboration in science, and on a larger scale a system like this could serve as a way to concretize and elevate the organic, continual negotiation over meaning and practice that centralized ontologies can only capture as a snapshot. It doesn’t have the same guarantees of consistency or support for algorithmic reasoning as a top-down system would in theory, but it would give us agency over the structure of our information and have the potential to be useful for a far broader base of researchers.</p>

<p>I have no idea where the physicists’ store their data or what format it’s in, <em>but the chemists might,</em> and the best way to get there from here might be a dense, multiplicative web of actual practical knowledge instead of some sparsely used corporate API.</p>

<p>I have been purposefully nonprescriptive about implementation and fine details here, what have we described so far? !! short summary of preceding section !! recall that what i am describing is protocol-like, so having multiple implementations that evolve is sorta the point.</p>

<p>Like the preceding description of the basic peer-to-peer system, this joint metadata/p2p system could be fully compatible with existing systems. Translating between a metadata query and a means of accessing it on heterogeneous databases is a requisite part of the system, so, for example, there’s no reason that an HTTP-based API like SmartAPI couldn’t be queried.</p>

<p><a href="https://www.datalad.org/">DataLad</a> <a class="citation" href="#halchenkoDataLadDistributedSystem2021">[131, 121]</a> and its application in Neuroscience as <a href="https://dandiarchive.org">DANDI</a> are two projects that are <em>very close</em> to what I have been describing here — developing a p2p backend for datalad and derivation into a protocol might even be a promising development path towards it.</p>

<p>!! close this section by taking a larger view - <a class="citation" href="#langilleBioTorrentsFileSharing2010">[86]</a> DANDI is in on the p2p system, as is kachery-p2p!! p2p systems already plenty in use, academic torrents, biotorrents, libgen on IPFS !! the proof of their utility is in the pudding, arguably when i’ve been talkiung about ‘centralized servers’ what i’m actually talking about content delivery networks, which are effectively p2p systems – they just own all the peers.</p>

<p>!! note that this is all fully compatible with existing systems and is a superset of centralized servers with centralized schemas!</p>

<h2 id="shared-tools">Shared Tools</h2>

<p>Straddling our system for sharing data are the tools to gather and analyze it. Experimental and analytical tools are the natural point of extension for collectively developed scientific digital infrastructure, and considering them together shows the combinatoric power of integrating interoperable domains of scientific practice. In particular, in addition to benefits from their development in isolation, we can ask how a more broadly integrated system helps problems like adoption and incentives for distributed work, enables a kind of deep provenance from experiment to results, and lets us reimagine the form of the community and communication tools for science.</p>

<p>This section will be relatively short compared to <a href="#shared-data">shared data</a>. We have already introduced, motivated, and exemplified many of the design practices of the broader infrastructural system. There is much less to argue against or “undo” in the spaces of analytical and experimental tools because so much more work has been done, and so much more power has been accrued in the domain of data systems. Distributed computing does have a dense history, with huge numbers of people working on the problem, but its hegemonic form is much closer to the system articulated below than centralized servers are to federated semantic p2p systems. I also have written extensively about <a href="#experimental-frameworks">experimental frameworks</a> before <a class="citation" href="#saundersAutopilotAutomatingBehavioral2019">[13]</a>, and develop <a href="https://docs.auto-pi-lot.com/en/latest/">one of them</a> so I will be brief at risk of repeating myself or appearing self-serving.</p>

<p>!! both these sections are also relatively unstandardized, so before jumping to some protocol just yet, we can build frameworks that start congealing the pieces en route to one.</p>

<p>Integrated scientific workflows have been written about many times before, typically in the context of the “open science” movement. One of the founders of the Center for Open Science, Jeffrey Spies, described a similar ethic of toolbuilding as I have in a 2017 presentation:</p>

<blockquote>
  <p>Open Workflow:</p>
  <ol>
    <li>Meet users where they are</li>
    <li>Respect current incentives</li>
    <li>Respect current workflow</li>
  </ol>

  <p>We could… demonstrate that it makes research more efficient, of higher quality, and more accessible.</p>

  <p>Better, we could… demonstrate that researchers will get published more often.</p>

  <p>Even better, we could… make it easy</p>

  <p>Best, we could… make it automatic <a class="citation" href="#spiesWorkflowCentricApproachIncreasing2017">[132]</a></p>
</blockquote>

<p>To build an infrastructural system that enables “open” practices, <em>convincing</em> or <em>mandating</em> a change are much less likely to be successful and sustainable than focusing on building them to make doing work easier and openness automatic. To make this possible, we should focus on developing <em>frameworks to build</em> experimental and analysis tools, rather than developing more tools themselves.</p>

<h3 id="analytical-framework">Analytical Framework</h3>

<p>The first natural companion of shared data infrastructure is a shared analytical framework. A major driver for the need for everyone to write their own analysis code largely from scratch is that it needs to account for the idiosyncratic structure of everyone’s data. Most scientists are (blessedly) not trained programmers, so code for loading and negotiating loading data is often intertwined with the code used to analyze and plot it. As a result it is often difficult to repurpose code for other contexts, so the same analysis function is rewritten in each lab’s local analysis repository. Since sharing raw data and code is still a (difficult) novelty, on a broad scale this makes results in scientific literature as reliable as we imagine all the private or semi-private analysis code to be.</p>

<p>Analytical tools (anecdotally) make up the bulk of open source scientific software, and range from foundational and general-purpose tools like numpy <a class="citation" href="#harrisArrayProgrammingNumPy2020">[133]</a> and scipy <a class="citation" href="#virtanenSciPyFundamentalAlgorithms2020">[134]</a>, through tools that implement a class of analysis like DeepLabCut <a class="citation" href="#mathisDeepLabCutMarkerlessPose2018a">[16]</a> and scikit-learn <a class="citation" href="#pedregosaScikitlearnMachineLearning2011">[135]</a>, to tools for a specific technique like MoSeq <a class="citation" href="#wiltschkoRevealingStructurePharmacobehavioral2020">[136]</a> and DeepSqueak <a class="citation" href="#coffeyDeepSqueakDeepLearningbased2019">[137]</a>. The pattern of their use is then to build them into a custom analysis system that can then in turn range in sophistication from a handful of flash-drive-versioned scripts to automated pipelines.</p>

<p>Having tools like these of course puts researchers miles ahead of where they would be without them, and the developers of the mentioned tools have put in a tremendous amount of work to build sensible interfaces and make them easier to use. No matter how much good work might be done, inevitable differences between APIs is a relatively sizable technical challenge for researchers — a problem compounded by the incentives for fragmentation described previously. For toolbuilders, many parts of any given tool from architecture to interface have to be redesigned with varying degrees of success each time. For science at large, with few exceptions of well-annotated and packaged code, most results are only replicable with great effort.</p>

<p>To be clear, we have reached levels of “not the developer’s fault” to the tune of “API discontinuity” being <em>“the norm for 99% of software.”</em> Negotiating boundaries between (and even within) software and information structures is an elemental part of computing. The only time it becomes a conceivable problem to “solve” is when the problem domain coalesces to the point where it is possible to articulate its abstract structure as a protocol, and the incentives are great enough to adopt it. Thankfully that’s what we’re trying to do here.</p>

<p>It’s unlikely that we will solve the problem of data analysis being complicated, time consuming, and error prone by teaching every scientist to be a good programmer, but we can build experimental frameworks that make analysis tools easier to build and use.</p>

<p>Specifically, a shared analytical framework should be</p>

<ul>
  <li><strong>Modular</strong> - Rather than implementing an entire analysis pipeline as a monolith, the system should be broken into minimal, composable modules. The threshold of what constitutes “minimal” is of course to some degree a matter of taste, but the framework doesn’t need to make normative decisions like that. The system should support modularity by providing a clear set of hooks that tools can provide: eg. a clear place for a given tool to accept some input, parameters, and so on. Since data analysis can often be broken up into a series of relatively independent stages, a straightforward (and common) system for modularity is to build hooks to make a directed acyclic graph (DAG) of data transformation operations. This structure naturally lends itself to many common problems: caching intermediate results, splitting and joining multiple inputs and outputs, distributing computation over many machines, among others. Modularity is also needed within the different parts of the system itself – eg. running an analysis chain shouldn’t require a GUI, but one should be available, etc.</li>
  <li><strong>Pluggable</strong> - The framework needs to provide a clear way of incorporating external analysis packages, handling their dependencies, and exposing their parameters to the user. Development should ideally not be limited to a single body of code with a single mode of governance, but should instead be relatively conservative about requirements for integrating code, and liberal with the types of functionality that can be modified with a plugin. Supporting plugins means supporting people developing tools for the framework, so it needs to make some part of the toolbuilding process easier or otherwise empower them relative to an independent package. This includes building a visible and expressive system for submitting and indexing plugins so they can be discovered and credit can be given to the developers. Reciprocal to supporting plugins is being interoperable with existing and future systems, which the reader may have assumed was a given by now.</li>
  <li><strong>Deployable</strong> - For wide use, the framework needs to be easy to install and deploy locally and on computing clusters. A primary obstacle is dependency management, or making sure that the computer has everything needed to run the program. Some care needs to be taken here, as there are multiple emphases in deployability that can be in conflict. Deployable for who? A system that can be relatively challenging to use for routine exploratory data analysis but can distribute analysis across 10,000 GPUs has a very circumscribed set of people it is useful for. This is a matter of balancing design constraints, but we should prioritize broad access, minimal assumptions of technological access, and ease of use over being able to perform the most computationally demanding analyses possible when in conflict. Containerization is a common, and the most likely strategy here, but the interface to containers may need a lot of care to make accessible compared to opening a fresh .py file.</li>
  <li><strong>Reproducible</strong> - The framework should separate the <em>parameterization</em> of a pipeline, the specific options set by the user, and its <em>implementation</em>, the code that constitutes it. The parameterization of a pipeline or analysis DAG should be portable such that it, for example, can be published in the supplementary materials of a paper and reproduced exactly by anyone using the system. The isolation of parameters from implementation is complementary to the separation of metadata from data and if implemented with semantic triplets would facilitate a continuous interface from our data to analysis system. This will be explored further below and in <a href="#shared-knowledge">shared knowledge</a></li>
</ul>

<p>Thankfully a number of existing projects that are very similar to this description are actively being built. One example is <a href="https://datajoint.io/">DataJoint</a> <a class="citation" href="#yatsenkoDataJointSimplerRelational2018">[138]</a>, which recently expanded its facility for modularity with its recent <a href="https://github.com/datajoint/datajoint-elements">Elements</a> project <a class="citation" href="#yatsenkoDataJointElementsData2021">[139]</a>. Datajoint is a system for creating analysis pipelines built from a graph of processing stages (among <a href="https://docs.datajoint.org/python/v0.13/intro/01-Data-Pipelines.html#what-is-datajoint">other features</a>). It is designed around a refinement on traditional relational data models, which is reflected throughout the system as most operations being expressed in its particular schema, data manipulation, and query languages. This is useful for operations that are expressed in the system, but makes it harder to integrate external tools with their dependencies — <a href="https://github.com/datajoint/element-array-ephys/blob/1fdbcf12d1a518e686b6b79e9fbe77b736cb606a/Background.md">at the moment</a> it appears that spike sorting (with <a href="https://github.com/MouseLand/Kilosort">Kilosort</a> <a class="citation" href="#pachitariuKilosortRealtimeSpikesorting2016">[140]</a>) has to happen outside of the extracellular electrophysiology elements pipeline.</p>

<p>Kilosort is an excellent and incredibly useful tool, but its idiomatic architecture designed for standalone use is illustrative of the challenge of making a general-purpose analytic framework that can integrate a broad array of existing tools. It is built in MATLAB, which requires a paid license, making arbitrary deployment difficult, and MATLAB’s flat path system requires careful and usual manual orchestration of potentially conflicting names in different packages. Its parameterization and use are combined in a “<a href="https://github.com/MouseLand/Kilosort/blob/db3a3353d9a374ea2f71674bbe443be21986c82c/main_kilosort3.m">main</a>” script in the repository root that creates a MATLAB struct and runs a series of functions — requiring some means for a wrapping framework to translate between input parameters and the representation expected by the tool. Its preprocessing script combines <a href="https://github.com/MouseLand/Kilosort/blob/a1fccd9abf13ce5dc3340fae8050f9b1d0f8ab7a/preProcess/datashift.m#L74-L77">I/O</a>, preprocessing, and <a href="https://github.com/MouseLand/Kilosort/blob/a1fccd9abf13ce5dc3340fae8050f9b1d0f8ab7a/preProcess/datashift.m#L57-L68">plotting</a>, and requires data to be <a href="https://github.com/MouseLand/Kilosort/blob/a1fccd9abf13ce5dc3340fae8050f9b1d0f8ab7a/preProcess/preprocessDataSub.m#L82-L84">loaded from disk</a> rather than passed as arguments to preserve memory — making chaining in a pipeline difficult.</p>

<p>This is not a criticism of Datajoint or Kilosort, which were both designed for different uses and with different philosophies (that are of course, also valid). I mean this as a brief illustration of the design challenges and tradeoffs of these systems.</p>

<p>We can start getting a better picture for the way a decentralized analysis framework might work by considering the separation between the metadata and code modules, hinting at a protocol as in the federated systems sketh above. Since we’re considering modular analysis elements, each module would need some elemental properties like the parameters that define it, its inputs, outputs, dependencies, as well as some additional metadata about its implementation (eg. this one takes <em>numpy arrays</em> and this one takes <em>matlab structs</em>). The precise implementation of a modular protocol also depends on the graph structure of the analysis system. We invoked DAGs before, but analysis graph structure of course has its own body of researchers refining them into eg. <a href="https://en.wikipedia.org/wiki/Petri_net">Petri nets</a> which are graphs whose nodes necessarily alternate between “places” (eg. intermediate data) and “transitions” (eg. an analysis operation), and their related workflow markup languages (eg. <a href="https://openwdl.org/">WDL</a> or <a class="citation" href="#vanderaalstYAWLAnotherWorkflow2005">[141]</a>). In that scheme, a framework could provide tools for converting data between types, caching intermediate data, etc. between analysis steps, as an example of how different graph structures might influence its implementation.</p>

<p>Say we use <code class="language-plaintext highlighter-rouge">@analysis</code> as the namespace for our analysis protocol, and <code class="language-plaintext highlighter-rouge">~someone~</code> has provided mappings to objects in <code class="language-plaintext highlighter-rouge">numpy</code>. We can assume they are provided by the package maintainers, but that’s not necessary: this is my node and it takes what I want it to!</p>

<p>In pseudocode, I could define some analysis node for, say, converting an RGB image to grayscale under my namespace as <code class="language-plaintext highlighter-rouge">@jonny:bin-spikes</code> like this:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>&lt;#bin-spikes&gt;
  a @analysis:node
    Version &gt;=1.0.0

  hasDescription
    "Convert an RGB Image to a grayscale image"

  inputType
    @numpy:ndarray
      # ... some spec of shape ...

  outputType
    @numpy:ndarray
      # ... some spec of shape ...
</code></pre></div></div>

<p>I have abbreviated the specification of shape to not overcomplicate the pseudocode example, but say we successfully specify a 3 dimensional (width x height x channels) array with 3 channels as input, and a a 2 dimensional (width x height) array as output.</p>

<p>The code doesn’t run on nothing! We need to specify our node’s dependencies, say in this case we need to specify an operating system image <code class="language-plaintext highlighter-rouge">ubuntu</code>, a version of <code class="language-plaintext highlighter-rouge">python</code>, a system-level package <code class="language-plaintext highlighter-rouge">opencv</code>, and a few python packages on <code class="language-plaintext highlighter-rouge">pip</code>. We are pinning specific versions with <a href="https://semver.org/">semantic versioning</a>, but the syntax isn’t terribly important. Then we just need to specify where the code for the node itself comes from:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  dependsOn
    @ubuntu:^20.*:x64
    @python:3.8
    @apt:opencv:^4.*.*
    @pip:opencv-python:^4.*.*
    @pip:numpy:^14.*.*

  providedBy
    @git:repository https://mygitserver.com/binspikes/fast-binspikes.git
      @git:hash fj9wbkl
    @python:class /main-module/binspikes.py:Bin_Spikes
</code></pre></div></div>

<p>Here we can see the advantage of being able to mix and match different namespaces in a practical sense. Our <code class="language-plaintext highlighter-rouge">@analysis.node</code> protocol gives us several slots to connect different tools together, each in turn presumably provides some minimal functionality expected by that slot: eg. <code class="language-plaintext highlighter-rouge">inputType</code> can expect <code class="language-plaintext highlighter-rouge">@numpy:ndarray</code> to specify its own dependencies, the programming language it is written in, shape, data type, and so on. Coercing data between chained nodes then becomes a matter of mapping between the <code class="language-plaintext highlighter-rouge">@numpy</code> and, say a <code class="language-plaintext highlighter-rouge">@nwb</code> namespace of another format. In the same way that there can be multiple, potentially overlapping between data schemas, it would then be possible for people to implement mappings between intermediate data formats as-needed.</p>

<p>This node also becomes available to extend, say someone wanted to add an additional input format to my node:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>&lt;@friend#bin-spikes&gt;
  a @jonny:bin-spikes

  inputType
    @pandas:DataFrame

  providedBy
    ...
</code></pre></div></div>

<p>They don’t have to interact with my potentially messy codebase at all, but it is automatically linked to my work so I am credited. One could imagine a particular analysis framework implementation that would then search through extensions of a particular node for a version that supports the input/output combinations appropriate for their analysis pipeline, so the work is cumulative. This functions as a dramatic decrease in the size of a unit of work that can be shared.</p>

<p>This also gives us healthy abstraction over implementation. Since the functionality is provided by different, mutable namespaces, we’re not locked into any particular piece of software — even our <code class="language-plaintext highlighter-rouge">@analysis</code> namespace that gives the <code class="language-plaintext highlighter-rouge">inputType</code> etc. slots could be forked. We could implement the dependency resolution system as, eg. a docker container, but it also could be just a check on the local environment if someone is just looking to run a small analysis on their laptop with those packages already installed.</p>

<p>We use providedBy to indicate a python class which implements the node in code. We could use an <code class="language-plaintext highlighter-rouge">Example_Framework</code> that provides a set of classes and methods to implement the different parts of the node (a la <a href="https://luigi.readthedocs.io/en/stable/tasks.html">luigi</a>). Our <code class="language-plaintext highlighter-rouge">Bin</code> class inherits from <code class="language-plaintext highlighter-rouge">Node</code>, and we implement the logic of the function by overriding its <code class="language-plaintext highlighter-rouge">run</code> method and specify an output file to store intermediate data (if requested by the pipeline) with an <code class="language-plaintext highlighter-rouge">output</code> method. We also specify a <code class="language-plaintext highlighter-rouge">bin_width</code> as a <code class="language-plaintext highlighter-rouge">Param</code>eter for our node, as an example of how a lightweight protocol could be bidirectionally specified: we could receive a parameterization from our pseudocode specification, or we could write a framework with a <code class="language-plaintext highlighter-rouge">Bin.export_schema()</code> that constructs the pseudocode specification from code.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">Example_Framework</span> <span class="kn">import</span> <span class="n">Node</span><span class="p">,</span> <span class="n">Param</span><span class="p">,</span> <span class="n">Target</span>

<span class="k">class</span> <span class="nc">Bin</span><span class="p">(</span><span class="n">Node</span><span class="p">):</span>
  <span class="n">bin_width</span> <span class="o">=</span> <span class="n">Param</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="nb">int</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>

  <span class="k">def</span> <span class="nf">output</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Target</span><span class="p">:</span>
    <span class="k">return</span> <span class="n">Target</span><span class="p">(</span><span class="s">'temporary_data.pck'</span><span class="p">)</span>

  <span class="k">def</span> <span class="nf">run</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">:</span><span class="s">'numpy.ndarray'</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="s">'numpy.ndarray'</span><span class="p">:</span>
    <span class="c1"># do some stuff
</span>    <span class="k">return</span> <span class="n">output</span>
</code></pre></div></div>

<p>Now that we have a handful of processing nodes, we could then describe some <code class="language-plaintext highlighter-rouge">@workflow</code>, taking some <code class="language-plaintext highlighter-rouge">@nwb:NWBFile</code> as input, and then returning some output as a <code class="language-plaintext highlighter-rouge">:processed</code> child beneath its existing namespace. We’ll only make a linear pipeline with two stages, but there’s no reason more complex branching and merging couldn’t be described as well.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>&lt;#my-analysis&gt;
  a @analysis:workflow

  inputType 
    @jonny:bin-spikes:inputType

  outputName
    .inputType:processed

  step Step1 @jonny:bin-spikes
  step Step2 @someone-else:another-step
    input Step1:output
</code></pre></div></div>

<p>Having kept the description of our data in particular abstract from the implementation of the code and the workflow specification, the only thing left is to apply it to our data! Since the parameters are linked from the analysis nodes, we can specify them here (or in the workflow). Assuming literally zero abstraction and using the tried-and-true “hardcoded dataset list” pattern, something like:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>&lt;#project-name&gt;
  a @analysis:project

  hasDescription
    "I gathered some data, and it is great!"

  researchTopic
    @neuro:systems:auditory:speech-processing
    @linguistics:phonetics:perception:auditory-only

  inPaper
    @doi:10.1121:1.5091776 

  workflow Analysis1 @jonny:my-analysis
    globalParams
      .Step1:params:bin_width 10

    datasets
      @jonny.mydata1:v0.1.0:raw
      @jonny.mydata2:^0.2.*:raw
      @jonny.mydata3:&gt;=0.1.1:raw
</code></pre></div></div>

<p>And there we are! The missing parameters like <code class="language-plaintext highlighter-rouge">outputName</code> from our workflow can be filled in from the defaults filled in the workflow node. We get some inkling of where we’re going later by also being able to specify the paper this data is associated with, as well as some broad categories of research topics so that our data as well as the results of the analysis can be found.</p>

<p>!! brief description of the state of the system at this point, we can link from data to analyses! reapply analyses across different datasets! and so on…</p>

<p>So that’s useful, but the faint residue of “well actually” that hangs in the air while people google the link for that xkcd comic about format expansion is not lost on me. The important part is in the way this hypothetical analysis framework and markup interact with our data system and emerging federated metadata system — The layers of abstraction here are worth unpacking, but we’ll hold until the end of the shared tools section and we have a chance to consider what this system might look like for experimental tools.</p>

<hr />

<div id="draftmarker"><h1># draftmarker</h1><br />~ everything past here is purely draft placeholder text ~  </div>

<h3 id="experimental-framework">Experimental Framework</h3>

<p>I’ve taken this out for now because it needs to be reworked dramatically, but it’s basically a lot of what I talk about in the autopilot manuscript, cross apply a lot of the same thinking from the previous section, and put it on the other side of the data where we’re feeding data directly from the tool into the data stream. The other part to note is that it becomes possible to make the same kinds of semantic links that we’re talking about from a semantic communication medium (next section) that can gather contextual knowledge, publications, etc. back to the code we used to run the experiments.</p>

<h3 id="collectivizing-the-state-of-the-art">Collectivizing the State of the Art</h3>

<p>!! how about just move this until after the tools section because i bet we’ll end up saying a lot of the same things.</p>

<ul>
  <li>First, the markup description of the node gives us abstraction from programming language and implementation. This lets us do stuff like use multiple tools with competing environmental needs, adapt to multiple versions of the code markup as it develops, etc. Note the interaction with the rest of the metadata system: because we required a particular type of data file, and that link should provide us some means of opening/instantiating the file with dependencies, we didn’t need to write loading code. Since it’s in a linked system, someone could override the implementation of my node – say someone comes up with a faster means of binning, then they just inherit from my node and replace the reference to the code. Boom we have cumulative and linked development.</li>
  <li>The separation of the node from the workflow means that the node can be shared and swapped and reintegrated easily, dramatically reducing the brittleness of the systme. Since there is no restriction on what constitutes a node, though, there’s no reason that nodes can’t be either made massive, like putting a whole library in the process method, or be packaged up together. If we made the argument and method names recursive between the workflow and the node objects then tooling could automatically traverse multiple layers of node/workflow combinations at different levels of abstraction. This being a schematic description means that there can be multiple “workflow runner” packages that eg. distribute the task across a billion supercomputers or not.</li>
  <li>
    <p>Finally, the separation between the data applied and the workflow itself are very cool indeed given our linked and namespaced system. My workflow effectively constitutes “an unit of analysis.” I have linked my data to this unit of analysis. Play out the permutations:</p>

    <ul>
      <li>I can see all the analyses that this particular pipeline has been applied to. Since it is embedded within the same federated system as our schema system, I can draw and connect semantic links to similar analysis pipelines as well as pipeline/data combinations.</li>
      <li>I can see all the different analyses that have been applied to my data: if my data is analyzed a zillion different times, in a zillion different combinations of data, I effectively get a “multiverse analysis” (cite dani) and we get to measure robustness of my data for free. It also gets to live forever and keep contributing to problems !! and i also get credited for it automatically by golly! This also applies on cases like cross-validation or evaluating different models on the same data: the versioning of it falls out naturally. Also since model weights would be an input to an analysis chain, we also get stuff like DLC’s model zoo where we can share different model weights, combine them, and have a cumulative library of pretrained models as well!</li>
      <li>being able to look across the landscape… we start being able to actually really make cumulative progress on best practices. A common admonishment in cryptographically-adjacent communities is to “never roll your own crypto,” because your homebrew crypto library will never be more secure than reference implementations that have an entire profession of people trying to expose and patch their weaknesses. Bugs in analysis code that produce inaccurate results are inevitable and rampant <a class="citation" href="#millerScientistNightmareSoftware2006">[142, 143, 144, 145]</a>, but impossible to diagnose when every paper writes its own pipeline. A common analysis framework would be a single point of inspection for bugs, and facilitate re-analysis and re-evaluation of affected results after a patch.</li>
      <li>looking forward, we might imagine our project object being linked to a DOI… we’ll get there.</li>
    </ul>
  </li>
</ul>

<p>!! this is all extraordinarily reproducible because even though I have my portable markup description of the analysis, I can just refer to it by name in my paper (ya ya need some content based hash or archive but you get the idea)</p>

<p>!! since we have a bunch of p2p systems all hooked up with constantly-running daemons, to compete with the compute side of cloud technology we also should implement a voluntary compute grid akin to  <a href="https://foldingathome.org/">Folding@Home</a>. This has the same strawmen and answers to them as the peer-to-peer system — no i’m not saying everyone puts their shitty GPU up, but it lets us combine the resources that are present at an institutional level and makes a very cheap onramp for government-level systems to be added to the mix.</p>

<p>!! this is all very exciting, and we can immediately start digging towards larger scientific problems, eg. what it would mean for the file drawer problem and publication bias when the barriers to analyzing data are so low you don’t even need to write the null result: the data is already there, semantically annotated and all. Dreams of infinite meta-analyses across all data and all time, but hold your horses! We don’t get magic for free, we haven’t talked about the community systems yet that are the unspoken glue of all of this!!</p>

<p>!! continue the example of needing to select within datasets instead of metadata from federation section.</p>

<h2 id="shared-knowledge">Shared Knowledge</h2>

<p>The remaining set of problems implied by the infrastructural system sketched in the preceding sections is the <em>communication</em> and <em>organization</em> systems that make up the interfaces to maintain and use it. We can finally return to some of the breadcrumbs laid before: negotiating over distributed and conflicting data schema, the potential forms of social systems for incentivizing and organizing labor discussed previously in the context of bittorrent trackers, and a means of maintaining the links between the broadly dispersed components of the infrastructure.</p>

<p>To do that we’ll trace a bit more of the history of the semantic web community, as well as the parallel but distinct history of the early wiki movement. We’ll land back in the realm of ActivityPub</p>

<p>!! we’ll describe a way to [summarize open spaces left by preceding sections] !! while a full consideration of the journal system as a whole is strictly out of scope of this paper, the system as developed renders it effectively irrelevant.</p>

<p>current status</p>
<ul>
  <li>our current systems are largely journals and conferences, but as evidenced by the unfortunate but widescale adoption of twitter, scientific communication naturally spans dispersed forms of communication at different scales of formality, length, public engagement, etc.</li>
  <li>when we go to organize ourselves, why is the best we can do google docs and slack?</li>
</ul>

<p>problems</p>
<ul>
  <li>Despite being, in some sense, an effort to organize and systematize human knowledge, <em>science effectively has no system of organization.</em></li>
  <li>The best we have is journals that are only loosely organized around large topic areas and citation networks, citation networks, which are un-annotated and un-descriptive, and unidimensional keywords.</li>
  <li>So one way of reading <a class="citation" href="#chuSlowedCanonicalProgress2021">[146]</a> is that there are too many papers, but another way of reading it is that our systems of organizing scientific communication simply don’t scale well since they rely mostly on scanning TOC alerts, google scholar, and happenstance.</li>
  <li>Another impact of the arcanity of scientific knowledge organization is that it is effectively impenetrable to people that aren’t domain experts. Why is trust in science so low right now? one contributor is that they have no idea what the hell we do or how different domains of knowledge have evolved. (cite cold war peer review and journals paper)</li>
  <li>Practically, this makes the quality of scientific literature constantly in question. Each paper effectively exists as an island, and engagement with prior literature is effectively optional (outside the minimum bar set by the 3-5 additional private peer reviewers, each with their own limited scope and conflicting interests). Forensic peer-reviewers have been ringing the alarm bell, saying that there is “no net” to bad research <a class="citation" href="#heathersRealScandalIvermectin2021">[147]</a>, and brave and highly-skilled investigators like <a href="https://scienceintegritydigest.com/">Elisabeth Bik</a> have found thousands of papers with evidence of purposeful manipulation <a class="citation" href="#shenMeetThisSuperspotter2020">[148, 149]</a>.</li>
  <li>So our existing systems of communication and organization are woefully inadequate for our needs, and don’t serve the role of guaranteeing consistency or reliability in research that they claim to.</li>
</ul>

<p>contextual knowledge needed</p>
<ul>
  <li>our limited systems of communication also render large sections of needed scientific communication without venue. The existing tools that <em>do</em> give some means of sharing technical knowledge are distinctly charity-driven, and don’t confer the same type of credit incentive that publications do.</li>
</ul>

<p>!! important of ease of leaving http://meatballwiki.org/wiki/RightToLeave</p>

<p>!! we’ve been tracing a distinction between the ability to express fluidly the contents of our reality with developing platforms that sift through it in an automated way, something that was an explicit cultural division throughout the semantic web project <a class="citation" href="#swartzTechniquesMassCollaboration2006">[150]</a>, which Peter Norvig (director of search at Google at the time) primarily attributes to user incompetence <a class="citation" href="#lombardiGoogleExecChallenges2007">[151]</a>. On trust, TBL says “Berners-Lee agreed with Norvig that deception on the Internet is a problem, but he argued that part of the Semantic Web is about identifying the originator of information, and identifying why the information can be trusted, not just the content of the information itself.”</p>

<p>!! more techniques of communtiy growth http://meatballwiki.org/wiki/RewardReputation</p>

<p>!! wikis work! but they can break when people get too much power! http://www.aaronsw.com/weblog/whorunswikipedia</p>

<p>There simply isn’t a place to have longform, thoughtful, durable discussions about science. The direct connection between the lack of a communcaition venue to the lack of a way of storing technical, contextual knowledge is often overlooked. Because we don’t have a place to talk about what we do, we don’t have a place to write down how to do it. Science needs a communcation platform, but the needs and constraints of a scientific communication platform are different than those satisfied by the major paradigms of chatrooms, forums etc. By considering this platform as another infrastructure project alongside and integrated with those described in the previous sections, its form becomes much clearer, and it could serve as the centerpiece of scientific infrastructure.</p>

<p>!! importantly, should also have means of ingest for existing tools and elements – easy to import existing papers and citation trees, plugins for existing data sharing systems.</p>

<p>!! description of its role as a schema resolution system – currently we implement all these protocols and standards in these siloed, centralized groups that are inherently slow to respond to changes and needs in the field. instead we want to give people the tools so that their the knowledge can be directly preserved and acted on.</p>

<p>!! descrption of its role as a tool of scientific discussion – integrated with the data server and standardized analysis pipelines, it could be possible to have a discussion board where we were able to pose novel scientific questions, answerable with transparent, interrogatable analysis systems. Semantic linking makes the major questions in the field possible to answer, as discussions are linked to one another in a structured way and it is possible to literally trace the flow of thought.</p>

<p>!! should trace the development of AP and the difficulty of doing these things as a way to explaining the ecosystem and the different parts that are needed in it: https://www.w3.org/TR/social-web-protocols/</p>

<h3 id="axes-of-communication-systems">Axes of Communication Systems</h3>

<p>!! we need a few things, but there’s no reason they should be different things! we need a system for</p>

<ul>
  <li>permanent communication for archiving</li>
  <li>durable communication like technical knowledge and scientific discourse proper, like a wiki</li>
  <li>rapid communication for like talking lmao – but not purposely temporary the way that social media is!</li>
</ul>

<p>each of these systems can have multiple iterations, with different rules, and so forth, and we should have control over our content and contribution to all of them (opt-in).</p>

<h3 id="the-wiki-way">The Wiki Way</h3>

<blockquote>
  <p>So that’s it — insecure but reliable, indiscriminate and subtle, user hostile yet easy to use, slow but up to date, and full of difficult, nit-picking people who exhibit a remarkable community camaraderie. Confused? Any other online community would count each of these “negatives” as a terrible flaw, and the contradictions as impossible to reconcile. Perhaps wiki works because the other online communities don’t. <a class="citation" href="#leufWikiWayQuick2001a">[152]</a></p>
</blockquote>

<p>!! wiki cultural history stuff!! differences in wiki philosophy, deletists vs not. !! meatball stuff on community maintenace, conflict resolution,</p>

<p>!! also hints at the problems, difficulties with wiki culture</p>

<blockquote>
  <p>It’s not too late to turn things around. Specs could be moved back into the wiki until they’re nearly done. Editors, instead of being gatekeepers, could be helpful moderators. A clear process for making controvertial decisions could be decided on. And the validator could follow consensus instead of leading it. But do the people running the show want this?</p>

  <p>Standards bodies tread a fine line between organizations for the public good and shelters for protecting collusion that would be otherwise illegal under antitrust law. For the dominent vendors involved, the goal is to give the illusion of openness while giving themselves full control to enforce their will behind the scenes.</p>

  <p>The IETF is a good example of this. Often lauded by the public as a model of openness and and and freedom, the reality is that working group chairs, appointed by a self-elected ruling board, get away with declaring whatever they want (usually an inferior and difficult to implement alternative) as “rough consensus”, routinely ignoring comments from the public and objections from working group members. One working group (in charge of DNS extentsions) went so far as to censor mail from working group members. The dictators running the IETF, when informed, didn’t seem to mind.</p>

  <p>Is the same sort of thing at work in the Pie/Echo/Atom Project? It appears so at first glance: Sam running the show from behind the scenes, putting friends in charge of the specs (although that isn’t what actually happened). The lack of a dispute-resolution process only makes things worse: when there’s no clear guide on how to make decisions or contributions, it’s far from obvious how to challenge a decision Sam has made. <a class="citation" href="#swartzSecretsStandards2003">[153]</a></p>
</blockquote>

<p>!! give the example of the autopilot wiki</p>

<p>!! contextual knowledge stuff in this section, theory wiki stuff in next section</p>

<blockquote>
  <p>Two essential features coordinate this information to better serve our organizational decision-making, learning, and memory. The first is our constellation of Working Groups that maintain and distribute local, specialized knowledge to other groups across the network. […] A second, more emergent property is the subgroup of IBL researchers who have become experts, liaisons, and interpreters of knowledge across the network. These members each manage a domain of explicit records (e.g., written protocols) and tacit information (e.g., colloquialisms, decision histories) that are quickly and informally disseminated to address real-time needs and problems. A remarkable nimbleness is afforded by this system of rapid responders deployed across our web of Working Groups. However, this kind of internalized knowledge can be vulnerable to drop-out when people leave the collaboration, and can be complex to archive. An ongoing challenge for our collaboration is how to archive both our explicit and tacit processes held in both people and places. This is not only to document our own history but as part of a roadmap for future science teams, whose dynamics are still not fully understood. <a class="citation" href="#woolKnowledgeNetworksHow2020">[7]</a></p>
</blockquote>

<p><a class="citation" href="#kamelboulosSemanticWikisComprehensible2009">[154]</a></p>

<p>!! Read and cite! <a class="citation" href="#classeDistributedInfrastructureSupport2017">[155]</a></p>

<p>!! <a class="citation" href="#goodSocialTaggingLife2009">[156]</a></p>

<p>!! wikibase can do federated SPARQL queries https://wikiba.se/</p>
<ul>
  <li>and has been used to make folksonomies https://biss.pensoft.net/article/37212/</li>
</ul>

<p>!! lots of scientific wikis</p>
<ul>
  <li>https://en.wikipedia.org/wiki/Wikipedia:WikiProject_Molecular_Biology/Genetics/Gene_Wiki/Other_Wikis</li>
  <li>https://en.wikipedia.org/wiki/Wikipedia:WikiProject_Molecular_Biology/Genetics/Gene_Wiki</li>
</ul>

<p>!! bids is doing something like this https://nidm-terms.github.io/</p>

<p>!! interlex</p>

<blockquote>
  <p>The Semantic Web is about two things. It is about common formats for integration and combination of data drawn from diverse sources, where on the original Web mainly concentrated on the interchange of documents. It is also about language for recording how the data relates to real world objects. That allows a person, or a machine, to start off in one database, and then move through an unending set of databases which are connected not by wires but by being about the same thing. https://www.w3.org/2001/sw/</p>
</blockquote>

<p>!! Semantic combination of databases in science are also not new <a class="citation" href="#cheungSemanticWebApproach2007">[157, 130]</a>. We need both though! semantic federated databases!</p>

<p>!! let’s tour through wikipedia for a second and see how it’s organized. Look at these community incentive structures and the huge macro-to-micro level organization of the wiki projects. The infinitely mutable nature of a wiki is what makes it powerful, but the SaaS wikis we’re familiar with don’t capture the same kind of ‘build the ground you walk on ‘ energy of the real wiki movement.</p>

<h3 id="rebuilding-scientific-communication">Rebuilding Scientific Communication</h3>

<p>!! take stock of our communication technology, we publish pdfs in journals, have science twitter, and then a bunch of private slacks and smalltime stuff??? Science is fundamentally a communicative process, literally every part fo the system that I have described has been built aroudn the ability to express the structure of things, the order of things, how it relates to other things and <em>that’s communication baby.</em> The system we’ve imagined so far takes us so far from forums and the ultradominant feed -&gt; shallow thread-based communication that we’re used to though. This is a system where we can have continuous dialogue about linked topics, be able to branch and see the reflections and subtle variations on ideas in the same place that we have our data, analysis, and tools.</p>

<p>!! theory wiki example from presentation</p>

<p>!! discovery of papers for scientists as well as general public, being able to trace history.</p>

<blockquote>
  <p>Though frequently viewed as a product to finish, it is dynamic ontologies with associated process-building activities designed, developed, and deployed locally that will allow ontologies to grow and to change. And finally, the technical activity of ontology building is always coupled with the background work of identifying and informing a broader community of future ontology users. <a class="citation" href="#bowkerInformationInfrastructureStudies2010">[1]</a></p>
</blockquote>

<p>!! stop sweating about computational accuracy and completeness. the only danger is a system that makes appeal to perfection and promises accuracy like those sold in golden foil by the platform capitalists. if we are conceptualizing this appropriately as a <em>system of communication</em> where particular results are intended to be <em>interpreted in context</em> then we would treat computational errors and semantic inaccuracies like we do with <em>language</em>: like a <em>joke</em>.</p>

<blockquote>
  <p>For example, one person may define a vehicle as having a number of wheels and a weight and a length, but not foresee a color. This will not stop another person making the assertion that a given car is red, using the color vocabular from elsewhere. - https://www.w3.org/DesignIssues/RDB-RDF.html</p>
</blockquote>

<blockquote>
  <p>Relational database systems, manage RDF data, but in a specialized way. In a table, there are many records with the same set of properties. An individual cell (which corresponds to an RDF property) is not often thought of on its own. SQL queries can join tables and extract data from tables, and the result is generally a table. So, the practical use for which RDB software is used typically optimized for soing operations with a small number of tables some of which may have a large number of elements.</p>

  <p>RDB systems have datatypes at the atomic (unstructured) level, as RDF and XML will/do. Combination rules tend in RDBs to be loosely enforced, in that a query can join tables by any comlumns which match by datatype – without any check on the semantics. You could for example create a list of houses that have the same number as rooms as an employee’s shoe size, for every employee, even though the sense of that would be questionable.</p>

  <p>The Semantic Web is not designed just as a new data model - it is specifically appropriate to the linking of data of many different models. One of the great things it will allow is to add information relating different databases on the Web, to allow sophisticated operations to be performed across them. https://www.w3.org/DesignIssues/RDFnot.html</p>
</blockquote>

<p>!! caution about slipping into techno-utopianism even here, we need the UI and tooling here to be simple to not only use but also build on. yes that does mean yet another framework! but this one is the most mythical yet, because I don’t really know what it would look like! but bad UI has killed lots of projects, eg. IPFS (though it’s not dead just slow!)
 https://macwright.com/2019/06/08/ipfs-again.html
https://blog.bluzelle.com/ipfs-is-not-what-you-think-it-is-e0aa8dc69b</p>

<h3 id="credit-assignment">Credit Assignment</h3>

<p>the work of maintaining the system can’t be invisible, read &amp; cite <a class="citation" href="#classeDistributedInfrastructureSupport2017">[155, 1]</a></p>

<p>!! essentially all questions about “changing the system of science” inevitably lead to credit assignment, but in our system it is the same as provenance. We can give credit to all work from data production, analysis tooling, technical work, theoretical work, and so on that we currently do with just author lists. brief nod to semantic publishing, though a treatment of the journal system is officially out of scope.</p>

<h1 id="conclusion">Conclusion</h1>

<p>!! summary of the system design</p>

<p>!! description of a new kind of scientific consensus <em>en toto</em></p>

<h2 id="shared-governance">Shared Governance</h2>

<p>!! the broad and uncertain future here is how this system will be goverened and how it will be oeprated. Though we design a system that decentralizes its operation, decentralizing power is not an automatic guarantee of the technology, so we need to remember the main question here is a refocusing of our culture <em>along with</em> refocusing our technology. We need to reconceptualize what we demand from our communication systems, how much power and agency we have over them, and how we relate with other scientists.</p>

<p>Dont want to be prescriptive here, but that we can learn from previous efforts like  https://en.wikipedia.org/wiki/Evergreen_(software) ,</p>

<p>!! multiplicity is in itself a form of governance, where rather than needing to canalize things into a single decision, it is possible to have all the options exist simultaneously and let history sort them out. http://meatballwiki.org/wiki/VotingIsEvil  http://meatballwiki.org/wiki/EnlargeSpace</p>

<h2 id="contrasting-visions-for-science">Contrasting visions for science</h2>

<p>!! through this text I have tried to sketch in parallel the vision of scientific practice as I see it heading now, into a platform capitalist hell, and an alternative, which is not a utopia but it is a place where we save a shitload of labor and (revisit the harms in the introduction).</p>

<h3 id="the-worst-platform-capitalist-world">The worst platform capitalist world</h3>

<p>!! ahh huh you know what it is</p>

<h3 id="what-we-could-hope-for">What we could hope for</h3>

<p>!! ya remake this description only less ivy and rosewaters and reintroduce some of the frustrations that might occur in the system. yno there are limitations but shit would actually genuinely be useful.</p>

<h1 id="references">References</h1>

<ol class="bibliography"><li><span id="bowkerInformationInfrastructureStudies2010">1. Bowker GC, Baker K, Millerand F, Ribes D (2010) Toward Information Infrastructure Studies: Ways of Knowing in a Networked Environment. <i>International Handbook of Internet Research</i>, :97–117. https://doi.org/10.1007/978-1-4020-9789-8_5</span></li>
<li><span id="tilsonDigitalInfrastructuresMissing2010">2. Tilson D, Lyytinen K, Sørensen C (2010) Digital Infrastructures: The Missing IS Research Agenda. <i>Information Systems Research</i>, 21(4):748–759. https://doi.org/10.1287/isre.1100.0318</span></li>
<li><span id="michicancivilrightscommissionFlintWaterCrisis2017">3. Commission MCR (2017) The Flint Water Crisis: Systemic Racism Through the Lens of Flint. https://web.archive.org/web/20210518020755/https://www.michigan.gov/documents/mdcr/VFlintCrisisRep-F-Edited3-13-17_554317_7.pdf</span></li>
<li><span id="mirowskiFutureOpenScience2018">4. Mirowski P (2018) The Future(s) of Open Science. <i>Social Studies of Science</i>, 48(2):171–203. https://doi.org/10.1177/0306312718772086</span></li>
<li><span id="ponziSciencePyramidScheme2020">5. Ponzi C (2020) Is Science a Pyramid Scheme? The Correlation between an Author’s Position in the Academic Hierarchy and Her Scientific Output per Year. https://doi.org/10.31235/osf.io/c3xg5</span></li>
<li><span id="bietzSustainingDevelopmentCyberinfrastructure2012">6. Bietz MJ, Ferro T, Lee CP (2012) Sustaining the Development of Cyberinfrastructure: An Organization Adapting to Change. <i>Proceedings of the ACM 2012 Conference on Computer Supported Cooperative Work</i>, :901–910. https://doi.org/10.1145/2145204.2145339</span></li>
<li><span id="woolKnowledgeNetworksHow2020">7. Wool LE, Laboratory TIB (2020) Knowledge across Networks: How to Build a Global Neuroscience Collaboration. https://doi.org/10.1016/j.conb.2020.10.020</span></li>
<li><span id="barleyBackroomsScienceWork1994">8. BARLEY STEPHENR, BECHKY BETHA (1994) In the Backrooms of Science: The Work of Technicians in Science Labs. <i>Work and Occupations</i>, 21(1):85–126. https://doi.org/10.1177/0730888494021001004</span></li>
<li><span id="bietzCollaborationMetagenomicsSequence2009">9. Bietz MJ, Lee CP (2009) Collaboration in Metagenomics: Sequence Databases and the Organization of Scientific Work. <i>ECSCW 2009</i>, :243–262. https://doi.org/10.1007/978-1-84882-854-4_15</span></li>
<li><span id="mainenBetterWayCrack2016">10. Mainen ZF, Häusser M, Pouget A (2016) A Better Way to Crack the Brain. <i>Nature News</i>, 539(7628):159. https://doi.org/10.1038/539159a</span></li>
<li><span id="bakerMaintainingDublinCore2005">11. Baker T (2005) Maintaining Dublin Core as a Semantic Web Vocabulary. <i>From Integrated Publication and Information Systems to Information and Knowledge Environments: Essays Dedicated to Erich J. Neuhold on the Occasion of His 65th Birthday</i>, :61–68. https://doi.org/10.1007/978-3-540-31842-2_7</span></li>
<li><span id="berners-leeSEMANTICWEB2001">12. BERNERS-LEE TIM, HENDLER JAMES, LASSILA ORA (2001) THE SEMANTIC WEB. <i>Scientific American</i>, 284(5):34–43. https://www.jstor.org/stable/26059207</span></li>
<li><span id="saundersAutopilotAutomatingBehavioral2019">13. Saunders JL, Wehr M (2019) Autopilot: Automating Behavioral Experiments with Lots of Raspberry Pis. <i>bioRxiv</i>, :807693. https://doi.org/10.1101/807693</span></li>
<li><span id="abbottInternationalLaboratorySystems2017">14. Abbott LF, Angelaki DE, Carandini M, Churchland AK, Dan Y, Dayan P, Deneve S, Fiete I, Ganguli S, Harris KD, Häusser M, Hofer S, Latham PE, Mainen ZF, Mrsic-Flogel T, Paninski L, Pillow JW, Pouget A, Svoboda K, Witten IB, Zador AM (2017) An International Laboratory for Systems and Computational Neuroscience. <i>Neuron</i>, 96(6):1213–1218. https://doi.org/10.1016/j.neuron.2017.12.013</span></li>
<li><span id="howisonIncentivesIntegrationScientific2013">15. Howison J, Herbsleb JD (2013) Incentives and Integration in Scientific Software Production. <i>Proceedings of the 2013 Conference on Computer Supported Cooperative Work</i>, :459–470. https://doi.org/10.1145/2441776.2441828</span></li>
<li><span id="mathisDeepLabCutMarkerlessPose2018a">16. Mathis A, Mamidanna P, Cury KM, Abe T, Murthy VN, Mathis MW, Bethge M (2018) DeepLabCut: Markerless Pose Estimation of User-Defined Body Parts with Deep Learning. <i>Nature Neuroscience</i>, 21(9):1281–1289. https://doi.org/10.1038/s41593-018-0209-y</span></li>
<li><span id="mangulImprovingUsabilityArchival2019">17. Mangul S, Martin LS, Eskin E, Blekhman R (2019) Improving the Usability and Archival Stability of Bioinformatics Software. <i>Genome Biology</i>, 20(1):47. https://doi.org/10.1186/s13059-019-1649-8</span></li>
<li><span id="kumarBioinformaticsSoftwareBiologists2007">18. Kumar S, Dudley J (2007) Bioinformatics Software for Biologists in the Genomics Era. <i>Bioinformatics</i>, 23(14):1713–1717. https://doi.org/10.1093/bioinformatics/btm239</span></li>
<li><span id="howisonSoftwareScientificLiterature2016">19. Howison J, Bullard J (2016) Software in the Scientific Literature: Problems with Seeing, Finding, and Using Software Mentioned in the Biology Literature. <i>Journal of the Association for Information Science and Technology</i>, 67(9):2137–2155. https://doi.org/10.1002/asi.23538</span></li>
<li><span id="NIHStrategicPlan2018">20. (2018) NIH Strategic Plan for Data Science. https://web.archive.org/web/20210907014444/https://datascience.nih.gov/sites/default/files/NIH_Strategic_Plan_for_Data_Science_Final_508.pdf</span></li>
<li><span id="ribesLongNowTechnology2009">21. Ribes D, Finholt T (2009) The Long Now of Technology Infrastructure: Articulating Tensions in Development. <i>Journal of the Association for Information Systems</i>, 10(5):375–398. https://doi.org/10.17705/1jais.00199</span></li>
<li><span id="altschulAnatomySuccessfulComputational2013">22. Altschul S, Demchak B, Durbin R, Gentleman R, Krzywinski M, Li H, Nekrutenko A, Robinson J, Rasband W, Taylor J, Trapnell C (2013) The Anatomy of Successful Computational Biology Software. <i>Nature Biotechnology</i>, 31(10):894–897. https://doi.org/10.1038/nbt.2721</span></li>
<li><span id="palmerDitchingSemanticWeb2008">23. Palmer SB (2008) Ditching the Semantic Web? http://inamidst.com/whits/2008/ditching</span></li>
<li><span id="gawerBridgingDifferingPerspectives2014">24. Gawer A (2014) Bridging Differing Perspectives on Technological Platforms: Toward an Integrative Framework. <i>Research Policy</i>, 43(7):1239–1249. https://doi.org/10.1016/j.respol.2014.03.006</span></li>
<li><span id="poirierTurnScruffyEthnographic2017">25. Poirier L (2017) A Turn for the Scruffy: An Ethnographic Study of Semantic Web Architecture. <i>Proceedings of the 2017 ACM on Web Science Conference</i>, :359–367. https://doi.org/10.1145/3091478.3091505</span></li>
<li><span id="swartzAaronSwartzProgrammable2013">26. Swartz A (2013) Aaron Swartz’s A Programmable Web: An Unfinished Work. <i>Synthesis Lectures on the Semantic Web: Theory and Technology</i>, 3(2):1–64. https://doi.org/10.2200/S00481ED1V01Y201302WBE005</span></li>
<li><span id="brembsReplacingAcademicJournals2021">27. Brembs B, Huneman P, Schönbrodt F, Nilsonne G, Susi T, Siems R, Perakakis P, Trachana V, Ma L, Rodriguez-Cuadrado S (2021) Replacing Academic Journals. https://doi.org/10.5281/zenodo.5526635</span></li>
<li><span id="macinnesCompatibilityStandardsMonopoly2005">28. MacInnes I (2005) Compatibility Standards and Monopoly Incentives: The Impact of Service-Based Software Licensing. <i>International Journal of Services and Standards</i>, 1(3):255–270. https://doi.org/10.1504/IJSS.2005.005799</span></li>
<li><span id="biddleLexisNexisProvideGiant2021">29. Biddle S (2021) LexisNexis to Provide Giant Database of Personal Information to ICE. <i>The Intercept</i>, https://theintercept.com/2021/04/02/ice-database-surveillance-lexisnexis/</span></li>
<li><span id="CriticismAmazon2021">30. (2021) Criticism of Amazon. <i>Wikipedia</i>, https://en.wikipedia.org/w/index.php?title=Criticism_of_Amazon&amp;oldid=1043543093</span></li>
<li><span id="buranyiStaggeringlyProfitableBusiness2017">31. Buranyi S (2017) Is the Staggeringly Profitable Business of Scientific Publishing Bad for Science? <i>The Guardian</i>, https://www.theguardian.com/science/2017/jun/27/profitable-business-scientific-publishing-bad-for-science</span></li>
<li><span id="ElsevierSevenBridges2017">32. (2017) Elsevier and Seven Bridges Receive NIH Data Commons Grant for Biomedical Data Analysis. https://www.elsevier.com/about/press-releases/archive/science-and-technology/elsevier-and-seven-bridges-receive-nih-data-commons-grant-for-biomedical-data-analysis</span></li>
<li><span id="reillyNIHSTRIDESInitiative2021">33. Reilly RT (2021) NIH STRIDES Initiative. https://web.archive.org/web/20211006011408/https://ncihub.org/resources/2422/download/21.01.08_NIH_STRIDES_Presentation.pptx</span></li>
<li><span id="STRIDESInitiativeSuccess2020">34. (2020) STRIDES Initiative Success Story: University of Michigan TOPMed | Data Science at NIH. https://web.archive.org/web/20210324024612/https://datascience.nih.gov/strides-initiative-success-story-university-michigan-topmed</span></li>
<li><span id="leinGenomewideAtlasGene2007">35. Lein ES, Hawrylycz MJ, Ao N, Ayres M, Bensinger A, Bernard A, Boe AF, Boguski MS, Brockway KS, Byrnes EJ, Chen L, Chen L, Chen T-M, Chi Chin M, Chong J, Crook BE, Czaplinska A, Dang CN, Datta S, Dee NR, Desaki AL, Desta T, Diep E, Dolbeare TA, Donelan MJ, Dong H-W, Dougherty JG, Duncan BJ, Ebbert AJ, Eichele G, Estin LK, Faber C, Facer BA, Fields R, Fischer SR, Fliss TP, Frensley C, Gates SN, Glattfelder KJ, Halverson KR, Hart MR, Hohmann JG, Howell MP, Jeung DP, Johnson RA, Karr PT, Kawal R, Kidney JM, Knapik RH, Kuan CL, Lake JH, Laramee AR, Larsen KD, Lau C, Lemon TA, Liang AJ, Liu Y, Luong LT, Michaels J, Morgan JJ, Morgan RJ, Mortrud MT, Mosqueda NF, Ng LL, Ng R, Orta GJ, Overly CC, Pak TH, Parry SE, Pathak SD, Pearson OC, Puchalski RB, Riley ZL, Rockett HR, Rowland SA, Royall JJ, Ruiz MJ, Sarno NR, Schaffnit K, Shapovalova NV, Sivisay T, Slaughterbeck CR, Smith SC, Smith KA, Smith BI, Sodt AJ, Stewart NN, Stumpf K-R, Sunkin SM, Sutram M, Tam A, Teemer CD, Thaller C, Thompson CL, Varnam LR, Visel A, Whitlock RM, Wohnoutka PE, Wolkey CK, Wong VY, Wood M, Yaylaoglu MB, Young RC, Youngstrom BL, Feng Yuan X, Zhang B, Zwingman TA, Jones AR (2007) Genome-Wide Atlas of Gene Expression in the Adult Mouse Brain. <i>Nature</i>, 445(7124):168–176. https://doi.org/10.1038/nature05453</span></li>
<li><span id="grillnerWorldwideInitiativesAdvance2016">36. Grillner S, Ip N, Koch C, Koroshetz W, Okano H, Polachek M, Poo M-ming, Sejnowski TJ (2016) Worldwide Initiatives to Advance Brain Research. <i>Nature Neuroscience</i>, 19(9):1118–1122. https://doi.org/10.1038/nn.4371</span></li>
<li><span id="kochBigScienceTeam2016">37. Koch C, Jones A (2016) Big Science, Team Science, and Open Science for Neuroscience. <i>Neuron</i>, 92(3):612–616. https://doi.org/10.1016/j.neuron.2016.10.019</span></li>
<li><span id="laboratoryStandardizedReproducibleMeasurement2020">38. Laboratory TIB, Aguillon-Rodriguez V, Angelaki DE, Bayer HM, Bonacchi N, Carandini M, Cazettes F, Chapuis GA, Churchland AK, Dan Y, Dewitt EEJ, Faulkner M, Forrest H, Haetzel LM, Hausser M, Hofer SB, Hu F, Khanal A, Krasniak CS, Laranjeira I, Mainen ZF, Meijer GT, Miska NJ, Mrsic-Flogel TD, Murakami M, Noel J-P, Pan-Vazquez A, Rossant C, Sanders JI, Socha KZ, Terry R, Urai AE, Vergara HM, Wells MJ, Wilson CJ, Witten IB, Wool LE, Zador A (2020) Standardized and Reproducible Measurement of Decision-Making in Mice. <i>bioRxiv</i>, :2020.01.17.909838. https://doi.org/10.1101/2020.01.17.909838</span></li>
<li><span id="laboratoryDataArchitectureLargescale2020">39. Laboratory TIB, Bonacchi N, Chapuis G, Churchland A, Harris KD, Hunter M, Rossant C, Sasaki M, Shen S, Steinmetz NA, Walker EY, Winter O, Wells M (2020) Data Architecture for a Large-Scale Neuroscience Collaboration. <i>bioRxiv</i>, :827873. https://doi.org/10.1101/827873</span></li>
<li><span id="lopesBonsaiEventbasedFramework2015">40. Lopes G, Bonacchi N, Frazão J, Neto JP, Atallah BV, Soares S, Moreira L, Matias S, Itskov PM, Correia PA, Medina RE, Calcaterra L, Dreosti E, Paton JJ, Kampff AR (2015) Bonsai: An Event-Based Framework for Processing and Controlling Data Streams. <i>Frontiers in Neuroinformatics</i>, 9https://doi.org/10.3389/fninf.2015.00007</span></li>
<li><span id="Rfc5321SimpleMail">41. Rfc5321 - Simple Mail Transfer Protocol. https://datatracker.ietf.org/doc/html/rfc5321#section-3</span></li>
<li><span id="clarkDesignPhilosophyDARPA1988">42. Clark D (1988) The Design Philosophy of the DARPA Internet Protocols. <i>Symposium Proceedings on Communications Architectures and Protocols</i>, :106–114. https://doi.org/10.1145/52324.52336</span></li>
<li><span id="carpenterRFC1958Architectural1996">43. Carpenter BE (1996) RFC 1958 - Architectural Principles of the Internet. https://tools.ietf.org/html/rfc1958</span></li>
<li><span id="berners-leePrinciplesDesign1998">44. Berners-Lee T (1998) Principles of Design. https://www.w3.org/DesignIssues/Principles.html#Decentrali</span></li>
<li><span id="grudinGroupwareSocialDynamics1994">45. Grudin J (1994) Groupware and Social Dynamics: Eight Challenges for Developers. <i>Communications of the ACM</i>, 37(1):92–105. https://doi.org/10.1145/175222.175230</span></li>
<li><span id="randallDistributedOntologyBuilding2011">46. Randall D, Procter R, Lin Y, Poschen M, Sharrock W, Stevens R (2011) Distributed Ontology Building as Practical Work. <i>International Journal of Human-Computer Studies</i>, 69(4):220–233. https://doi.org/10.1016/j.ijhcs.2010.12.011</span></li>
<li><span id="markoffTomorrowWorldWide1996">47. Markoff J (1996) Tomorrow, the World Wide Web!;Microsoft, the PC King, Wants to Reign Over the Internet. <i>The New York Times</i>, https://www.nytimes.com/1996/07/16/business/tomorrow-world-wide-web-microsoft-pc-king-wants-reign-over-internet.html</span></li>
<li><span id="teamScientificDataFormats">48. Team A Scientific Data Formats - Just Solve the File Format Problem. http://justsolve.archiveteam.org/wiki/Scientific_Data_formats</span></li>
<li><span id="rubelNWBAccessibleData2019a">49. Rübel O, Tritt A, Dichter B, Braun T, Cain N, Clack N, Davidson TJ, Dougherty M, Fillion-Robin J-C, Graddis N, Grauer M, Kiggins JT, Niu L, Ozturk D, Schroeder W, Soltesz I, Sommer FT, Svoboda K, Lydia N, Frank LM, Bouchard K (2019) NWB:N 2.0: An Accessible Data Standard for Neurophysiology. <i>bioRxiv</i>, :523035. https://doi.org/10.1101/523035</span></li>
<li><span id="rubelNeurodataBordersEcosystem2021">50. Rübel O, Tritt A, Ly R, Dichter BK, Ghosh S, Niu L, Soltesz I, Svoboda K, Frank L, Bouchard KE (2021) The Neurodata Without Borders Ecosystem for Neurophysiological Data Science. :2021.03.13.435173. https://doi.org/10.1101/2021.03.13.435173</span></li>
<li><span id="shenHandbookPeertoPeerNetworking2010">51. Shen X, Yu H, Buford J, Akon M (2010) Handbook of Peer-to-Peer Networking. </span></li>
<li><span id="cohenBitTorrentProtocolSpecification2017">52. Cohen B (2017) The BitTorrent Protocol Specification. https://www.bittorrent.org/beps/bep_0003.html</span></li>
<li><span id="roettgersPirateBayDistributing2009">53. Roettgers J (2009) The Pirate Bay: Distributing the World’s Entertainment for $3,000 a Month. <i>Gigaom</i>, https://gigaom.com/2009/07/19/the-pirate-bay-distributing-the-worlds-entertainment-for-3000-a-month/</span></li>
<li><span id="PirateBayArchiveteam2020">54. (2020) The Pirate Bay - Archiveteam. <i>Archive Team - The Pirate Bay</i>, https://wiki.archiveteam.org/index.php?title=The_Pirate_Bay&amp;oldid=45467</span></li>
<li><span id="spiesDataIntegrityLibrarians2017">55. Spies J (2017) Data Integrity for Librarians, Archivists, and Criminals: What We Can Steal from Bitcoin, BitTorrent, and Usenet. <i>CNI: Coalition for Networked Information</i>, https://www.cni.org/topics/digital-curation/data-integrity-for-librarians-archivists-and-criminals-what-we-can-steal-from-bitcoin-bittorrent-and-usenet</span></li>
<li><span id="kim15YearsPirate2019">56. Kim E (2019) After 15 Years, the Pirate Bay Still Can’t Be Killed. <i>MEL Magazine</i>, https://melmagazine.com/en-us/story/after-15-years-the-pirate-bay-still-cant-be-killed</span></li>
<li><span id="vandersarOpenBayNow2014">57. Van der Sar E (2014) The Open Bay: Now Anyone Can Run A Pirate Bay ’Copy.’ <i>TorrentFreak</i>, https://torrentfreak.com/open-bay-now-everyone-can-run-pirate-bay-copy-141219/</span></li>
<li><span id="vandersarWhatCdDead2016">58. Van der Sar E (2016) What.Cd Is Dead, But The Torrent Hydra Lives On. <i>TorrentFreak</i>, https://torrentfreak.com/what-cd-is-dead-but-the-torrent-hydra-lives-on-161202/</span></li>
<li><span id="scottGeocitiesTorrentUpdate2010">59. Scott J (2010) Geocities Torrent Update. <i>ASCII by Jason Scott</i>, http://ascii.textfiles.com/archives/2894</span></li>
<li><span id="rossiPeekingBitTorrentSeedbox2014">60. Rossi D, Pujol G, Wang X, Mathieu F (2014) Peeking through the BitTorrent Seedbox Hosting Ecosystem. <i>Traffic Monitoring and Analysis</i>, :115–126. https://doi.org/10.1007/978-3-642-54999-1_10</span></li>
<li><span id="hoffmanHTTPBasedSeedingSpecification">61. Hoffman J, DeHackEd HTTP-Based Seeding Specification. http://www.bittornado.com/docs/webseed-spec.txt</span></li>
<li><span id="kahle000000Torrents2012">62. Kahle B (2012) Over 1,000,000 Torrents of Downloadable Books, Music, and Movies. <i>Internet Archive Blogs</i>, http://blog.archive.org/2012/08/07/over-1000000-torrents-of-downloadable-books-music-and-movies/</span></li>
<li><span id="kreitzSpotifyLargeScale2010b">63. Kreitz G, Niemela F (2010) Spotify – Large Scale, Low Latency, P2P Music-on-Demand Streaming. <i>2010 IEEE Tenth International Conference on Peer-to-Peer Computing (P2P)</i>, :1–10. https://doi.org/10.1109/P2P.2010.5569963</span></li>
<li><span id="andreevBiologistsNeedModern2021">64. Andreev A, Morrell T, Briney K, Gesing S, Manor U (2021) Biologists Need Modern Data Infrastructure on Campus. <i>arXiv:2108.07631 [q-bio]</i>, http://arxiv.org/abs/2108.07631</span></li>
<li><span id="charlesCommunityDrivenBigOpen2020">65. Charles AS, Falk B, Turner N, Pereira TD, Tward D, Pedigo BD, Chung J, Burns R, Ghosh SS, Kebschull JM, Silversmith W, Vogelstein JT (2020) Toward Community-Driven Big Open Brain Science: Open Big Data and Tools for Structure, Function, and Genetics. <i>Annual Review of Neuroscience</i>, 43:441–464. https://doi.org/10.1146/annurev-neuro-100119-110036</span></li>
<li><span id="benetIPFSContentAddressed2014">66. Benet J (2014) IPFS - Content Addressed, Versioned, P2P File System. <i>arXiv:1407.3561 [cs]</i>, http://arxiv.org/abs/1407.3561</span></li>
<li><span id="ogdenDatDistributedDataset2017">67. Ogden M (2017) Dat - Distributed Dataset Synchronization And Versioning. https://doi.org/10.31219/osf.io/nsv2c</span></li>
<li><span id="patsakisHydrasIPFSDecentralised2019">68. Patsakis C, Casino F (2019) Hydras and IPFS: A Decentralised Playground for Malware. <i>International Journal of Information Security</i>, 18(6):787–799. https://doi.org/10.1007/s10207-019-00443-0</span></li>
<li><span id="zhangUnravelingBitTorrentEcosystem2011">69. Zhang C, Dhungel P, Wu D, Ross KW (2011) Unraveling the BitTorrent Ecosystem. <i>IEEE Transactions on Parallel and Distributed Systems</i>, 22(7):1164–1177. https://doi.org/10.1109/TPDS.2010.123</span></li>
<li><span id="clarkeFreenetDistributedAnonymous2001">70. Clarke I, Sandberg O, Wiley B, Hong TW (2001) Freenet: A Distributed Anonymous Information Storage and Retrieval System. <i>Designing Privacy Enhancing Technologies: International Workshop on Design Issues in Anonymity and Unobservability Berkeley, CA, USA, July 25–26, 2000 Proceedings</i>, :46–66. https://doi.org/10.1007/3-540-44702-4_4</span></li>
<li><span id="capadisliSolidProtocol2020">71. Capadisli S, Berners-Lee T, Verborgh R, Kjernsmo K, Bingham J, Zagidulin D (2020) Solid Protocol. https://solidproject.org/TR/protocol</span></li>
<li><span id="sambraSolidPlatformDecentralized2016">72. Sambra AV, Mansour E, Hawke S, Zereba M, Greco N, Ghanem A, Zagidulin D, Aboulnaga A, Berners-Lee T (2016) Solid: A Platform for Decentralized Social Applications Based on Linked Data. <i>MIT CSAIL &amp; Qatar Computing Research Institute, Tech. Rep.</i>, :16. </span></li>
<li><span id="SolidP2PFoundation">73. Solid - P2P Foundation. https://wiki.p2pfoundation.net/Solid</span></li>
<li><span id="basamanowiczReleaseGroupsDigital2011">74. Basamanowicz JR (2011) Release Groups and Digital Copyright Piracy. https://doi.org/10/etd6644_JBasamanowicz.pdf</span></li>
<li><span id="hindujaDeindividuationInternetSoftware2008">75. Hinduja S (2008) Deindividuation and Internet Software Piracy. <i>CyberPsychology &amp; Behavior</i>, 11(4):391–398. https://doi.org/10.1089/cpb.2007.0048</span></li>
<li><span id="dunhamWhatCDLegacy2018">76. Dunham I (2018) What.CD: A Legacy of Sharing. https://doi.org/10.7282/T3V128F3</span></li>
<li><span id="rosenDayMusicBurned2019">77. Rosen J (2019) The Day the Music Burned. <i>The New York Times</i>, https://www.nytimes.com/2019/06/11/magazine/universal-fire-master-recordings.html</span></li>
<li><span id="sonnadEulogyWhatCd2016">78. Sonnad N (2016) A Eulogy for What.Cd, the Greatest Music Collection in the History of the World—until It Vanished. <i>Quartz</i>, https://qz.com/840661/what-cd-is-gone-a-eulogy-for-the-greatest-music-collection-in-the-world/</span></li>
<li><span id="meulpolderPublicPrivateBitTorrent">79. Meulpolder M, D’Acunto L, Capota M, Wojciechowski M, Pouwelse JA, Epema DHJ, Sips HJ Public and Private BitTorrent Communities: A Measurement Study. :5. </span></li>
<li><span id="jiaHowSurviveThrive2013">80. Jia AL, Chen X, Chu X, Pouwelse JA, Epema DHJ (2013) How to Survive and Thrive in a Private BitTorrent Community. <i>Distributed Computing and Networking</i>, :270–284. https://doi.org/10.1007/978-3-642-35668-1_19</span></li>
<li><span id="liuUnderstandingImprovingRatio2010">81. Liu Z, Dhungel P, Wu D, Zhang C, Ross KW (2010) Understanding and Improving Ratio Incentives in Private Communities. <i>2010 IEEE 30th International Conference on Distributed Computing Systems</i>, :610–621. https://doi.org/10.1109/ICDCS.2010.90</span></li>
<li><span id="kashEconomicsBitTorrentCommunities2012">82. Kash IA, Lai JK, Zhang H, Zohar A (2012) Economics of BitTorrent Communities. <i>Proceedings of the 21st International Conference on World Wide Web</i>, :221–230. https://doi.org/10.1145/2187836.2187867</span></li>
<li><span id="chenImprovingSustainabilityPrivate2011a">83. Chen X, Chu X, Li Z (2011) Improving Sustainability of Private P2P Communities. <i>2011 Proceedings of 20th International Conference on Computer Communications and Networks (ICCCN)</i>, :1–6. https://doi.org/10.1109/ICCCN.2011.6005944</span></li>
<li><span id="fecherReputationEconomyHow2017">84. Fecher B, Friesike S, Hebing M, Linek S (2017) A Reputation Economy: How Individual Reward Considerations Trump Systemic Arguments for Open Access to Data. <i>Palgrave Communications</i>, 3(1):1–10. https://doi.org/10.1057/palcomms.2017.51</span></li>
<li><span id="brossCommunityCollaborationContribution2013">85. Bross J (2013) Community, Collaboration and Contribution: Evaluating a BitTorrent Tracker as a Digital Library. https://doi.org/10.17615/g1cw-kw06</span></li>
<li><span id="langilleBioTorrentsFileSharing2010">86. Langille MGI, Eisen JA (2010) BioTorrents: A File Sharing Service for Scientific Data. <i>PLoS ONE</i>, 5(4)https://doi.org/10.1371/journal.pone.0010071</span></li>
<li><span id="cohenAcademicTorrentsCommunityMaintained2014">87. Cohen JP, Lo HZ (2014) Academic Torrents: A Community-Maintained Distributed Repository. <i>Proceedings of the 2014 Annual Conference on Extreme Science and Engineering Discovery Environment</i>, :1–2. https://doi.org/10.1145/2616498.2616528</span></li>
<li><span id="ceustersFoundationsRealistOntology2010">88. Ceusters W, Smith B (2010) Foundations for a Realist Ontology of Mental Disease. <i>Journal of Biomedical Semantics</i>, 1(1):10. https://doi.org/10.1186/2041-1480-1-10</span></li>
<li><span id="consortiumBiomedicalDataTranslator2019">89. Consortium TBDT (2019) The Biomedical Data Translator Program: Conception, Culture, and Community. <i>Clinical and Translational Science</i>, 12(2):91–94. https://doi.org/10.1111/cts.12592</span></li>
<li><span id="fleisherOtherTransactionAward2019">90. Fleisher S (2019) Other Transaction Award Policy Guide - Biomedical Data Translator Program. :38. </span></li>
<li><span id="consortiumUniversalBiomedicalData2019">91. Consortium TBDT (2019) Toward A Universal Biomedical Data Translator. <i>Clinical and Translational Science</i>, 12(2):86–90. https://doi.org/10.1111/cts.12591</span></li>
<li><span id="bruskiewichBiolinkBiolinkmodel2021">92. Bruskiewich R, Deepak, Moxon S, Mungall C, Solbrig H, cbizon, Brush M, Shefchek K, Hannestad L, YaphetKG, Harris N, bbopjenkins, diatomsRcool, Wang P, Balhoff J, Schaper K, XIN JIWEN, Owen P, Stupp G, JervenBolleman, Badger TG, Emonet V, vdancik (2021) Biolink/Biolink-Model: 2.2.5. https://zenodo.org/record/5520104</span></li>
<li><span id="goelExplanationContainerCaseBased2021">93. Goel P, Johs AJ, Shrestha M, Weber RO (2021) Explanation Container in Case-Based Biomedical Question-Answering. :10. https://web.archive.org/web/*/https://gaia.fdi.ucm.es/events/xcbr/papers/ICCBR_2021_paper_100.pdf</span></li>
<li><span id="ROBOKOPCoVar2021">94. (2021) ROBOKOP - CoVar. https://web.archive.org/web/20211006030919/https://covar.com/case-study/robokop/</span></li>
<li><span id="ramTransphobiaEncodedExamination2021">95. Ram A, Kronk CA, Eleazer JR, Goulet JL, Brandt CA, Wang KH (2021) Transphobia, Encoded: An Examination of Trans-Specific Terminology in SNOMED CT and ICD-10-CM. <i>Journal of the American Medical Informatics Association</i>, (ocab200)https://doi.org/10.1093/jamia/ocab200</span></li>
<li><span id="hailuNIHfundedProjectAims2019">96. Hailu R (2019) NIH-Funded Project Aims to Build a ’Google’ for Biomedical Data. <i>STAT</i>, https://www.statnews.com/2019/07/31/nih-funded-project-aims-to-build-a-google-for-biomedical-data/</span></li>
<li><span id="groteEthicsAlgorithmicDecisionmaking2020">97. Grote T, Berens P (2020) On the Ethics of Algorithmic Decision-Making in Healthcare. <i>Journal of Medical Ethics</i>, 46(3):205–211. https://doi.org/10.1136/medethics-2019-105586</span></li>
<li><span id="obermeyerDissectingRacialBias2019">98. Obermeyer Z, Powers B, Vogeli C, Mullainathan S (2019) Dissecting Racial Bias in an Algorithm Used to Manage the Health of Populations. <i>Science</i>, 366(6464):447–453. https://doi.org/10.1126/science.aax2342</span></li>
<li><span id="panchArtificialIntelligenceAlgorithmic2019">99. Panch T, Mattie H, Atun R (2019) Artificial Intelligence and Algorithmic Bias: Implications for Health Systems. <i>Journal of Global Health</i>, 9(2):020318. https://doi.org/10.7189/jogh.09.020318</span></li>
<li><span id="panchInconvenientTruthAI2019">100. Panch T, Mattie H, Celi LA (2019) The “Inconvenient Truth” about AI in Healthcare. <i>npj Digital Medicine</i>, 2(1):1–3. https://doi.org/10.1038/s41746-019-0155-4</span></li>
<li><span id="haendelCommonDialectInfrastructure2021">101. Haendel MA (2021) A Common Dialect for Infrastructure and Services in Translator. https://reporter.nih.gov/project-details/10330632</span></li>
<li><span id="hukezalieVitroExVivo2012">102. Hukezalie KR, Thumati NR, Côté HCF, Wong JMY (2012) In Vitro and Ex Vivo Inhibition of Human Telomerase by Anti-HIV Nucleoside Reverse Transcriptase Inhibitors (NRTIs) but Not by Non-NRTIs. <i>PLoS ONE</i>, 7(11):e47505. https://doi.org/10.1371/journal.pone.0047505</span></li>
<li><span id="ZidovudinePatientNIH">103. Zidovudine - Patient | NIH. https://clinicalinfo.hiv.gov/en/drugs/zidovudine/patient</span></li>
<li><span id="RePORTRePORTERBiomedical2021">104. (2021) RePORT ⟩ RePORTER "Biomedical Data Translator". https://reporter.nih.gov/search/kDJ97zGUFEaIBIltUmyd_Q/projects?sort_field=FiscalYear&amp;sort_order=desc</span></li>
<li><span id="AWSAnnouncesAWS2021">105. (2021) AWS Announces AWS Healthcare Accelerator for Startups in the Public Sector. <i>Amazon Web Services</i>, https://aws.amazon.com/blogs/publicsector/aws-announces-healthcare-accelerator-program-startups-public-sector/</span></li>
<li><span id="lermanAmazonBuiltIts2021">106. Lerman R (2021) Amazon Built Its Own Health-Care Service for Employees. Now It’s Selling It to Other Companies. <i>Washington Post</i>, https://www.washingtonpost.com/technology/2021/03/17/amazon-healthcare-service-care-expansion/</span></li>
<li><span id="quinnYouCanTrust2021">107. Quinn C (2021) You Can’t Trust Amazon When It Feels Threatened. <i>Last Week in AWS</i>, https://www.lastweekinaws.com/blog/you-cant-trust-amazon-when-it-feels-threatened/</span></li>
<li><span id="bharatGeneratingUserInformation2005">108. Bharat K, Lawrence S, Sahami M (2005) Generating User Information for Use in Targeted Advertising. (US20050131762A1)https://patents.google.com/patent/US20050131762A1/en</span></li>
<li><span id="SmithFacebookInc2018">109. (2018) Smith v. Facebook, Inc., No. 17-16206 (9th Cir. Dec. 6, 2018). https://casetext.com/case/smith-v-facebook-inc-2</span></li>
<li><span id="krashinskyGoogleBrokeCanada2014">110. Krashinsky S (2014) Google Broke Canada’s Privacy Laws with Targeted Health Ads, Watchdog Says. <i>The Globe and Mail</i>, https://www.theglobeandmail.com/technology/tech-news/google-broke-canadas-privacy-laws-with-targeted-ads-regulator-says/article16343346/</span></li>
<li><span id="bourreauGoogleFitbitWill2020">111. Bourreau M, Caffarra C, Chen Z, Choe C, Crawford GS, Duso T, Genakos C, Heidhues P, Peitz M, Rønde T, Schnitzer M, Schutz N, Sovinsky M, Spagnolo G, Toivanen O, Valletti T, Vergé T (2020) Google/Fitbit Will Monetise Health Data and Harm Consumers. (107):13. </span></li>
<li><span id="rasmyMedBERTPretrainedContextualized2021">112. Rasmy L, Xiang Y, Xie Z, Tao C, Zhi D (2021) Med-BERT: Pretrained Contextualized Embeddings on Large-Scale Structured Electronic Health Records for Disease Prediction. <i>npj Digital Medicine</i>, 4(1):1–13. https://doi.org/10.1038/s41746-021-00455-y</span></li>
<li><span id="strangioCanReproductiveTrans2016">113. Strangio C (2016) Can Reproductive Trans Bodies Exist? <i>City University of New York Law Review</i>, 19(2):223. https://academicworks.cuny.edu/clr/vol19/iss2/3</span></li>
<li><span id="heimbignerFederatedArchitectureInformation1985">114. Heimbigner D, McLeod D (1985) A Federated Architecture for Information Management. <i>ACM Transactions on Information Systems</i>, 3(3):253–278. https://doi.org/10.1145/4229.4233</span></li>
<li><span id="litwinInteroperabilityMultipleAutonomous1990">115. Litwin W, Mark L, Roussopoulos N (1990) Interoperability of Multiple Autonomous Databases. <i>ACM Computing Surveys</i>, 22(3):267–293. https://doi.org/10.1145/96602.96608</span></li>
<li><span id="kashyapSemanticSchematicSimilarities1996">116. Kashyap V, Sheth A (1996) Semantic and Schematic Similarities between Database Objects:A Context-Based Approach. <i>The VLDB Journal</i>, 5(4):276–304. https://doi.org/10.1007/s007780050029</span></li>
<li><span id="hullManagingSemanticHeterogeneity1997">117. Hull R (1997) Managing Semantic Heterogeneity in Databases: A Theoretical Prospective. <i>Proceedings of the Sixteenth ACM SIGACT-SIGMOD-SIGART Symposium on Principles of Database Systems</i>, :51–61. https://doi.org/10.1145/263661.263668</span></li>
<li><span id="busseFederatedInformationSystems1999">118. Busse S, Kutsche R-D, Leser U, Weber H (1999) Federated Information Systems: Concepts, Terminology and Architectures. :40. </span></li>
<li><span id="djokic-petrovicPIBASFedSPARQLWebbased2017">119. Djokic-Petrovic M, Cvjetkovic V, Yang J, Zivanovic M, Wild DJ (2017) PIBAS FedSPARQL: A Web-Based Platform for Integration and Exploration of Bioinformatics Datasets. <i>Journal of Biomedical Semantics</i>, 8(1):42. https://doi.org/10.1186/s13326-017-0151-z</span></li>
<li><span id="hasnainBioFedFederatedQuery2017">120. Hasnain A, Mehmood Q, Sana e Zainab S, Saleem M, Warren C, Zehra D, Decker S, Rebholz-Schuhmann D (2017) BioFed: Federated Query Processing over Life Sciences Linked Open Data. <i>Journal of Biomedical Semantics</i>, 8(1):13. https://doi.org/10.1186/s13326-017-0118-0</span></li>
<li><span id="hankeDefenseDecentralizedResearch2021">121. Hanke M, Pestilli F, Wagner AS, Markiewicz CJ, Poline J-B, Halchenko YO (2021) In Defense of Decentralized Research Data Management. <i>Neuroforum</i>, 27(1):17–25. https://doi.org/10.1515/nf-2020-0037</span></li>
<li><span id="shethFederatedDatabaseSystems1990">122. Sheth AP, Larson JA (1990) Federated Database Systems for Managing Distributed, Heterogeneous, and Autonomous Databases. <i>ACM Computing Surveys</i>, 22(3):183–236. https://doi.org/10.1145/96602.96604</span></li>
<li><span id="bonifatiDistributedDatabasesPeertopeer2008">123. Bonifati A, Chrysanthis PK, Ouksel AM, Sattler K-U (2008) Distributed Databases and Peer-to-Peer Databases: Past and Present. <i>ACM SIGMOD Record</i>, 37(1):5–11. https://doi.org/10.1145/1374780.1374781</span></li>
<li><span id="MeatballWikiPersonalCategories">124. Meatball Wiki: PersonalCategories. http://meatballwiki.org/wiki/PersonalCategories</span></li>
<li><span id="pirroDHTbasedSemanticOverlay2012">125. Pirrò G, Talia D, Trunfio P (2012) A DHT-Based Semantic Overlay Network for Service Discovery. <i>Future Generation Computer Systems</i>, 28(4):689–707. https://doi.org/10.1016/j.future.2011.11.007</span></li>
<li><span id="Webber:18:A">126. Webber C, Tallon J, Shepherd E, Guy A, Prodromou E (2018) ActivityPub. https://www.w3.org/TR/2018/REC-activitypub-20180123/</span></li>
<li><span id="spornyJSONLDJSONbasedSerialization2020">127. Sporny M, Longley D, Kellogg G, Lanthaler M, Champin P-A, Lindström N (2020) JSON-LD 1.1 - A JSON-Based Serialization for Linked Data. https://www.w3.org/TR/json-ld/</span></li>
<li><span id="snellActivityStreams2017">128. Snell JM, Prodromou E (2017) Activity Streams 2.0. https://www.w3.org/TR/activitystreams-core/</span></li>
<li><span id="SPARQLFederatedQuery2013">129. (2013) SPARQL 1.1 Federated Query. https://www.w3.org/TR/sparql11-federated-query/</span></li>
<li><span id="simaEnablingSemanticQueries2019">130. Sima AC, Mendes de Farias T, Zbinden E, Anisimova M, Gil M, Stockinger H, Stockinger K, Robinson-Rechavi M, Dessimoz C (2019) Enabling Semantic Queries across Federated Bioinformatics Databases. <i>Database</i>, 2019(baz106)https://doi.org/10.1093/database/baz106</span></li>
<li><span id="halchenkoDataLadDistributedSystem2021">131. Halchenko YO, Meyer K, Poldrack B, Solanky DS, Wagner AS, Gors J, MacFarlane D, Pustina D, Sochat V, Ghosh SS, Mönch C, Markiewicz CJ, Waite L, Shlyakhter I, de la Vega A, Hayashi S, Häusler CO, Poline J-B, Kadelka T, Skytén K, Jarecka D, Kennedy D, Strauss T, Cieslak M, Vavra P, Ioanas H-I, Schneider R, Pflüger M, Haxby JV, Eickhoff SB, Hanke M (2021) DataLad: Distributed System for Joint Management of Code, Data, and Their Relationship. <i>Journal of Open Source Software</i>, 6(63):3262. https://doi.org/10.21105/joss.03262</span></li>
<li><span id="spiesWorkflowCentricApproachIncreasing2017">132. Spies J (2017) A Workflow-Centric Approach to Increasing Reproducibility and Data Integrity. https://scholarworks.iu.edu/dspace/handle/2022/21729</span></li>
<li><span id="harrisArrayProgrammingNumPy2020">133. Harris CR, Millman KJ, van der Walt SJ, Gommers R, Virtanen P, Cournapeau D, Wieser E, Taylor J, Berg S, Smith NJ, Kern R, Picus M, Hoyer S, van Kerkwijk MH, Brett M, Haldane A, del Río JF, Wiebe M, Peterson P, Gérard-Marchant P, Sheppard K, Reddy T, Weckesser W, Abbasi H, Gohlke C, Oliphant TE (2020) Array Programming with NumPy. <i>Nature</i>, 585(7825):357–362. https://doi.org/10.1038/s41586-020-2649-2</span></li>
<li><span id="virtanenSciPyFundamentalAlgorithms2020">134. Virtanen P, Gommers R, Oliphant TE, Haberland M, Reddy T, Cournapeau D, Burovski E, Peterson P, Weckesser W, Bright J, van der Walt SJ, Brett M, Wilson J, Millman KJ, Mayorov N, Nelson ARJ, Jones E, Kern R, Larson E, Carey CJ, Polat İ, Feng Y, Moore EW, VanderPlas J, Laxalde D, Perktold J, Cimrman R, Henriksen I, Quintero EA, Harris CR, Archibald AM, Ribeiro AH, Pedregosa F, van Mulbregt P (2020) SciPy 1.0: Fundamental Algorithms for Scientific Computing in Python. <i>Nature Methods</i>, 17(3):261–272. https://doi.org/10.1038/s41592-019-0686-2</span></li>
<li><span id="pedregosaScikitlearnMachineLearning2011">135. Pedregosa F, Varoquaux G, Gramfort A, Michel V, Thirion B, Grisel O, Blondel M, Prettenhofer P, Weiss R, Dubourg V, Vanderplas J, Passos A, Cournapeau D, Brucher M, Perrot M, Duchesnay É (2011) Scikit-Learn: Machine Learning in Python. <i>The Journal of Machine Learning Research</i>, 12(null):2825–2830. </span></li>
<li><span id="wiltschkoRevealingStructurePharmacobehavioral2020">136. Wiltschko AB, Tsukahara T, Zeine A, Anyoha R, Gillis WF, Markowitz JE, Peterson RE, Katon J, Johnson MJ, Datta SR (2020) Revealing the Structure of Pharmacobehavioral Space through Motion Sequencing. <i>Nature Neuroscience</i>, 23(11):1433–1443. https://doi.org/10.1038/s41593-020-00706-3</span></li>
<li><span id="coffeyDeepSqueakDeepLearningbased2019">137. Coffey KR, Marx RG, Neumaier JF (2019) DeepSqueak: A Deep Learning-Based System for Detection and Analysis of Ultrasonic Vocalizations. <i>Neuropsychopharmacology</i>, 44(5):859–868. https://doi.org/10.1038/s41386-018-0303-6</span></li>
<li><span id="yatsenkoDataJointSimplerRelational2018">138. Yatsenko D, Walker EY, Tolias AS (2018) DataJoint: A Simpler Relational Data Model. <i>arXiv:1807.11104 [cs]</i>, http://arxiv.org/abs/1807.11104</span></li>
<li><span id="yatsenkoDataJointElementsData2021">139. Yatsenko D, Nguyen T, Shen S, Gunalan K, Turner CA, Guzman R, Sasaki M, Sitonic D, Reimer J, Walker EY, Tolias AS (2021) DataJoint Elements: Data Workflows for Neurophysiology. <i>bioRxiv</i>, :2021.03.30.437358. https://doi.org/10.1101/2021.03.30.437358</span></li>
<li><span id="pachitariuKilosortRealtimeSpikesorting2016">140. Pachitariu M, Steinmetz N, Kadir S, Carandini M, D HK (2016) Kilosort: Realtime Spike-Sorting for Extracellular Electrophysiology with Hundreds of Channels. :061481. https://doi.org/10.1101/061481</span></li>
<li><span id="vanderaalstYAWLAnotherWorkflow2005">141. van der Aalst WMP, ter Hofstede AHM (2005) YAWL: Yet Another Workflow Language. <i>Information Systems</i>, 30(4):245–275. https://doi.org/10.1016/j.is.2004.02.002</span></li>
<li><span id="millerScientistNightmareSoftware2006">142. Miller G (2006) A Scientist’s Nightmare: Software Problem Leads to Five Retractions. <i>Science</i>, 314(5807):1856–1857. https://doi.org/10.1126/science.314.5807.1856</span></li>
<li><span id="soergelRampantSoftwareErrors2015">143. Soergel DAW (2015) Rampant Software Errors May Undermine Scientific Results. <i>F1000Research</i>, 3https://doi.org/10.12688/f1000research.5930.2</span></li>
<li><span id="eklundClusterFailureWhy2016a">144. Eklund A, Nichols TE, Knutsson H (2016) Cluster Failure: Why fMRI Inferences for Spatial Extent Have Inflated False-Positive Rates. <i>Proceedings of the National Academy of Sciences</i>, 113(28):7900–7905. https://doi.org/10.1073/pnas.1602413113</span></li>
<li><span id="bhandarineupaneCharacterizationLeptazolinesPolar2019">145. Bhandari Neupane J, Neupane RP, Luo Y, Yoshida WY, Sun R, Williams PG (2019) Characterization of Leptazolines A–D, Polar Oxazolines from the Cyanobacterium Leptolyngbya Sp., Reveals a Glitch with the “Willoughby–Hoye” Scripts for Calculating NMR Chemical Shifts. <i>Organic Letters</i>, 21(20):8449–8453. https://doi.org/10.1021/acs.orglett.9b03216</span></li>
<li><span id="chuSlowedCanonicalProgress2021">146. Chu JSG, Evans JA (2021) Slowed Canonical Progress in Large Fields of Science. <i>Proceedings of the National Academy of Sciences</i>, 118(41)https://doi.org/10.1073/pnas.2021636118</span></li>
<li><span id="heathersRealScandalIvermectin2021">147. Heathers J (2021) The Real Scandal About Ivermectin. <i>The Atlantic</i>, https://www.theatlantic.com/science/archive/2021/10/ivermectin-research-problems/620473/</span></li>
<li><span id="shenMeetThisSuperspotter2020">148. Shen H (2020) Meet This Super-Spotter of Duplicated Images in Science Papers. <i>Nature</i>, 581(7807):132–136. https://doi.org/10.1038/d41586-020-01363-z</span></li>
<li><span id="bikPrevalenceInappropriateImage2016">149. Bik EM, Casadevall A, Fang FC (2016) The Prevalence of Inappropriate Image Duplication in Biomedical Research Publications. <i>mBio</i>, 7(3):e00809–16. https://doi.org/10.1128/mBio.00809-16</span></li>
<li><span id="swartzTechniquesMassCollaboration2006">150. Swartz A (2006) The Techniques of Mass Collaboration: A Third Way Out (Aaron Swartz’s Raw Thought). http://www.aaronsw.com/weblog/masscollab2</span></li>
<li><span id="lombardiGoogleExecChallenges2007">151. Lombardi C (2007) Google Exec Challenges Berners-Lee | CNET News.Com. https://web.archive.org/web/20070105030625/http://news.com.com/Google+exec+challenges+Berners-Lee/2100-1025_3-6095705.html</span></li>
<li><span id="leufWikiWayQuick2001a">152. Leuf B, Cunningham W (2001) The Wiki Way : Quick Collaboration on the Web. http://archive.org/details/isbn_9780201714999</span></li>
<li><span id="swartzSecretsStandards2003">153. Swartz A (2003) Secrets of Standards. <i>Aaron Swartz: The Weblog</i>, http://www.aaronsw.com/weblog/001027</span></li>
<li><span id="kamelboulosSemanticWikisComprehensible2009">154. Kamel Boulos M (2009) Semantic Wikis: A Comprehensible Introduction with Examples from the Health Sciences. <i>Journal of Emerging Technologies in Web Intelligence</i>, 1https://doi.org/10.4304/jetwi.1.1.94-96</span></li>
<li><span id="classeDistributedInfrastructureSupport2017">155. Classe T, Braga R, David JMN, Campos F, Arbex W (2017) A Distributed Infrastructure to Support Scientific Experiments. <i>Journal of Grid Computing</i>, 15(4):475–500. https://doi.org/10.1007/s10723-017-9401-7</span></li>
<li><span id="goodSocialTaggingLife2009">156. Good BM, Tennis JT, Wilkinson MD (2009) Social Tagging in the Life Sciences: Characterizing a New Metadata Resource for Bioinformatics. <i>BMC Bioinformatics</i>, 10(1):313. https://doi.org/10.1186/1471-2105-10-313</span></li>
<li><span id="cheungSemanticWebApproach2007">157. Cheung K-H, Smith AK, Yip KYL, Baker CJO, Gerstein MB (2007) Semantic Web Approach to Database Integration in the Life Sciences. <i>Semantic Web</i>, :11–30. https://doi.org/10.1007/978-0-387-48438-9_2</span></li></ol>

<h1 id="footnotes">Footnotes</h1>

<div class="footnotes" role="doc-endnotes">
  <ol>
    <li id="fn:4" role="doc-endnote">
      <p>which, to be clear, is a valid feeling and is reflective of a failure of infrastructure, not a personal failure. <a href="#fnref:4" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:solaris" role="doc-endnote">
      <blockquote>
        <p>…  the recording instruments registered a profusion of signals - fragmentary indications of some outlandish activity, which in fact defeated all attempts at analysis. Did these data point to a momentary condition of stimulation, or to regular impulses correlated with the gigantic structures which the ocean was in the process of creating elsewhere, at the antipodes of the region under investigation? Had the electronic apparatus recorded the cryptic manifestation of the ocean’s ancient secrets? Had it revealed its innermost workings to us? Who could tell? No two reactions to the stimuli were the same. Sometimes the instruments almost exploded under the violence of the impulses, sometimes there was total silence; it was impossible to obtain a repetition of any previously observed phenomenon. Constantly, it seemed, the experts were on the brink of deciphering the ever-growing mass of information. Was it not, after all, with this object in mind that computers had been built of virtually limitless capacity, such as no previous problem had ever demanded?</p>

        <p>And, indeed, some results were obtained. The ocean as a source of electric and magnetic impulses and of gravitation expressed itself in a more or less mathematical language. Also, by calling on the most abstruse branches of statistical analysis, it was possible to classify certain frequencies in the discharges of current. Structural homologues were discovered, not unlike those already observed by physicists in that sector of science which deals with the reciprocal interaction of energy and matter, elements and compounds, the finite and the infinite. This correspondence convinced the scientists that they were confronted with a monstrous entity endowed with reason, a protoplasmic ocean-brain enveloping the entire planet and idling its time away in extravagant theoretical cognitation about the nature of the universe. Our instruments had intercepted minute random fragments of a prodigious and everlasting monologue unfolding in the depths of this colossal brain, which was inevitably beyond our understanding.</p>
      </blockquote>

      <blockquote>
        <p>So much for the mathematicians. These hypotheses, according to some people, underestimated the resources of the human mind; they bowed to the unknown, proclaiming the ancient doctrine, arrogantly resurrected, of ignoramus et ignorabimus. Others regarded the mathematicians’ hypotheses as sterile and dangerous nonsense, contributing towards the creation of a modern mythology based on the notion of this giant brain - whether plasmic or electronic was immaterial - as the ultimate objective of existence, the very synthesis of life.</p>
      </blockquote>

      <blockquote>
        <p>Yet others… but the would-be experts were legion and each had his own theory. A comparison of the ‘contact’ school of thought with other branches of Solarist studies, in which specialization had rapidly developed, especially during the last quarter of a century, made it clear that a Solarist-cybernetician had difficulty in making himself understood to a Solarist-symmetriadologist. Veubeke, director of the Institute when I was studying there, had asked jokingly one day: <strong>“How do you expect to communicate with the ocean, when you can’t even understand one another?”</strong> The jest contained more than a grain of truth. […]</p>
      </blockquote>

      <blockquote>
        <p>Lifting the heavy volume with both hands, I replaced it on the shelf, and thought to myself that our scholarship, all the information accumulated in the libraries, amounted to a useless jumble of words, a sludge of statements and suppositions, and that we had not progressed an inch in the 78 years since researches had begun. The situation seemed much worse now than in the time of the pioneers, since the assiduous efforts of so many years had not resulted in a single indisputable conclusion. “</p>
      </blockquote>

      <p>Stanisław Lem, <em>Solaris</em>, essential reading for all neuroscientists <a href="#fnref:solaris" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:butno" role="doc-endnote">
      <p>A <em>lovely</em> jumble! that probably has a lot of good qualities, it’s just a little lonely maybe :( <a href="#fnref:butno" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:figuratively" role="doc-endnote">
      <p>Figuratively! Non-quantitatively! <a href="#fnref:figuratively" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:tymae" role="doc-endnote">
      <p>Thanks a lot to the one-and-only stunning and brilliant Dr. Eartha Mae Guthman for suggesting looking at the BRAIN initiative grants as a way of getting insight on core facilities. <a href="#fnref:tymae" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:pnidatascience" role="doc-endnote">
      <blockquote>
        <p>Project Summary: Core 2, Data Science Working memory, the ability to temporarily hold multiple pieces of information in mind for manipulation, is central to virtually all cognitive abilities. This multi-component research project aims to comprehensively dissect the neural circuit mechanisms of this ability across multiple brain areas. In doing so, it will generate an extremely large quantity of data, from multiple types of experiments, which will then need to be integrated together. The Data Science Core will support the individual research projects in discovering relationships among behavior, neural activity, and neural connectivity. The Core will create a standardized computational pipeline and human workflow for preprocessing of calcium-imaging data. The pipeline will run either on local computers or in cloud computing services, and users will interact with it through a web browser. The preprocessing will incorporate existing image-processing algorithms, such as Constrained Nonnegative Matrix Factorization and convolutional networks. In addition, the Core will build a data science platform that stores behavior, neural activity, and neural connectivity in a relational database that is queried by the DataJoint language. Diverse analysis tools will be integrated into DataJoint, enabling the robust maintenance of data-processing chains. This data-science platform will facilitate collaborative analysis of datasets by multiple researchers within the project, and make the analyses reproducible and extensible by other researchers. We will develop effective methods for training and otherwise disseminating our computational tools and workflows. Finally, the Core will make raw data, derived data, and analyses available to the public upon publication via the data-science platform, source-code repositories, and web-based visualization tools. To facilitate the conduct of this research, the creation of software tools, and the reuse of the data by others after the primary research has concluded, the project will adopt shared data and metadata formats using the HDF5 implementation of the Neurodata without Borders format. Data will be made public in accord with the FAIR guiding principles—findndable by a DOI and/or URL, accessible through a RESTful web API, and interoperable and reusable due to DataJoint and the Neurodata Without Borders format for data https://projectreporter.nih.gov/project_info_description.cfm?aid=9444126&amp;icde=0</p>
      </blockquote>
      <p><a href="#fnref:pnidatascience" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:pnicaveat" role="doc-endnote">
      <p>Though again, this project is examplary, built by friends, and would be an excellent place to start extending towards global infrastructure. <a href="#fnref:pnicaveat" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:grantdb" role="doc-endnote">
      <p>granting agencies seem to love funding new databases, idk. <a href="#fnref:grantdb" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:osfspeed" role="doc-endnote">
      <p>As I am writing this, I am getting a (very unscientific) maximum speed of 5MB/s on the <a href="https://osf.io">Open Science Framework</a> <a href="#fnref:osfspeed" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:p2pdiscipline" role="doc-endnote">
      <p>peer to peer systems are, maybe predictably, a whole academic subdiscipline. See <a class="citation" href="#shenHandbookPeertoPeerNetworking2010">[51]</a> for reference. <a href="#fnref:p2pdiscipline" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:knockin" role="doc-endnote">
      <p>knock on wood <a href="#fnref:knockin" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:whatsgit" role="doc-endnote">
      <p>Git, briefly, is a version control system that keeps a history of changes of files (blobs) as a Merkle DAG: files can be updated, and different versions can be branched and reconciled. <a href="#fnref:whatsgit" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:whatdiss" role="doc-endnote">
      <p>for a detailed description of the site and community, see Ian Dunham’s dissertation <a class="citation" href="#dunhamWhatCDLegacy2018">[76]</a> <a href="#fnref:whatdiss" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:dbsize" role="doc-endnote">
      <p>Though spotify now boasts its library having 50 million tracks, back of the envelope calculations relating number of releases to number of tracks are fraught, given the long tail of track numbers on albums like classical music anthologies with several hundred tracks on a single “release.” <a href="#fnref:dbsize" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:lostartists" role="doc-endnote">
      <blockquote>
        <p>“Among the incinerated Decca masters were recordings by titanic figures in American music: Louis Armstrong, Duke Ellington, Al Jolson, Bing Crosby, Ella Fitzgerald, Judy Garland. The tape masters for Billie Holiday’s Decca catalog were most likely lost in total. The Decca masters also included recordings by such greats as Louis Jordan and His Tympany Five and Patsy Cline.</p>

        <p>The fire most likely claimed most of Chuck Berry’s Chess masters and multitrack masters, a body of work that constitutes Berry’s greatest recordings. The destroyed Chess masters encompassed nearly everything else recorded for the label and its subsidiaries, including most of the Chess output of Muddy Waters, Howlin’ Wolf, Willie Dixon, Bo Diddley, Etta James, John Lee Hooker, Buddy Guy and Little Walter. Also very likely lost were master tapes of the first commercially released material by Aretha Franklin, recorded when she was a young teenager performing in the church services of her father, the Rev. C.L. Franklin, who made dozens of albums for Chess and its sublabels.</p>

        <p>Virtually all of Buddy Holly’s masters were lost in the fire. Most of John Coltrane’s Impulse masters were lost, as were masters for treasured Impulse releases by Ellington, Count Basie, Coleman Hawkins, Dizzy Gillespie, Max Roach, Art Blakey, Sonny Rollins, Charles Mingus, Ornette Coleman, Alice Coltrane, Sun Ra, Albert Ayler, Pharoah Sanders and other jazz greats. Also apparently destroyed were the masters for dozens of canonical hit singles, including Bill Haley and His Comets’ “Rock Around the Clock,” Jackie Brenston and His Delta Cats’ “Rocket 88,” Bo Diddley’s “Bo Diddley/I’m A Man,” Etta James’s “At Last,” the Kingsmen’s “Louie Louie” and the</p>

        <p>The list of destroyed single and album masters takes in titles by dozens of legendary artists, a genre-spanning who’s who of 20th- and 21st-century popular music. It includes recordings by Benny Goodman, Cab Calloway, the Andrews Sisters, the Ink Spots, the Mills Brothers, Lionel Hampton, Ray Charles, Sister Rosetta Tharpe, Clara Ward, Sammy Davis Jr., Les Paul, Fats Domino, Big Mama Thornton, Burl Ives, the Weavers, Kitty Wells, Ernest Tubb, Lefty Frizzell, Loretta Lynn, George Jones, Merle Haggard, Bobby (Blue) Bland, B.B. King, Ike Turner, the Four Tops, Quincy Jones, Burt Bacharach, Joan Baez, Neil Diamond, Sonny and Cher, the Mamas and the Papas, Joni Mitchell, Captain Beefheart, Cat Stevens, the Carpenters, Gladys Knight and the Pips, Al Green, the Flying Burrito Brothers, Elton John, Lynyrd Skynyrd, Eric Clapton, Jimmy Buffett, the Eagles, Don Henley, Aerosmith, Steely Dan, Iggy Pop, Rufus and Chaka Khan, Barry White, Patti LaBelle, Yoko Ono, Tom Petty and the Heartbreakers, the Police, Sting, George Strait, Steve Earle, R.E.M., Janet Jackson, Eric B. and Rakim, New Edition, Bobby Brown, Guns N’ Roses, Queen Latifah, Mary J. Blige, Sonic Youth, No Doubt, Nine Inch Nails, Snoop Dogg, Nirvana, Soundgarden, Hole, Beck, Sheryl Crow, Tupac Shakur, Eminem, 50 Cent and the Roots.</p>

        <p>Then there are masters for largely forgotten artists that were stored in the vault: tens of thousands of gospel, blues, jazz, country, soul, disco, pop, easy listening, classical, comedy and spoken-word records that may now exist only as written entries in discographies.” <a class="citation" href="#rosenDayMusicBurned2019">[77]</a></p>
      </blockquote>
      <p><a href="#fnref:lostartists" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:spectral" role="doc-endnote">
      <p>The average what.cd user was, as a result, on par with many of the auditory neuroscientists I know in their ability to read a spectrogram. <a href="#fnref:spectral" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:subtlety" role="doc-endnote">
      <p>Though music metadata might seem like a trivial problem (just look at the fields in an MP3 header), the number of edge cases are profound. How would you categorize an early Madlib casette mixtape remastered and uploaded to his website where he is mumbling to himself while recording some live show performed by multiple artists, but on the b-side is one of his Beat Konducta collections that mix together studio recordings from a collection of other artists? Who is the artist? How would you even identify the unnamed artists in the live show? Is that a compilation or a bootleg? Is it a cassette rip, a remaster, or a web release? <a href="#fnref:subtlety" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:mostly" role="doc-endnote">
      <p>Mostly. You know how the internet goes… <a href="#fnref:mostly" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:trackeranalogy" role="doc-endnote">
      <p>To continue the analogy to bittorrent trackers, an example domain-specific vs. domain-general dichotomy might be What.cd (with its specific formatting and aggregation tools for representing artists, albums, collections, genres, and so on) vs. ThePirateBay (with its general categories of content and otherwise search-based aggregation interface) <a href="#fnref:trackeranalogy" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:yrcool" role="doc-endnote">
      <p>No shade to Figshare, which, among others, paved the way for open data and are a massively useful thing to have in society. <a href="#fnref:yrcool" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:impossibledata" role="doc-endnote">
      <blockquote>
        <p>First, we assert that a single monolithic data set that directly connects the complete set of clinical characteristics to the complete set of biomolecular features, including “-omics” data, will never exist because the number of characteristics and features is constantly shifting and exponentially growing. Second, even if such a single monolithic data set existed, all-vs.-all associations will inevitably succumb to problems with statistical power (i.e., the curse of dimensionality).9 Such problems will get worse, not better, as more and more clinical and biomolecular data are collected and become available. We also assert that there is no single language, software or natural, with which to express clinical and biomolecular observations—these observations are necessarily and appropriately linked to the measurement technologies that produce them, as well as the nuances of language. The lack of a universal language for expressing clinical and biomolecular observations presents a risk of isolation or marginalization of data that are relevant for answering a particular inquiry, but are never accessed because of a failure in translation.</p>
      </blockquote>
      <p><a href="#fnref:impossibledata" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:ipredit" role="doc-endnote">
      <p>I submitted a <a href="https://github.com/jannahastings/mental-functioning-ontology/pull/8">pull request</a> to remove it. A teardrop in the ocean. <a href="#fnref:ipredit" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:googlepatent" role="doc-endnote">
      <p>A patent from Google is telling about how they view privacy concerns: whatever we can’t get explicitly, we’ll infer to sell better ads!</p>
      <blockquote>
        <p>One possible method to improve ad targeting is for ad targeting systems to obtain and use user profiles. For example, user profiles may be determined using information voluntarily given by users (e.g., when they subscribe to a service). This user attribute information may then be matched against advertiser specified attributes of the ad (e.g., targeting criteria). Unfortunately, user profile information is not always available since many Websites (e.g., search engines) do not require subscription or user registration. Moreover, even when available, the user profile may be incomplete (e.g., because the information given at the time of subscription may be limited to what is needed for the service and hence not comprehensive, because of privacy considerations, etc.). Furthermore, advertisers may need to manually define user profile targeting information. In addition, even if user profile information is available, advertisers may not be able to use this information to target ads effectively. <a class="citation" href="#bharatGeneratingUserInformation2005">[108]</a></p>
      </blockquote>
      <p><a href="#fnref:googlepatent" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:diderot" role="doc-endnote">
      <p>not to mention a sort of enlightenment-era diderot-like quest for the encyclopedia of everything <a href="#fnref:diderot" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:federatedterm" role="doc-endnote">
      <p>though there are subtleties to the terminology, with related terms like “multidatabase,” “data integration,” and “data lake” composing subtle shades of a shared idea. I will use federated databases as a single term that encompasses these multiple ideas here, for the sake of constraining the scope of the paper. <a href="#fnref:federatedterm" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:chainz" role="doc-endnote">
      <p>!! now would be the time blockchain ppl are like “but wait! that’s centralization! how can you trust ORCID??” Those kinds of systems are designed for zero-trust environments, but we don’t need absolute zero trust in this system since we are assuming we’re operating with visible entities in a system already bound to some degree by reputation. <a href="#fnref:chainz" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:notreallynwb" role="doc-endnote">
      <p>not really where it would be in the standard, but go with it plz <a href="#fnref:notreallynwb" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:creditte" role="doc-endnote">
      <p>we’ll return to credit assignment, don’t worry! I wouldn’t leave a friend out to dry. <a href="#fnref:creditte" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
  </ol>
</div>

</div>

    </main>

    <footer>
  <!-- hypothes.is -->
    <script src="/infrastructure/assets/js/react.js"></script>
</footer>

  </body>
</html>
